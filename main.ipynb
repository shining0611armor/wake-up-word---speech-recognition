{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFSKg7yi24Rt"
      },
      "source": [
        "### first we read our data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYO5JlPY1ncM",
        "outputId": "bab10bb4-f48f-4f63-873e-dd1e9efd1d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "k9xuYSGp4uo-",
        "outputId": "b1947254-f4b0-447d-e649-db08321b020b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UcK0JCyODND2pkhapuZUlMDtv9QgnViA\n",
            "To: /content/alexa.zip\n",
            "100%|██████████| 21.2M/21.2M [00:01<00:00, 18.5MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/alexa.zip'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "file_id = '1UcK0JCyODND2pkhapuZUlMDtv9QgnViA'\n",
        "output = '/content/alexa.zip'\n",
        "\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZLRkjB3BK_"
      },
      "source": [
        "unzipping data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7lSEWGt4y4b",
        "outputId": "38f844a6-e2a8-46bb-e625-59626bdc69c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/alexa.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/alexa_data\")\n",
        "\n",
        "print(\"Data extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWgJ4HsD3D6V"
      },
      "source": [
        "checking whether it exists on our directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwbMAiAA42di",
        "outputId": "54af77f5-9e08-40b6-a954-3c40249398b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/alexa_data contains 0 files\n",
            "/content/alexa_data/negative contains 350 files\n",
            "9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977573543.wav\n",
            "9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974692717.wav\n",
            "not-wakeword-es-88095a4a-bf56-11e9-8db3-bf37d8f5664b.wav\n",
            "9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977543768.wav\n",
            "not-wakeword-es-f414243c-bf55-11e9-b0b1-6388a807a6a6.wav\n",
            "/content/alexa_data/positive contains 127 files\n",
            "109.wav\n",
            "189.wav\n",
            "44.wav\n",
            "103.wav\n",
            "201.wav\n",
            "/content/alexa_data/__MACOSX contains 2 files\n",
            "._positive\n",
            "._negative\n",
            "/content/alexa_data/__MACOSX/negative contains 350 files\n",
            "._not-wakeword-es-29e79026-bf56-11e9-8654-97918f88856f.wav\n",
            "._0a977721-6fae-4eae-baa0-434888e0c802.wav\n",
            "._9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979552138.wav\n",
            "._9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977148562.wav\n",
            "._not-wakeword-es-fb21de82-bf54-11e9-9f10-33d4aacc9651.wav\n",
            "/content/alexa_data/__MACOSX/positive contains 127 files\n",
            "._72.wav\n",
            "._54.wav\n",
            "._313.wav\n",
            "._191.wav\n",
            "._165.wav\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_directory = '/content/alexa_data'\n",
        "\n",
        "for root, dirs, files in os.walk(data_directory):\n",
        "    print(root, \"contains\", len(files), \"files\")\n",
        "    for file in files[:5]:\n",
        "        print(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdSU9fNx3Qvk"
      },
      "source": [
        "## In the not augmented case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nGvz-vb3Vzw"
      },
      "source": [
        "Firts extracting features and preprocess it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY6AXpVq65KF",
        "outputId": "d4102ea4-689d-4267-ee59-fb511821a252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977573543.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974692717.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-88095a4a-bf56-11e9-8db3-bf37d8f5664b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977543768.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f414243c-bf55-11e9-b0b1-6388a807a6a6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975451942.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971968776.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977225958.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974612407.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-8aa051c0-bf54-11e9-86cf-a3b0097c56c1.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-a2fc8872-bf56-11e9-96da-d78cff454b66.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c44ea20a-bf54-11e9-9f6e-d37a7c55122e.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977246459.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-5742c6f4-bf55-11e9-a6e7-ffdd961e2d14.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975461825.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975402145.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013105546.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975366095.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-a2112e50-bf55-11e9-9c67-438939669cf1.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_d0da6c87-dc91-4a63-b959-f82f89e274b4_1608015745222.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975459385.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-9ff53f0c-bf56-11e9-b13d-0b82d20e6a89.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974789833.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-5ece8746-bf55-11e9-b295-234458b218c2.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-d2c1c7f2-bf56-11e9-b269-6fb23de146a6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976981870.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-fb11c7b0-c805-484d-b65f-d4f6e1d96f06.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-bd8635e4-bf56-11e9-afdf-23c80ddc86c1.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975339892.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977547181.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-e645a65e-bf56-11e9-9fc0-a76321c0e8a5.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975486916.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_d0da6c87-dc91-4a63-b959-f82f89e274b4_1608015571762.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975336684.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977539728.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976969648.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975333511.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-fb21de82-bf54-11e9-9f10-33d4aacc9651.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_fda571e7-61b8-491f-89c3-a00efcdc2b34_1608013917243.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979542261.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607975002157.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-27301772-bf56-11e9-9eb3-2f7379433fcc.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_37d9afcaf9099b62acdf89414b84b12b_android_733014fb-7e20-453e-9902-dce86efedaec_1608017024821.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975372110.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977002872.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977172788.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-ee407f4c-bf55-11e9-a290-cfe1fdd3de1e.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-6c045cd2-bf56-11e9-95e2-dbac3ea20b99.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974814261.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_7c74d6cc-d71b-42b6-a5c4-9969eff0d545_1608013559070.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977089026.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974576006.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-47856550-bf55-11e9-a556-2b5a96591956.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-78c9a950-bf55-11e9-85d4-ab4b3c115542.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_9021c874-549e-4a31-a926-9623da83e9b7_1608016989840.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977082616.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_d0da6c87-dc91-4a63-b959-f82f89e274b4_1608015841441.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-2476a348-bf56-11e9-b085-d397915472ea.wav\n",
            "Processing file: /content/alexa_data/negative/heycomputer_en_heycomputer-en-46bb497b-833e-423f-90f7-da28987402ac.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-8f3c9450-bf54-11e9-b934-f353d2e8281e.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971971598.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979566063.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977566239.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977522672.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974697598.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974798347.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-4abb6692-bf56-11e9-828e-23bfb607c48f.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-420dcf08-bf56-11e9-9d48-ef390995996e.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-7eae9f78-bf56-11e9-992d-3fb91c09a72b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_a5fffeed9a2775bbc4531f72a746ecba_android_38a7061d-0f14-425c-93ba-4b2bd3588296_1607971697368.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-ec3b39c2-bf54-11e9-a80f-a3cebd47caa9.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise8.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974795721.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977475668.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-5a7640de-bf56-11e9-8579-ebb931e28b54.wav\n",
            "Processing file: /content/alexa_data/negative/heychatterbox_en-us_heychatterbox-en-us-10c59835-a0ef-4d0b-b0dd-37cd24e5c7aa.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971974055.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-9d068014-161d-4006-9502-1948a10e565c.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-47ff4342-bf56-11e9-bee5-5778fa81b0e5.wav\n",
            "Processing file: /content/alexa_data/negative/heycomputer_en_heycomputer-en-81ce4274-da03-4342-b695-7a92d507afba.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_ca281dac-dc76-456a-a020-013a8ec90887_1608014419662.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976962517.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f154d00c-bf55-11e9-b833-df51852be5d1.wav\n",
            "Processing file: /content/alexa_data/negative/heysavant-en-20de3396-4c4d-46c1-a2a1-d9ff82eb9322.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977006528.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-d5c73346-5607-4fdf-832e-06db1d329182.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-cc503caa-bf56-11e9-8449-e7e47e8bfdb7.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise6.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-2ab80fc2-bf55-11e9-8e57-632a52545770.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977480919.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975394720.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977023641.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977118293.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975449125.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013075140.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013179508.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975387210.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977560976.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974676605.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c851393a-bf54-11e9-9b7e-5fe4e15c22c6.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-88503c7e-f1cd-457e-bdd2-1b519ee1c41c.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise3.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-21b5bcd4-bf56-11e9-b4b2-ab8986524a3b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974669574.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-cf778500-bf56-11e9-af9c-dbb0e8f88cbe.wav\n",
            "Processing file: /content/alexa_data/negative/0a455704-286a-4851-82e2-b4f486ba8f21.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977109453.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975444512.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-d36d72ce-bf55-11e9-ae1d-97f4f2c73a51.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977210700.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_9347bbc9-fd9d-4d19-b270-c09359852dd8_1608016543814.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013122008.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971991527.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977223132.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-c527417c-1985-4cc4-935b-469fa477f327.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974567168.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-afd9d8b4-8b9d-43d9-af09-435afdfe377c.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013153069.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977180421.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975438660.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975456365.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c85a67ec-bf56-11e9-80ee-73805b2bdf23.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607975023611.wav\n",
            "Processing file: /content/alexa_data/negative/heysavant-en-50761ea3-330f-466a-8fe7-09f3aa7af60c.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-90db9f92-78a5-40f5-8b18-0b645e3cbcbc.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979530969.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977558140.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-a0f3df11-de28-46be-a079-564a9be182fa.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977231215.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977121103.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971933603.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974569797.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-fb5aa8be-bf56-11e9-b367-03c733e39e3e.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976976592.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608012890469.wav\n",
            "Processing file: /content/alexa_data/negative/heycomputer_en_heycomputer-en-258d487d-a2bb-4619-a636-cff032b5c5d0.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-86b15ec8-bf55-11e9-9101-4f639f1e44ed.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise2.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608013141223.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974606584.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977163168.wav\n",
            "Processing file: /content/alexa_data/negative/0ab0b3c1-3536-4730-af58-66f773142eaa.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974641767.wav\n",
            "Processing file: /content/alexa_data/negative/0abb1cb6-9c67-41cf-bba8-8c39a953e2e9.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977030965.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-3f44716e-bf56-11e9-8fd9-d72fb835b9f5.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-810207b6-bf55-11e9-a04f-4bf041366e0c.wav\n",
            "Processing file: /content/alexa_data/negative/0b601785-a157-4263-920c-7890e3564efc.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974996901.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-4db1f1d6-bf56-11e9-adce-a785900be15b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976987329.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974672007.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974800970.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977079595.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979526738.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-9d8602d6-d062-423d-95e1-5d0d200109a6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974667140.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-61c96bd7-b25e-4c59-a36c-d999cef14e22.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974687283.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974722229.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979522694.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974615030.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974805632.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-d05958ec-bf54-11e9-8b8e-2b4a5a260fce.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-13a6eba0-bf55-11e9-8a1d-cb1a592bde15.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977213522.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977424150.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975369097.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-702763c2-bf56-11e9-a7da-ff68c5d02041.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-2ffbebb3-d38f-4b12-9fa4-e4deb8a77803.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-297db0c4-b05d-43b0-bf3c-511834b3ed38.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971879040.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-efe3f51c-bf56-11e9-88da-a7c8377a67b7.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-295356ee-14a4-4f0d-9e1b-1fcddc488142.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977148562.wav\n",
            "Processing file: /content/alexa_data/negative/amelia_en_amelia-1b90dfdb-73f9-4558-9270-00b5e8fe0280.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979572546.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-1c5a7af5-659d-486a-be10-b71b1c48a8e9.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-04b426bb-5a19-4708-acbe-35445bd213d4.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-113240c4-b6e8-4efe-93d1-f7613a569aa9.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974621036.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-fdce62e4-bf55-11e9-8aab-7b8c8108cb46.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971984582.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-ed3c23de-bf56-11e9-abe1-c7a5017d4c4c.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-b69c905c-bf56-11e9-95e6-cff3e859b7fa.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977535888.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974743221.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-44d8cab2-bf56-11e9-b4a0-9f0f5ceffdac.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_c4fd992a-47e4-4bc3-a46b-54fc92687dbd_1608016787211.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-0c3207dc-bf56-11e9-8959-e3cdd5d9e2fe.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_7c74d6cc-d71b-42b6-a5c4-9969eff0d545_1608013494968.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c0c77408-bf55-11e9-8942-ab2e43234b0e.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975391703.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_a5fffeed9a2775bbc4531f72a746ecba_android_38a7061d-0f14-425c-93ba-4b2bd3588296_1607971712752.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974820268.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977471621.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974907307.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-62286560-bf55-11e9-8064-9ba7da21d2d7.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977026924.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-df0870fc-bf55-11e9-8b93-c78972f237b1.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977195662.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-7b6f1658-bf56-11e9-9661-7fd93b632ec6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975409000.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971964539.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-3d272ba2-bf55-11e9-aed6-0b9b97be64a6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974733192.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974639149.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974650039.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-1ddbebac-bf55-11e9-a9cf-afdf30eed6d3.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-6c09d0e7-d9fc-40a1-ba7c-b873c7f2c87b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_d0da6c87-dc91-4a63-b959-f82f89e274b4_1608015809743.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_ca281dac-dc76-456a-a020-013a8ec90887_1608014447427.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-87b22a89-71aa-4f64-9c0a-da4f4eaea9e0.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975468515.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_7c74d6cc-d71b-42b6-a5c4-9969eff0d545_1608013519856.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-de500fd4-bf56-11e9-a405-c7015f514268.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971960813.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971869108.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977010377.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-1fd8b5fe-cb5f-46f3-a65e-1aae0ae975e6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979546879.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977129297.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f01b237c-bf54-11e9-98b3-7f83f99d8dc4.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974564555.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974588076.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977015189.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974603890.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977465772.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-091f02de-bf56-11e9-9cbb-eb77bf3465d8.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975360175.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-ab421b3e-bf54-11e9-b9ae-5b6dd2f546fb.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-84187d4a-bf55-11e9-bd5e-f70abace2ec8.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise4.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974559336.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-4421a58c-07bb-4f01-800e-9bf0b48b98fb.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-ba6182ba-bf56-11e9-8684-ab22e39415de.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-168b7458-bf55-11e9-b5bf-13fdd5f05304.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975421189.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974811071.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-94b2c268-66b8-446b-a3d0-3ed517b6a35f.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974739187.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-9c8715c6-bf55-11e9-b916-b3e846e9ee18.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977050363.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977417200.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_ca281dac-dc76-456a-a020-013a8ec90887_1608014475833.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise5.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974592810.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977514141.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-6d609c8f-fd78-4e88-9172-fa8a1f5346e6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976990350.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-9e69c2bf-04e5-427d-81a4-0664c1e0cb32.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-9f4bb91a-bf55-11e9-9b1a-0bed6c04e8c5.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979534629.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-fa643e76-bf55-11e9-89c4-e3b7b01acaac.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-054e18a8-bf55-11e9-9993-e38d916118eb.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977533368.wav\n",
            "Processing file: /content/alexa_data/negative/amelia_en_amelia-027efc35-2ca0-4ac7-bcbe-3704eabdc4f8.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-69cb447c-bf55-11e9-a976-bb7eaa90269a.wav\n",
            "Processing file: /content/alexa_data/negative/00af045b-ead8-4379-9110-c038e0bdd855.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977201095.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977250123.wav\n",
            "Processing file: /content/alexa_data/negative/heychatterbox_en-us_heychatterbox-en-us-1055b057-098d-486e-800e-c36015c66e8f.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977093251.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d76c41bc2e6b27593bf78dfafc956a86_sauro_ca281dac-dc76-456a-a020-013a8ec90887_1608014432581.wav\n",
            "Processing file: /content/alexa_data/negative/0a9121c4-8a1a-4833-9e09-4aa258fcfd65.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977576182.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-a1dc4ca4-bf54-11e9-aa80-db9c5d9a26d0.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-20e8ae02-bf55-11e9-96a0-fb5403b71c0f.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974655677.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977462118.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974578620.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979579395.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975352137.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-a7a20e3a-bf54-11e9-a333-5b3169dd9a4e.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974595442.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977468975.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f87017a6-bf56-11e9-bf64-2f54d8cf951f.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-9c61b4da-bf54-11e9-89c3-2f7088494ffb.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974823916.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-759a9301-0edd-4d4e-a7d5-999b03439007.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-0db11cb6-bf55-11e9-bb2d-eb4ba17dfed8.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974684663.wav\n",
            "Processing file: /content/alexa_data/negative/0b7b3af0-947b-494e-8001-04f157279473.wav\n",
            "Processing file: /content/alexa_data/negative/0c5213ae-db02-40c3-98ff-9201f44946a8.wav\n",
            "Processing file: /content/alexa_data/negative/0a3576a1-8a31-4e09-b340-474243fdf1c7.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974634148.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974618416.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-b3c19a26-bf56-11e9-acb8-33b149522ac0.wav\n",
            "Processing file: /content/alexa_data/negative/notwakeword-en-0caff310-b631-4bd1-8e40-4b37dd1c1429.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-96b61c38-bf54-11e9-a307-cf9c32bcf976.wav\n",
            "Processing file: /content/alexa_data/negative/0db1a061-59d0-42a0-a5b2-129651054ff1.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-7be78f22-bf54-11e9-9ed2-f707a5358fbf.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f59f5636-bf56-11e9-a7e2-4f92e82a4e1c.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977137752.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-b0e35bdc-bf56-11e9-a126-ff4cfc2fa48a.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-dc60ad60-bf55-11e9-a5b1-676956bc8ba6.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971872825.wav\n",
            "Processing file: /content/alexa_data/negative/0c990922-95e4-4a60-81b4-36b29d0759d6.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-2e59b90e-bf56-11e9-a7fe-7f47369f2ae5.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977478291.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975466067.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974561754.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-9d283950-bf56-11e9-a46f-f7fcae25939f.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977072750.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977555118.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_1ce58107-d0aa-4b79-bf7c-5114f6dd93c3_1607977458720.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974631069.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c895f902-bf55-11e9-bbcc-4fe68eed1822.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-92957dec-bf54-11e9-b090-43e215ed496b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975432046.wav\n",
            "Processing file: /content/alexa_data/negative/amelia_en_amelia-166ad489-a124-498f-90e7-5a7aecb1d832.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-29e79026-bf56-11e9-8654-97918f88856f.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-3682f0b4-bf56-11e9-97c3-ffddec7da0f6.wav\n",
            "Processing file: /content/alexa_data/negative/0ad4dcb2-d407-4f1d-83a5-ce5f8492c2fb.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974682046.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975397345.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-23d61fbe-bf55-11e9-b040-1feef400cde4.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976995230.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979552138.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975490127.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607976999473.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-eab3cb26-bf56-11e9-9552-bb4143dec4c3.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_f8487b4fdee76be041fc94a8a76a4ed9_android_0e4a0645-6584-4472-91d2-8184b719e313_1607971930699.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-76ad201a-bf56-11e9-aa15-1f4e6477b347.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979560606.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-c4aa2e6c-bf55-11e9-88d4-e71284069022.wav\n",
            "Processing file: /content/alexa_data/negative/0a977721-6fae-4eae-baa0-434888e0c802.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-e2a54216-bf56-11e9-a93b-eb0aff5626ce.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f7b6c252-bf55-11e9-8b38-ab453af32234.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974661508.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-d3bdcf2c-bf54-11e9-8304-37b7b45a1ec9.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_06d12d71fbd0bf70fa593ec2475640fc_sauro_2a00db17-15da-4d2b-9550-0c1619b11670_1607975411433.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_76e231b94677cda213d3ee13206c182a_sauro_64edad3b-755c-45c2-8352-b12445391f3a_1607979538032.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974556719.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_ea861acb0a8e8c9ccd5ededcb2abcfed_sauro_92681c5e-463f-432a-9da0-713f474e8edf_1607977241464.wav\n",
            "Processing file: /content/alexa_data/negative/00aba123-ae3a-4e0a-8603-9f7277b7d41f.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-0ebe99f2-bf56-11e9-b291-5f211049864b.wav\n",
            "Processing file: /content/alexa_data/negative/heysavant-en-1ce999a5-3875-44b2-a1a1-5e52f5f99214.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-f29f1c5a-bf56-11e9-9657-4fc807bc4b3b.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_d7c527b1ff5580a4ea2724c874d1af5f_sauro_03a3a366-e39e-4268-abbe-bf6812cd6bd1_1608012915885.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-27b00122-bf55-11e9-8c51-93059a01a982.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-518a7044-bf56-11e9-a8f7-d7efc3929784.wav\n",
            "Processing file: /content/alexa_data/negative/bg_noise9.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-d95adef6-bf55-11e9-b96f-cf309b66f297.wav\n",
            "Processing file: /content/alexa_data/negative/9df8ab6f-9357-4520-827f-c302ebb5a8d8_0f4df281688583e010c26831abdc2222_727d63da0efe8f7f069d094eed8741d5_sauro_cb5cea2b-7d89-4764-b0db-a61bce98f575_1607974710096.wav\n",
            "Processing file: /content/alexa_data/negative/0b62c269-a68f-4480-9e39-941cf6b7b085.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-19da41c4-bf56-11e9-9671-e358240c526e.wav\n",
            "Processing file: /content/alexa_data/negative/not-wakeword-es-107bc324-bf55-11e9-8873-0b9b411ca9d2.wav\n",
            "Processing file: /content/alexa_data/positive/109.wav\n",
            "Processing file: /content/alexa_data/positive/189.wav\n",
            "Processing file: /content/alexa_data/positive/44.wav\n",
            "Processing file: /content/alexa_data/positive/103.wav\n",
            "Processing file: /content/alexa_data/positive/201.wav\n",
            "Processing file: /content/alexa_data/positive/258.wav\n",
            "Processing file: /content/alexa_data/positive/97.wav\n",
            "Processing file: /content/alexa_data/positive/89.wav\n",
            "Processing file: /content/alexa_data/positive/236.wav\n",
            "Processing file: /content/alexa_data/positive/285.wav\n",
            "Processing file: /content/alexa_data/positive/281.wav\n",
            "Processing file: /content/alexa_data/positive/249.wav\n",
            "Processing file: /content/alexa_data/positive/296.wav\n",
            "Processing file: /content/alexa_data/positive/187.wav\n",
            "Processing file: /content/alexa_data/positive/247.wav\n",
            "Processing file: /content/alexa_data/positive/105.wav\n",
            "Processing file: /content/alexa_data/positive/209.wav\n",
            "Processing file: /content/alexa_data/positive/265.wav\n",
            "Processing file: /content/alexa_data/positive/174.wav\n",
            "Processing file: /content/alexa_data/positive/230.wav\n",
            "Processing file: /content/alexa_data/positive/237.wav\n",
            "Processing file: /content/alexa_data/positive/208.wav\n",
            "Processing file: /content/alexa_data/positive/282.wav\n",
            "Processing file: /content/alexa_data/positive/288.wav\n",
            "Processing file: /content/alexa_data/positive/144.wav\n",
            "Processing file: /content/alexa_data/positive/196.wav\n",
            "Processing file: /content/alexa_data/positive/51.wav\n",
            "Processing file: /content/alexa_data/positive/300.wav\n",
            "Processing file: /content/alexa_data/positive/176.wav\n",
            "Processing file: /content/alexa_data/positive/197.wav\n",
            "Processing file: /content/alexa_data/positive/146.wav\n",
            "Processing file: /content/alexa_data/positive/218.wav\n",
            "Processing file: /content/alexa_data/positive/101.wav\n",
            "Processing file: /content/alexa_data/positive/280.wav\n",
            "Processing file: /content/alexa_data/positive/313.wav\n",
            "Processing file: /content/alexa_data/positive/153.wav\n",
            "Processing file: /content/alexa_data/positive/9.wav\n",
            "Processing file: /content/alexa_data/positive/39.wav\n",
            "Processing file: /content/alexa_data/positive/22.wav\n",
            "Processing file: /content/alexa_data/positive/95.wav\n",
            "Processing file: /content/alexa_data/positive/102.wav\n",
            "Processing file: /content/alexa_data/positive/326.wav\n",
            "Processing file: /content/alexa_data/positive/248.wav\n",
            "Processing file: /content/alexa_data/positive/145.wav\n",
            "Processing file: /content/alexa_data/positive/71.wav\n",
            "Processing file: /content/alexa_data/positive/199.wav\n",
            "Processing file: /content/alexa_data/positive/195.wav\n",
            "Processing file: /content/alexa_data/positive/129.wav\n",
            "Processing file: /content/alexa_data/positive/2.wav\n",
            "Processing file: /content/alexa_data/positive/224.wav\n",
            "Processing file: /content/alexa_data/positive/11.wav\n",
            "Processing file: /content/alexa_data/positive/151.wav\n",
            "Processing file: /content/alexa_data/positive/231.wav\n",
            "Processing file: /content/alexa_data/positive/24.wav\n",
            "Processing file: /content/alexa_data/positive/205.wav\n",
            "Processing file: /content/alexa_data/positive/76.wav\n",
            "Processing file: /content/alexa_data/positive/57.wav\n",
            "Processing file: /content/alexa_data/positive/203.wav\n",
            "Processing file: /content/alexa_data/positive/316.wav\n",
            "Processing file: /content/alexa_data/positive/191.wav\n",
            "Processing file: /content/alexa_data/positive/166.wav\n",
            "Processing file: /content/alexa_data/positive/79.wav\n",
            "Processing file: /content/alexa_data/positive/235.wav\n",
            "Processing file: /content/alexa_data/positive/92.wav\n",
            "Processing file: /content/alexa_data/positive/182.wav\n",
            "Processing file: /content/alexa_data/positive/309.wav\n",
            "Processing file: /content/alexa_data/positive/183.wav\n",
            "Processing file: /content/alexa_data/positive/293.wav\n",
            "Processing file: /content/alexa_data/positive/113.wav\n",
            "Processing file: /content/alexa_data/positive/63.wav\n",
            "Processing file: /content/alexa_data/positive/15.wav\n",
            "Processing file: /content/alexa_data/positive/90.wav\n",
            "Processing file: /content/alexa_data/positive/286.wav\n",
            "Processing file: /content/alexa_data/positive/175.wav\n",
            "Processing file: /content/alexa_data/positive/172.wav\n",
            "Processing file: /content/alexa_data/positive/134.wav\n",
            "Processing file: /content/alexa_data/positive/240.wav\n",
            "Processing file: /content/alexa_data/positive/93.wav\n",
            "Processing file: /content/alexa_data/positive/192.wav\n",
            "Processing file: /content/alexa_data/positive/100.wav\n",
            "Processing file: /content/alexa_data/positive/150.wav\n",
            "Processing file: /content/alexa_data/positive/190.wav\n",
            "Processing file: /content/alexa_data/positive/54.wav\n",
            "Processing file: /content/alexa_data/positive/143.wav\n",
            "Processing file: /content/alexa_data/positive/10.wav\n",
            "Processing file: /content/alexa_data/positive/124.wav\n",
            "Processing file: /content/alexa_data/positive/36.wav\n",
            "Processing file: /content/alexa_data/positive/283.wav\n",
            "Processing file: /content/alexa_data/positive/31.wav\n",
            "Processing file: /content/alexa_data/positive/87.wav\n",
            "Processing file: /content/alexa_data/positive/234.wav\n",
            "Processing file: /content/alexa_data/positive/194.wav\n",
            "Processing file: /content/alexa_data/positive/226.wav\n",
            "Processing file: /content/alexa_data/positive/244.wav\n",
            "Processing file: /content/alexa_data/positive/233.wav\n",
            "Processing file: /content/alexa_data/positive/254.wav\n",
            "Processing file: /content/alexa_data/positive/232.wav\n",
            "Processing file: /content/alexa_data/positive/179.wav\n",
            "Processing file: /content/alexa_data/positive/88.wav\n",
            "Processing file: /content/alexa_data/positive/21.wav\n",
            "Processing file: /content/alexa_data/positive/287.wav\n",
            "Processing file: /content/alexa_data/positive/277.wav\n",
            "Processing file: /content/alexa_data/positive/257.wav\n",
            "Processing file: /content/alexa_data/positive/255.wav\n",
            "Processing file: /content/alexa_data/positive/52.wav\n",
            "Processing file: /content/alexa_data/positive/252.wav\n",
            "Processing file: /content/alexa_data/positive/72.wav\n",
            "Processing file: /content/alexa_data/positive/47.wav\n",
            "Processing file: /content/alexa_data/positive/117.wav\n",
            "Processing file: /content/alexa_data/positive/184.wav\n",
            "Processing file: /content/alexa_data/positive/147.wav\n",
            "Processing file: /content/alexa_data/positive/250.wav\n",
            "Processing file: /content/alexa_data/positive/77.wav\n",
            "Processing file: /content/alexa_data/positive/75.wav\n",
            "Processing file: /content/alexa_data/positive/127.wav\n",
            "Processing file: /content/alexa_data/positive/165.wav\n",
            "Processing file: /content/alexa_data/positive/91.wav\n",
            "Processing file: /content/alexa_data/positive/17.wav\n",
            "Processing file: /content/alexa_data/positive/246.wav\n",
            "Processing file: /content/alexa_data/positive/168.wav\n",
            "Processing file: /content/alexa_data/positive/14.wav\n",
            "Processing file: /content/alexa_data/positive/96.wav\n",
            "Processing file: /content/alexa_data/positive/152.wav\n",
            "Processing file: /content/alexa_data/positive/128.wav\n",
            "Processing file: /content/alexa_data/positive/299.wav\n",
            "Processing file: /content/alexa_data/positive/99.wav\n",
            "Processing file: /content/alexa_data/positive/53.wav\n",
            "Features and labels saved to /content/features.npz\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def extract_features(file_path):\n",
        "    \"\"\"\n",
        "    Extracting MFCC features from an audio file.\n",
        "\n",
        "    :param file_path: Path to the audio file.\n",
        "    :return: Extracted MFCC features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=16000)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "        return np.mean(mfccs.T, axis=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_and_extract(directory, output_file):\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        if \"__MACOSX\" in subdir:\n",
        "            continue  # we skip the   __MACOSX directory\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\") and not file.startswith(\"._\"):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                print(f\"Processing file: {file_path}\")\n",
        "                feature = extract_features(file_path)\n",
        "                if feature is not None:\n",
        "                    label = 1 if \"positive\" in subdir else 0\n",
        "                    features.append(feature)\n",
        "                    labels.append(label)\n",
        "\n",
        "    if features and labels:\n",
        "        features = np.array(features)\n",
        "        labels = np.array(labels)\n",
        "        np.savez(output_file, features=features, labels=labels)\n",
        "        print(f\"Features and labels saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No features or labels extracted. Please check the input data.\")\n",
        "\n",
        "preprocess_and_extract('/content/alexa_data', '/content/features.npz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdY5m0-3no2"
      },
      "source": [
        "checking the labels initialization correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGKwYdGqcNwy",
        "outputId": "9afe375a-617f-4fc6-a647-7c40f74386f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = np.load('/content/features.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "y.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNKZZxIpciZ5",
        "outputId": "db4035b5-b309-4e83-c61f-6dc05e73f6d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(476,)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E04Z8sUm3rls"
      },
      "source": [
        "### Now lets train it without augmentation case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PNW5JBYoe9Up",
        "outputId": "2e69bfd7-19e2-452e-df35-6d814b259bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (476, 20)\n",
            "Labels shape: (476,)\n",
            "X_train shape: torch.Size([380, 20])\n",
            "X_test shape: torch.Size([96, 20])\n",
            "y_train shape: torch.Size([380])\n",
            "y_test shape: torch.Size([96])\n",
            "Epoch 1/100, Training Loss: 1.6457648277282715, Validation Loss: 0.8403990268707275\n",
            "Epoch 2/100, Training Loss: 0.7689220905303955, Validation Loss: 1.2592467069625854\n",
            "Epoch 3/100, Training Loss: 1.0845003128051758, Validation Loss: 1.0327656269073486\n",
            "Epoch 4/100, Training Loss: 0.8424261808395386, Validation Loss: 0.6402109861373901\n",
            "Epoch 5/100, Training Loss: 0.4935280680656433, Validation Loss: 0.5794382691383362\n",
            "Epoch 6/100, Training Loss: 0.5036900639533997, Validation Loss: 0.6144846677780151\n",
            "Epoch 7/100, Training Loss: 0.5847957134246826, Validation Loss: 0.5792694091796875\n",
            "Epoch 8/100, Training Loss: 0.5591627955436707, Validation Loss: 0.48398062586784363\n",
            "Epoch 9/100, Training Loss: 0.44581884145736694, Validation Loss: 0.4062565565109253\n",
            "Epoch 10/100, Training Loss: 0.3333069980144501, Validation Loss: 0.4123198688030243\n",
            "Epoch 11/100, Training Loss: 0.29681116342544556, Validation Loss: 0.48302116990089417\n",
            "Epoch 12/100, Training Loss: 0.328986257314682, Validation Loss: 0.539378821849823\n",
            "Epoch 13/100, Training Loss: 0.36594346165657043, Validation Loss: 0.5272305011749268\n",
            "Epoch 14/100, Training Loss: 0.3591545820236206, Validation Loss: 0.46234241127967834\n",
            "Epoch 15/100, Training Loss: 0.31690818071365356, Validation Loss: 0.3922771215438843\n",
            "Epoch 16/100, Training Loss: 0.2734927237033844, Validation Loss: 0.346703439950943\n",
            "Epoch 17/100, Training Loss: 0.25034454464912415, Validation Loss: 0.3307483196258545\n",
            "Epoch 18/100, Training Loss: 0.2507210671901703, Validation Loss: 0.3335585296154022\n",
            "Epoch 19/100, Training Loss: 0.264797568321228, Validation Loss: 0.33914825320243835\n",
            "Epoch 20/100, Training Loss: 0.27594223618507385, Validation Loss: 0.3367236852645874\n",
            "Epoch 21/100, Training Loss: 0.2724774479866028, Validation Loss: 0.32674095034599304\n",
            "Epoch 22/100, Training Loss: 0.25595158338546753, Validation Loss: 0.31740978360176086\n",
            "Epoch 23/100, Training Loss: 0.23697365820407867, Validation Loss: 0.31653085350990295\n",
            "Epoch 24/100, Training Loss: 0.22562400996685028, Validation Loss: 0.3254129886627197\n",
            "Epoch 25/100, Training Loss: 0.22474947571754456, Validation Loss: 0.33858075737953186\n",
            "Epoch 26/100, Training Loss: 0.23027412593364716, Validation Loss: 0.3477003872394562\n",
            "Epoch 27/100, Training Loss: 0.23494142293930054, Validation Loss: 0.34734097123146057\n",
            "Epoch 28/100, Training Loss: 0.23396527767181396, Validation Loss: 0.3369835913181305\n",
            "Epoch 29/100, Training Loss: 0.22690936923027039, Validation Loss: 0.32018405199050903\n",
            "Epoch 30/100, Training Loss: 0.21713466942310333, Validation Loss: 0.3034210205078125\n",
            "Epoch 31/100, Training Loss: 0.20911690592765808, Validation Loss: 0.29185789823532104\n",
            "Epoch 32/100, Training Loss: 0.20619547367095947, Validation Loss: 0.28612425923347473\n",
            "Epoch 33/100, Training Loss: 0.2077857106924057, Validation Loss: 0.28325578570365906\n",
            "Epoch 34/100, Training Loss: 0.20982125401496887, Validation Loss: 0.2802939713001251\n",
            "Epoch 35/100, Training Loss: 0.2083461582660675, Validation Loss: 0.27665016055107117\n",
            "Epoch 36/100, Training Loss: 0.20271198451519012, Validation Loss: 0.2740347981452942\n",
            "Epoch 37/100, Training Loss: 0.1957959085702896, Validation Loss: 0.2744767963886261\n",
            "Epoch 38/100, Training Loss: 0.19097109138965607, Validation Loss: 0.2776826024055481\n",
            "Epoch 39/100, Training Loss: 0.1892545372247696, Validation Loss: 0.2811049520969391\n",
            "Epoch 40/100, Training Loss: 0.18892277777194977, Validation Loss: 0.281537264585495\n",
            "Epoch 41/100, Training Loss: 0.18777887523174286, Validation Loss: 0.27734997868537903\n",
            "Epoch 42/100, Training Loss: 0.18470309674739838, Validation Loss: 0.26947924494743347\n",
            "Epoch 43/100, Training Loss: 0.18010209500789642, Validation Loss: 0.260558545589447\n",
            "Epoch 44/100, Training Loss: 0.1756390780210495, Validation Loss: 0.25304463505744934\n",
            "Epoch 45/100, Training Loss: 0.17277488112449646, Validation Loss: 0.24809382855892181\n",
            "Epoch 46/100, Training Loss: 0.17152802646160126, Validation Loss: 0.245084747672081\n",
            "Epoch 47/100, Training Loss: 0.1704714298248291, Validation Loss: 0.24238593876361847\n",
            "Epoch 48/100, Training Loss: 0.16824959218502045, Validation Loss: 0.24015402793884277\n",
            "Epoch 49/100, Training Loss: 0.16498978435993195, Validation Loss: 0.23915345966815948\n",
            "Epoch 50/100, Training Loss: 0.16159489750862122, Validation Loss: 0.23953591287136078\n",
            "Epoch 51/100, Training Loss: 0.15911926329135895, Validation Loss: 0.24042515456676483\n",
            "Epoch 52/100, Training Loss: 0.15764355659484863, Validation Loss: 0.24037419259548187\n",
            "Epoch 53/100, Training Loss: 0.15662117302417755, Validation Loss: 0.2383907437324524\n",
            "Epoch 54/100, Training Loss: 0.15502925217151642, Validation Loss: 0.2344677448272705\n",
            "Epoch 55/100, Training Loss: 0.1529068946838379, Validation Loss: 0.22974413633346558\n",
            "Epoch 56/100, Training Loss: 0.15081357955932617, Validation Loss: 0.2253788709640503\n",
            "Epoch 57/100, Training Loss: 0.14933426678180695, Validation Loss: 0.22200365364551544\n",
            "Epoch 58/100, Training Loss: 0.14843299984931946, Validation Loss: 0.21942251920700073\n",
            "Epoch 59/100, Training Loss: 0.14730961620807648, Validation Loss: 0.21757692098617554\n",
            "Epoch 60/100, Training Loss: 0.14564238488674164, Validation Loss: 0.2164539098739624\n",
            "Epoch 61/100, Training Loss: 0.14380688965320587, Validation Loss: 0.21595215797424316\n",
            "Epoch 62/100, Training Loss: 0.14229415357112885, Validation Loss: 0.21561898291110992\n",
            "Epoch 63/100, Training Loss: 0.14116264879703522, Validation Loss: 0.21457664668560028\n",
            "Epoch 64/100, Training Loss: 0.14006027579307556, Validation Loss: 0.2123175412416458\n",
            "Epoch 65/100, Training Loss: 0.1386728286743164, Validation Loss: 0.20910978317260742\n",
            "Epoch 66/100, Training Loss: 0.13707897067070007, Validation Loss: 0.20573388040065765\n",
            "Epoch 67/100, Training Loss: 0.13570933043956757, Validation Loss: 0.2028685212135315\n",
            "Epoch 68/100, Training Loss: 0.13466864824295044, Validation Loss: 0.20067530870437622\n",
            "Epoch 69/100, Training Loss: 0.13363736867904663, Validation Loss: 0.19912123680114746\n",
            "Epoch 70/100, Training Loss: 0.13236600160598755, Validation Loss: 0.1981617957353592\n",
            "Epoch 71/100, Training Loss: 0.13099117577075958, Validation Loss: 0.1976059079170227\n",
            "Epoch 72/100, Training Loss: 0.129779651761055, Validation Loss: 0.19695109128952026\n",
            "Epoch 73/100, Training Loss: 0.12873978912830353, Validation Loss: 0.19568483531475067\n",
            "Epoch 74/100, Training Loss: 0.1276274174451828, Validation Loss: 0.19377367198467255\n",
            "Epoch 75/100, Training Loss: 0.12638306617736816, Validation Loss: 0.19159628450870514\n",
            "Epoch 76/100, Training Loss: 0.1251741200685501, Validation Loss: 0.1896173506975174\n",
            "Epoch 77/100, Training Loss: 0.12410857528448105, Validation Loss: 0.18805022537708282\n",
            "Epoch 78/100, Training Loss: 0.12307196855545044, Validation Loss: 0.18690679967403412\n",
            "Epoch 79/100, Training Loss: 0.12194917351007462, Validation Loss: 0.186123326420784\n",
            "Epoch 80/100, Training Loss: 0.12078379839658737, Validation Loss: 0.18562375009059906\n",
            "Epoch 81/100, Training Loss: 0.11974058300256729, Validation Loss: 0.18506784737110138\n",
            "Epoch 82/100, Training Loss: 0.11878979206085205, Validation Loss: 0.1841970831155777\n",
            "Epoch 83/100, Training Loss: 0.1177997961640358, Validation Loss: 0.18304429948329926\n",
            "Epoch 84/100, Training Loss: 0.11677207797765732, Validation Loss: 0.18184451758861542\n",
            "Epoch 85/100, Training Loss: 0.11584565043449402, Validation Loss: 0.18063139915466309\n",
            "Epoch 86/100, Training Loss: 0.11493241041898727, Validation Loss: 0.17945881187915802\n",
            "Epoch 87/100, Training Loss: 0.11396726220846176, Validation Loss: 0.17826057970523834\n",
            "Epoch 88/100, Training Loss: 0.1129407063126564, Validation Loss: 0.17719309031963348\n",
            "Epoch 89/100, Training Loss: 0.11204104125499725, Validation Loss: 0.17598176002502441\n",
            "Epoch 90/100, Training Loss: 0.11118577420711517, Validation Loss: 0.17464393377304077\n",
            "Epoch 91/100, Training Loss: 0.11033084988594055, Validation Loss: 0.17320097982883453\n",
            "Epoch 92/100, Training Loss: 0.10948148369789124, Validation Loss: 0.1718178540468216\n",
            "Epoch 93/100, Training Loss: 0.10865512490272522, Validation Loss: 0.170648455619812\n",
            "Epoch 94/100, Training Loss: 0.10783735662698746, Validation Loss: 0.1697327345609665\n",
            "Epoch 95/100, Training Loss: 0.1069931611418724, Validation Loss: 0.16907159984111786\n",
            "Epoch 96/100, Training Loss: 0.10615333914756775, Validation Loss: 0.16858844459056854\n",
            "Epoch 97/100, Training Loss: 0.10536319017410278, Validation Loss: 0.16816331446170807\n",
            "Epoch 98/100, Training Loss: 0.10462009906768799, Validation Loss: 0.16761980950832367\n",
            "Epoch 99/100, Training Loss: 0.10388940572738647, Validation Loss: 0.16682685911655426\n",
            "Epoch 100/100, Training Loss: 0.10314033925533295, Validation Loss: 0.1658271849155426\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 91.66666666666666%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.load('/content/features.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "    raise ValueError(\"The features or labels are empty. Please check the data preprocessing step.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "class WakeWordModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WakeWordModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = WakeWordModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "num_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_test)\n",
        "        val_loss = criterion(val_outputs, y_test)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f'Accuracy: {accuracy * 100}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test.cpu(), predicted.cpu())\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAXUzqSl308c"
      },
      "source": [
        "Ok lets enhance our results by addding some augmentation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz1txmCKAxZB"
      },
      "source": [
        "### Adding augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZUslaRI9el4",
        "outputId": "e89d0c76-269b-4a92-b889-c8bb413e3dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features for 1904 augmented files.\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def preprocess_audio(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    y = librosa.util.normalize(y)\n",
        "\n",
        "    y_trimmed, _ = librosa.effects.trim(y)\n",
        "\n",
        "    return y_trimmed, sr\n",
        "\n",
        "def extract_features(y, sr):\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "    return np.mean(mfccs.T, axis=0)\n",
        "\n",
        "def data_augmentation(y, sr):\n",
        "    y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=2)\n",
        "\n",
        "    y_speed = librosa.effects.time_stretch(y, rate=1.5)\n",
        "\n",
        "    noise = np.random.randn(len(y))\n",
        "    y_noisy = y + 0.005 * noise\n",
        "\n",
        "    return y_shifted, y_speed, y_noisy\n",
        "\n",
        "audio_dir = \"/content/alexa_data/\"\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for label_type in ['positive', 'negative']:\n",
        "    class_label = 1 if label_type == 'positive' else 0\n",
        "    subdir = os.path.join(audio_dir, label_type)\n",
        "\n",
        "    for file_name in os.listdir(subdir):\n",
        "        if file_name.endswith(\".wav\") and not file_name.startswith(\"._\"):\n",
        "            audio_path = os.path.join(subdir, file_name)\n",
        "\n",
        "            y, sr = preprocess_audio(audio_path)\n",
        "            y_shifted, y_speed, y_noisy = data_augmentation(y, sr)\n",
        "\n",
        "            mfccs_original = extract_features(y, sr)\n",
        "            mfccs_shifted = extract_features(y_shifted, sr)\n",
        "            mfccs_speed = extract_features(y_speed, sr)\n",
        "            mfccs_noisy = extract_features(y_noisy, sr)\n",
        "\n",
        "            features.append(mfccs_original)\n",
        "            features.append(mfccs_shifted)\n",
        "            features.append(mfccs_speed)\n",
        "            features.append(mfccs_noisy)\n",
        "\n",
        "            labels.extend([class_label] * 4)\n",
        "\n",
        "if features:\n",
        "    features = np.array(features)\n",
        "    labels = np.array(labels)\n",
        "    np.savez('/content/features.npz', features=features, labels=labels)\n",
        "    print(f\"Extracted features for {len(features)} augmented files.\")\n",
        "else:\n",
        "    print(\"No features or labels extracted. Please check the input data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "up3T0MmuAqY-",
        "outputId": "22857117-814d-4bb5-c573-e59f6dd11ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (1904, 20)\n",
            "Labels shape: (1904,)\n",
            "X_train shape: torch.Size([1713, 20])\n",
            "X_test shape: torch.Size([191, 20])\n",
            "y_train shape: torch.Size([1713])\n",
            "y_test shape: torch.Size([191])\n",
            "Epoch 1/2500, Training Loss: 2.838853120803833, Validation Loss: 2.7439234256744385\n",
            "Epoch 2/2500, Training Loss: 2.6297950744628906, Validation Loss: 2.5337488651275635\n",
            "Epoch 3/2500, Training Loss: 2.4346752166748047, Validation Loss: 2.337768316268921\n",
            "Epoch 4/2500, Training Loss: 2.2553255558013916, Validation Loss: 2.157926321029663\n",
            "Epoch 5/2500, Training Loss: 2.0933892726898193, Validation Loss: 1.9959875345230103\n",
            "Epoch 6/2500, Training Loss: 1.950177550315857, Validation Loss: 1.853337049484253\n",
            "Epoch 7/2500, Training Loss: 1.8264176845550537, Validation Loss: 1.7307024002075195\n",
            "Epoch 8/2500, Training Loss: 1.7221437692642212, Validation Loss: 1.6280081272125244\n",
            "Epoch 9/2500, Training Loss: 1.6366362571716309, Validation Loss: 1.544326663017273\n",
            "Epoch 10/2500, Training Loss: 1.5685480833053589, Validation Loss: 1.4779739379882812\n",
            "Epoch 11/2500, Training Loss: 1.516037940979004, Validation Loss: 1.426735281944275\n",
            "Epoch 12/2500, Training Loss: 1.4769052267074585, Validation Loss: 1.388051986694336\n",
            "Epoch 13/2500, Training Loss: 1.4487452507019043, Validation Loss: 1.3592699766159058\n",
            "Epoch 14/2500, Training Loss: 1.4290951490402222, Validation Loss: 1.3378722667694092\n",
            "Epoch 15/2500, Training Loss: 1.4156098365783691, Validation Loss: 1.321625828742981\n",
            "Epoch 16/2500, Training Loss: 1.4061640501022339, Validation Loss: 1.3086779117584229\n",
            "Epoch 17/2500, Training Loss: 1.3989310264587402, Validation Loss: 1.2974929809570312\n",
            "Epoch 18/2500, Training Loss: 1.392428994178772, Validation Loss: 1.286906123161316\n",
            "Epoch 19/2500, Training Loss: 1.3855124711990356, Validation Loss: 1.276065707206726\n",
            "Epoch 20/2500, Training Loss: 1.377366304397583, Validation Loss: 1.2643765211105347\n",
            "Epoch 21/2500, Training Loss: 1.3674477338790894, Validation Loss: 1.2514623403549194\n",
            "Epoch 22/2500, Training Loss: 1.355454683303833, Validation Loss: 1.2371726036071777\n",
            "Epoch 23/2500, Training Loss: 1.3412750959396362, Validation Loss: 1.2214765548706055\n",
            "Epoch 24/2500, Training Loss: 1.3249531984329224, Validation Loss: 1.204504370689392\n",
            "Epoch 25/2500, Training Loss: 1.306658148765564, Validation Loss: 1.1864076852798462\n",
            "Epoch 26/2500, Training Loss: 1.2866532802581787, Validation Loss: 1.1674193143844604\n",
            "Epoch 27/2500, Training Loss: 1.2652539014816284, Validation Loss: 1.147834300994873\n",
            "Epoch 28/2500, Training Loss: 1.2428185939788818, Validation Loss: 1.1279346942901611\n",
            "Epoch 29/2500, Training Loss: 1.2197265625, Validation Loss: 1.1080151796340942\n",
            "Epoch 30/2500, Training Loss: 1.1963516473770142, Validation Loss: 1.0883532762527466\n",
            "Epoch 31/2500, Training Loss: 1.1730583906173706, Validation Loss: 1.0692074298858643\n",
            "Epoch 32/2500, Training Loss: 1.1501753330230713, Validation Loss: 1.0507878065109253\n",
            "Epoch 33/2500, Training Loss: 1.1279798746109009, Validation Loss: 1.0332705974578857\n",
            "Epoch 34/2500, Training Loss: 1.1067060232162476, Validation Loss: 1.0167746543884277\n",
            "Epoch 35/2500, Training Loss: 1.0865354537963867, Validation Loss: 1.0013707876205444\n",
            "Epoch 36/2500, Training Loss: 1.0675525665283203, Validation Loss: 0.9870149493217468\n",
            "Epoch 37/2500, Training Loss: 1.0497896671295166, Validation Loss: 0.9735944867134094\n",
            "Epoch 38/2500, Training Loss: 1.0331817865371704, Validation Loss: 0.9609512686729431\n",
            "Epoch 39/2500, Training Loss: 1.0176128149032593, Validation Loss: 0.9489017724990845\n",
            "Epoch 40/2500, Training Loss: 1.0029141902923584, Validation Loss: 0.9372066259384155\n",
            "Epoch 41/2500, Training Loss: 0.988882839679718, Validation Loss: 0.925619900226593\n",
            "Epoch 42/2500, Training Loss: 0.9753018021583557, Validation Loss: 0.9139219522476196\n",
            "Epoch 43/2500, Training Loss: 0.9619464874267578, Validation Loss: 0.9019355773925781\n",
            "Epoch 44/2500, Training Loss: 0.9486331939697266, Validation Loss: 0.8895383477210999\n",
            "Epoch 45/2500, Training Loss: 0.9352088570594788, Validation Loss: 0.8766341805458069\n",
            "Epoch 46/2500, Training Loss: 0.9215698838233948, Validation Loss: 0.8631864190101624\n",
            "Epoch 47/2500, Training Loss: 0.907677948474884, Validation Loss: 0.8492605090141296\n",
            "Epoch 48/2500, Training Loss: 0.8935473561286926, Validation Loss: 0.8349500894546509\n",
            "Epoch 49/2500, Training Loss: 0.8792387247085571, Validation Loss: 0.8203768134117126\n",
            "Epoch 50/2500, Training Loss: 0.8648374676704407, Validation Loss: 0.8056846857070923\n",
            "Epoch 51/2500, Training Loss: 0.8504499197006226, Validation Loss: 0.7910197973251343\n",
            "Epoch 52/2500, Training Loss: 0.836189329624176, Validation Loss: 0.7765110731124878\n",
            "Epoch 53/2500, Training Loss: 0.8221650123596191, Validation Loss: 0.7622664570808411\n",
            "Epoch 54/2500, Training Loss: 0.8084551692008972, Validation Loss: 0.7483825087547302\n",
            "Epoch 55/2500, Training Loss: 0.7951162457466125, Validation Loss: 0.7349315285682678\n",
            "Epoch 56/2500, Training Loss: 0.7821766138076782, Validation Loss: 0.7219470739364624\n",
            "Epoch 57/2500, Training Loss: 0.7696380615234375, Validation Loss: 0.7094191312789917\n",
            "Epoch 58/2500, Training Loss: 0.7574881911277771, Validation Loss: 0.6973294019699097\n",
            "Epoch 59/2500, Training Loss: 0.7456931471824646, Validation Loss: 0.685655951499939\n",
            "Epoch 60/2500, Training Loss: 0.7342085242271423, Validation Loss: 0.6743555665016174\n",
            "Epoch 61/2500, Training Loss: 0.7229852080345154, Validation Loss: 0.6633998155593872\n",
            "Epoch 62/2500, Training Loss: 0.7119777798652649, Validation Loss: 0.6527502536773682\n",
            "Epoch 63/2500, Training Loss: 0.701148509979248, Validation Loss: 0.6423638463020325\n",
            "Epoch 64/2500, Training Loss: 0.6904693245887756, Validation Loss: 0.6322280764579773\n",
            "Epoch 65/2500, Training Loss: 0.6799339056015015, Validation Loss: 0.6223256587982178\n",
            "Epoch 66/2500, Training Loss: 0.6695333123207092, Validation Loss: 0.6126331686973572\n",
            "Epoch 67/2500, Training Loss: 0.6592822074890137, Validation Loss: 0.6031695604324341\n",
            "Epoch 68/2500, Training Loss: 0.649195671081543, Validation Loss: 0.5939294099807739\n",
            "Epoch 69/2500, Training Loss: 0.6392848491668701, Validation Loss: 0.5849186182022095\n",
            "Epoch 70/2500, Training Loss: 0.6295729875564575, Validation Loss: 0.576166570186615\n",
            "Epoch 71/2500, Training Loss: 0.6200728416442871, Validation Loss: 0.5676536560058594\n",
            "Epoch 72/2500, Training Loss: 0.610798180103302, Validation Loss: 0.5593644976615906\n",
            "Epoch 73/2500, Training Loss: 0.6017563343048096, Validation Loss: 0.5512974262237549\n",
            "Epoch 74/2500, Training Loss: 0.5929388999938965, Validation Loss: 0.5434385538101196\n",
            "Epoch 75/2500, Training Loss: 0.5843429565429688, Validation Loss: 0.5357862710952759\n",
            "Epoch 76/2500, Training Loss: 0.5759655833244324, Validation Loss: 0.5283145308494568\n",
            "Epoch 77/2500, Training Loss: 0.567797064781189, Validation Loss: 0.5209966897964478\n",
            "Epoch 78/2500, Training Loss: 0.5598249435424805, Validation Loss: 0.5138206481933594\n",
            "Epoch 79/2500, Training Loss: 0.5520356893539429, Validation Loss: 0.5067759156227112\n",
            "Epoch 80/2500, Training Loss: 0.5444158911705017, Validation Loss: 0.49985185265541077\n",
            "Epoch 81/2500, Training Loss: 0.5369591116905212, Validation Loss: 0.49304407835006714\n",
            "Epoch 82/2500, Training Loss: 0.5296593904495239, Validation Loss: 0.4863651990890503\n",
            "Epoch 83/2500, Training Loss: 0.5225111246109009, Validation Loss: 0.47982120513916016\n",
            "Epoch 84/2500, Training Loss: 0.5155150890350342, Validation Loss: 0.4733985364437103\n",
            "Epoch 85/2500, Training Loss: 0.5086738467216492, Validation Loss: 0.467093825340271\n",
            "Epoch 86/2500, Training Loss: 0.5019851326942444, Validation Loss: 0.46093323826789856\n",
            "Epoch 87/2500, Training Loss: 0.4954502582550049, Validation Loss: 0.4549087882041931\n",
            "Epoch 88/2500, Training Loss: 0.4890691339969635, Validation Loss: 0.4490140378475189\n",
            "Epoch 89/2500, Training Loss: 0.48284074664115906, Validation Loss: 0.4432700276374817\n",
            "Epoch 90/2500, Training Loss: 0.4767613112926483, Validation Loss: 0.4376787543296814\n",
            "Epoch 91/2500, Training Loss: 0.47082850337028503, Validation Loss: 0.43224474787712097\n",
            "Epoch 92/2500, Training Loss: 0.4650435447692871, Validation Loss: 0.42696747183799744\n",
            "Epoch 93/2500, Training Loss: 0.4594017267227173, Validation Loss: 0.4218405783176422\n",
            "Epoch 94/2500, Training Loss: 0.4538942277431488, Validation Loss: 0.41685569286346436\n",
            "Epoch 95/2500, Training Loss: 0.4485153257846832, Validation Loss: 0.4120137393474579\n",
            "Epoch 96/2500, Training Loss: 0.4432646334171295, Validation Loss: 0.4073103964328766\n",
            "Epoch 97/2500, Training Loss: 0.4381397068500519, Validation Loss: 0.4027436375617981\n",
            "Epoch 98/2500, Training Loss: 0.43313491344451904, Validation Loss: 0.3983076810836792\n",
            "Epoch 99/2500, Training Loss: 0.42824670672416687, Validation Loss: 0.39399364590644836\n",
            "Epoch 100/2500, Training Loss: 0.4234723746776581, Validation Loss: 0.3897968828678131\n",
            "Epoch 101/2500, Training Loss: 0.41881024837493896, Validation Loss: 0.3857225477695465\n",
            "Epoch 102/2500, Training Loss: 0.4142608642578125, Validation Loss: 0.3817726969718933\n",
            "Epoch 103/2500, Training Loss: 0.40981850028038025, Validation Loss: 0.3779330551624298\n",
            "Epoch 104/2500, Training Loss: 0.40548059344291687, Validation Loss: 0.3742011487483978\n",
            "Epoch 105/2500, Training Loss: 0.4012507498264313, Validation Loss: 0.37057310342788696\n",
            "Epoch 106/2500, Training Loss: 0.39712485671043396, Validation Loss: 0.36704230308532715\n",
            "Epoch 107/2500, Training Loss: 0.39310023188591003, Validation Loss: 0.3636034429073334\n",
            "Epoch 108/2500, Training Loss: 0.3891734182834625, Validation Loss: 0.3602515459060669\n",
            "Epoch 109/2500, Training Loss: 0.3853435218334198, Validation Loss: 0.356985867023468\n",
            "Epoch 110/2500, Training Loss: 0.38160768151283264, Validation Loss: 0.3538050949573517\n",
            "Epoch 111/2500, Training Loss: 0.37796133756637573, Validation Loss: 0.35070449113845825\n",
            "Epoch 112/2500, Training Loss: 0.37439796328544617, Validation Loss: 0.34767937660217285\n",
            "Epoch 113/2500, Training Loss: 0.37091952562332153, Validation Loss: 0.3447309136390686\n",
            "Epoch 114/2500, Training Loss: 0.3675227165222168, Validation Loss: 0.3418537974357605\n",
            "Epoch 115/2500, Training Loss: 0.364204078912735, Validation Loss: 0.3390466570854187\n",
            "Epoch 116/2500, Training Loss: 0.36095860600471497, Validation Loss: 0.3363111913204193\n",
            "Epoch 117/2500, Training Loss: 0.35778602957725525, Validation Loss: 0.33364665508270264\n",
            "Epoch 118/2500, Training Loss: 0.35468152165412903, Validation Loss: 0.33104485273361206\n",
            "Epoch 119/2500, Training Loss: 0.35164597630500793, Validation Loss: 0.3285083770751953\n",
            "Epoch 120/2500, Training Loss: 0.34867942333221436, Validation Loss: 0.3260369896888733\n",
            "Epoch 121/2500, Training Loss: 0.3457832634449005, Validation Loss: 0.32363006472587585\n",
            "Epoch 122/2500, Training Loss: 0.3429482877254486, Validation Loss: 0.32127609848976135\n",
            "Epoch 123/2500, Training Loss: 0.34017637372016907, Validation Loss: 0.31897786259651184\n",
            "Epoch 124/2500, Training Loss: 0.337466299533844, Validation Loss: 0.31673482060432434\n",
            "Epoch 125/2500, Training Loss: 0.3348131477832794, Validation Loss: 0.31455206871032715\n",
            "Epoch 126/2500, Training Loss: 0.33221951127052307, Validation Loss: 0.3124271035194397\n",
            "Epoch 127/2500, Training Loss: 0.32968178391456604, Validation Loss: 0.31035250425338745\n",
            "Epoch 128/2500, Training Loss: 0.3271978497505188, Validation Loss: 0.3083294630050659\n",
            "Epoch 129/2500, Training Loss: 0.32476529479026794, Validation Loss: 0.30635660886764526\n",
            "Epoch 130/2500, Training Loss: 0.3223794400691986, Validation Loss: 0.30443716049194336\n",
            "Epoch 131/2500, Training Loss: 0.32004326581954956, Validation Loss: 0.30257049202919006\n",
            "Epoch 132/2500, Training Loss: 0.3177558183670044, Validation Loss: 0.30075499415397644\n",
            "Epoch 133/2500, Training Loss: 0.31551679968833923, Validation Loss: 0.29898592829704285\n",
            "Epoch 134/2500, Training Loss: 0.3133260905742645, Validation Loss: 0.29725879430770874\n",
            "Epoch 135/2500, Training Loss: 0.31118422746658325, Validation Loss: 0.2955707013607025\n",
            "Epoch 136/2500, Training Loss: 0.30908969044685364, Validation Loss: 0.2939278781414032\n",
            "Epoch 137/2500, Training Loss: 0.30704089999198914, Validation Loss: 0.29233676195144653\n",
            "Epoch 138/2500, Training Loss: 0.305038720369339, Validation Loss: 0.29079118371009827\n",
            "Epoch 139/2500, Training Loss: 0.3030809462070465, Validation Loss: 0.28928565979003906\n",
            "Epoch 140/2500, Training Loss: 0.30116355419158936, Validation Loss: 0.2878182530403137\n",
            "Epoch 141/2500, Training Loss: 0.29928621649742126, Validation Loss: 0.28638607263565063\n",
            "Epoch 142/2500, Training Loss: 0.2974494397640228, Validation Loss: 0.2849803864955902\n",
            "Epoch 143/2500, Training Loss: 0.295651912689209, Validation Loss: 0.28360769152641296\n",
            "Epoch 144/2500, Training Loss: 0.29389098286628723, Validation Loss: 0.2822728753089905\n",
            "Epoch 145/2500, Training Loss: 0.29216673970222473, Validation Loss: 0.28096890449523926\n",
            "Epoch 146/2500, Training Loss: 0.2904793918132782, Validation Loss: 0.2796970307826996\n",
            "Epoch 147/2500, Training Loss: 0.28882524371147156, Validation Loss: 0.2784537971019745\n",
            "Epoch 148/2500, Training Loss: 0.28720200061798096, Validation Loss: 0.27723830938339233\n",
            "Epoch 149/2500, Training Loss: 0.2856096625328064, Validation Loss: 0.27605852484703064\n",
            "Epoch 150/2500, Training Loss: 0.28404656052589417, Validation Loss: 0.27490782737731934\n",
            "Epoch 151/2500, Training Loss: 0.2825140058994293, Validation Loss: 0.2737853527069092\n",
            "Epoch 152/2500, Training Loss: 0.2810101807117462, Validation Loss: 0.27269110083580017\n",
            "Epoch 153/2500, Training Loss: 0.27953559160232544, Validation Loss: 0.2716236412525177\n",
            "Epoch 154/2500, Training Loss: 0.278086394071579, Validation Loss: 0.2705822288990021\n",
            "Epoch 155/2500, Training Loss: 0.2766624391078949, Validation Loss: 0.26956459879875183\n",
            "Epoch 156/2500, Training Loss: 0.27526429295539856, Validation Loss: 0.2685621380805969\n",
            "Epoch 157/2500, Training Loss: 0.2738909125328064, Validation Loss: 0.2675793170928955\n",
            "Epoch 158/2500, Training Loss: 0.27254003286361694, Validation Loss: 0.2666194438934326\n",
            "Epoch 159/2500, Training Loss: 0.2712109386920929, Validation Loss: 0.26568225026130676\n",
            "Epoch 160/2500, Training Loss: 0.26990485191345215, Validation Loss: 0.2647668123245239\n",
            "Epoch 161/2500, Training Loss: 0.26862001419067383, Validation Loss: 0.26387226581573486\n",
            "Epoch 162/2500, Training Loss: 0.26735714077949524, Validation Loss: 0.26299813389778137\n",
            "Epoch 163/2500, Training Loss: 0.26611408591270447, Validation Loss: 0.2621445655822754\n",
            "Epoch 164/2500, Training Loss: 0.26489052176475525, Validation Loss: 0.2613103985786438\n",
            "Epoch 165/2500, Training Loss: 0.26368436217308044, Validation Loss: 0.26049402356147766\n",
            "Epoch 166/2500, Training Loss: 0.26249632239341736, Validation Loss: 0.2596912980079651\n",
            "Epoch 167/2500, Training Loss: 0.2613271176815033, Validation Loss: 0.25889694690704346\n",
            "Epoch 168/2500, Training Loss: 0.2601761221885681, Validation Loss: 0.25811871886253357\n",
            "Epoch 169/2500, Training Loss: 0.25904202461242676, Validation Loss: 0.2573622167110443\n",
            "Epoch 170/2500, Training Loss: 0.2579229772090912, Validation Loss: 0.25663554668426514\n",
            "Epoch 171/2500, Training Loss: 0.2568207085132599, Validation Loss: 0.25592732429504395\n",
            "Epoch 172/2500, Training Loss: 0.2557346820831299, Validation Loss: 0.25523823499679565\n",
            "Epoch 173/2500, Training Loss: 0.2546655833721161, Validation Loss: 0.25456270575523376\n",
            "Epoch 174/2500, Training Loss: 0.25361326336860657, Validation Loss: 0.25389766693115234\n",
            "Epoch 175/2500, Training Loss: 0.25257810950279236, Validation Loss: 0.2532380521297455\n",
            "Epoch 176/2500, Training Loss: 0.2515585720539093, Validation Loss: 0.25259026885032654\n",
            "Epoch 177/2500, Training Loss: 0.2505541741847992, Validation Loss: 0.25195443630218506\n",
            "Epoch 178/2500, Training Loss: 0.24956300854682922, Validation Loss: 0.2513349950313568\n",
            "Epoch 179/2500, Training Loss: 0.248585045337677, Validation Loss: 0.25072622299194336\n",
            "Epoch 180/2500, Training Loss: 0.2476198673248291, Validation Loss: 0.2501274347305298\n",
            "Epoch 181/2500, Training Loss: 0.24666808545589447, Validation Loss: 0.24953967332839966\n",
            "Epoch 182/2500, Training Loss: 0.24572932720184326, Validation Loss: 0.24896275997161865\n",
            "Epoch 183/2500, Training Loss: 0.2448040097951889, Validation Loss: 0.24839632213115692\n",
            "Epoch 184/2500, Training Loss: 0.24389150738716125, Validation Loss: 0.24784035980701447\n",
            "Epoch 185/2500, Training Loss: 0.24299223721027374, Validation Loss: 0.2472936362028122\n",
            "Epoch 186/2500, Training Loss: 0.24210523068904877, Validation Loss: 0.2467631846666336\n",
            "Epoch 187/2500, Training Loss: 0.24122968316078186, Validation Loss: 0.24624252319335938\n",
            "Epoch 188/2500, Training Loss: 0.2403630167245865, Validation Loss: 0.2457296997308731\n",
            "Epoch 189/2500, Training Loss: 0.23950743675231934, Validation Loss: 0.2452240139245987\n",
            "Epoch 190/2500, Training Loss: 0.23866139352321625, Validation Loss: 0.24472591280937195\n",
            "Epoch 191/2500, Training Loss: 0.2378242164850235, Validation Loss: 0.24423342943191528\n",
            "Epoch 192/2500, Training Loss: 0.2369973510503769, Validation Loss: 0.2437448352575302\n",
            "Epoch 193/2500, Training Loss: 0.23618102073669434, Validation Loss: 0.24326536059379578\n",
            "Epoch 194/2500, Training Loss: 0.23537559807300568, Validation Loss: 0.24279603362083435\n",
            "Epoch 195/2500, Training Loss: 0.23457711935043335, Validation Loss: 0.24233396351337433\n",
            "Epoch 196/2500, Training Loss: 0.23378676176071167, Validation Loss: 0.2418825775384903\n",
            "Epoch 197/2500, Training Loss: 0.23300310969352722, Validation Loss: 0.2414391189813614\n",
            "Epoch 198/2500, Training Loss: 0.23222699761390686, Validation Loss: 0.24100248515605927\n",
            "Epoch 199/2500, Training Loss: 0.23145627975463867, Validation Loss: 0.2405683547258377\n",
            "Epoch 200/2500, Training Loss: 0.2306918501853943, Validation Loss: 0.24013297259807587\n",
            "Epoch 201/2500, Training Loss: 0.22993440926074982, Validation Loss: 0.23970337212085724\n",
            "Epoch 202/2500, Training Loss: 0.22918276488780975, Validation Loss: 0.23925724625587463\n",
            "Epoch 203/2500, Training Loss: 0.22843700647354126, Validation Loss: 0.23877529799938202\n",
            "Epoch 204/2500, Training Loss: 0.22769711911678314, Validation Loss: 0.23829586803913116\n",
            "Epoch 205/2500, Training Loss: 0.22696270048618317, Validation Loss: 0.23780815303325653\n",
            "Epoch 206/2500, Training Loss: 0.22623474895954132, Validation Loss: 0.23732034862041473\n",
            "Epoch 207/2500, Training Loss: 0.22551308572292328, Validation Loss: 0.23683735728263855\n",
            "Epoch 208/2500, Training Loss: 0.22479873895645142, Validation Loss: 0.2363588660955429\n",
            "Epoch 209/2500, Training Loss: 0.22408942878246307, Validation Loss: 0.23588022589683533\n",
            "Epoch 210/2500, Training Loss: 0.2233850210905075, Validation Loss: 0.23540440201759338\n",
            "Epoch 211/2500, Training Loss: 0.22268477082252502, Validation Loss: 0.23493008315563202\n",
            "Epoch 212/2500, Training Loss: 0.2219909429550171, Validation Loss: 0.23446601629257202\n",
            "Epoch 213/2500, Training Loss: 0.22129856050014496, Validation Loss: 0.2340070903301239\n",
            "Epoch 214/2500, Training Loss: 0.22060707211494446, Validation Loss: 0.23355358839035034\n",
            "Epoch 215/2500, Training Loss: 0.2199200540781021, Validation Loss: 0.23307432234287262\n",
            "Epoch 216/2500, Training Loss: 0.21925078332424164, Validation Loss: 0.23258456587791443\n",
            "Epoch 217/2500, Training Loss: 0.2185906618833542, Validation Loss: 0.23209808766841888\n",
            "Epoch 218/2500, Training Loss: 0.21794205904006958, Validation Loss: 0.23161520063877106\n",
            "Epoch 219/2500, Training Loss: 0.2173040211200714, Validation Loss: 0.23110999166965485\n",
            "Epoch 220/2500, Training Loss: 0.21667206287384033, Validation Loss: 0.2306104153394699\n",
            "Epoch 221/2500, Training Loss: 0.216043621301651, Validation Loss: 0.23012344539165497\n",
            "Epoch 222/2500, Training Loss: 0.21542027592658997, Validation Loss: 0.2296498715877533\n",
            "Epoch 223/2500, Training Loss: 0.21480384469032288, Validation Loss: 0.2291736751794815\n",
            "Epoch 224/2500, Training Loss: 0.21419568359851837, Validation Loss: 0.2287091165781021\n",
            "Epoch 225/2500, Training Loss: 0.21359355747699738, Validation Loss: 0.22825253009796143\n",
            "Epoch 226/2500, Training Loss: 0.21299749612808228, Validation Loss: 0.22779659926891327\n",
            "Epoch 227/2500, Training Loss: 0.21240609884262085, Validation Loss: 0.2273435890674591\n",
            "Epoch 228/2500, Training Loss: 0.21181784570217133, Validation Loss: 0.2268947809934616\n",
            "Epoch 229/2500, Training Loss: 0.21123583614826202, Validation Loss: 0.22642917931079865\n",
            "Epoch 230/2500, Training Loss: 0.21066099405288696, Validation Loss: 0.22596940398216248\n",
            "Epoch 231/2500, Training Loss: 0.21008948981761932, Validation Loss: 0.22551238536834717\n",
            "Epoch 232/2500, Training Loss: 0.20951993763446808, Validation Loss: 0.22505594789981842\n",
            "Epoch 233/2500, Training Loss: 0.2089536488056183, Validation Loss: 0.22460179030895233\n",
            "Epoch 234/2500, Training Loss: 0.20839349925518036, Validation Loss: 0.22415359318256378\n",
            "Epoch 235/2500, Training Loss: 0.20783989131450653, Validation Loss: 0.22371254861354828\n",
            "Epoch 236/2500, Training Loss: 0.2072923630475998, Validation Loss: 0.22327516973018646\n",
            "Epoch 237/2500, Training Loss: 0.2067500650882721, Validation Loss: 0.22283923625946045\n",
            "Epoch 238/2500, Training Loss: 0.20621155202388763, Validation Loss: 0.2223946899175644\n",
            "Epoch 239/2500, Training Loss: 0.20567750930786133, Validation Loss: 0.2219538688659668\n",
            "Epoch 240/2500, Training Loss: 0.2051428258419037, Validation Loss: 0.22151365876197815\n",
            "Epoch 241/2500, Training Loss: 0.20460821688175201, Validation Loss: 0.22108295559883118\n",
            "Epoch 242/2500, Training Loss: 0.20407654345035553, Validation Loss: 0.22065190970897675\n",
            "Epoch 243/2500, Training Loss: 0.2035430669784546, Validation Loss: 0.22021867334842682\n",
            "Epoch 244/2500, Training Loss: 0.20301257073879242, Validation Loss: 0.21979130804538727\n",
            "Epoch 245/2500, Training Loss: 0.20248644053936005, Validation Loss: 0.21936611831188202\n",
            "Epoch 246/2500, Training Loss: 0.20196518301963806, Validation Loss: 0.2189420908689499\n",
            "Epoch 247/2500, Training Loss: 0.2014480084180832, Validation Loss: 0.21852177381515503\n",
            "Epoch 248/2500, Training Loss: 0.20093631744384766, Validation Loss: 0.2181074172258377\n",
            "Epoch 249/2500, Training Loss: 0.20042817294597626, Validation Loss: 0.21769648790359497\n",
            "Epoch 250/2500, Training Loss: 0.1999199539422989, Validation Loss: 0.21728597581386566\n",
            "Epoch 251/2500, Training Loss: 0.19941364228725433, Validation Loss: 0.21687616407871246\n",
            "Epoch 252/2500, Training Loss: 0.1989109367132187, Validation Loss: 0.21646840870380402\n",
            "Epoch 253/2500, Training Loss: 0.19840902090072632, Validation Loss: 0.21606279909610748\n",
            "Epoch 254/2500, Training Loss: 0.197911337018013, Validation Loss: 0.2156512439250946\n",
            "Epoch 255/2500, Training Loss: 0.19741640985012054, Validation Loss: 0.21524599194526672\n",
            "Epoch 256/2500, Training Loss: 0.19692394137382507, Validation Loss: 0.21484263241291046\n",
            "Epoch 257/2500, Training Loss: 0.19643457233905792, Validation Loss: 0.21443958580493927\n",
            "Epoch 258/2500, Training Loss: 0.19594675302505493, Validation Loss: 0.2140376716852188\n",
            "Epoch 259/2500, Training Loss: 0.19546128809452057, Validation Loss: 0.21363668143749237\n",
            "Epoch 260/2500, Training Loss: 0.1949772983789444, Validation Loss: 0.21323588490486145\n",
            "Epoch 261/2500, Training Loss: 0.19449810683727264, Validation Loss: 0.21283681690692902\n",
            "Epoch 262/2500, Training Loss: 0.19402481615543365, Validation Loss: 0.21243023872375488\n",
            "Epoch 263/2500, Training Loss: 0.19355420768260956, Validation Loss: 0.2120255082845688\n",
            "Epoch 264/2500, Training Loss: 0.19308781623840332, Validation Loss: 0.21162380278110504\n",
            "Epoch 265/2500, Training Loss: 0.19262482225894928, Validation Loss: 0.21121281385421753\n",
            "Epoch 266/2500, Training Loss: 0.19216281175613403, Validation Loss: 0.21079465746879578\n",
            "Epoch 267/2500, Training Loss: 0.1917026787996292, Validation Loss: 0.2103777974843979\n",
            "Epoch 268/2500, Training Loss: 0.19124726951122284, Validation Loss: 0.20995089411735535\n",
            "Epoch 269/2500, Training Loss: 0.19079698622226715, Validation Loss: 0.2095094621181488\n",
            "Epoch 270/2500, Training Loss: 0.19034940004348755, Validation Loss: 0.2090376764535904\n",
            "Epoch 271/2500, Training Loss: 0.1899031698703766, Validation Loss: 0.20855477452278137\n",
            "Epoch 272/2500, Training Loss: 0.18946021795272827, Validation Loss: 0.20807184278964996\n",
            "Epoch 273/2500, Training Loss: 0.18901795148849487, Validation Loss: 0.20758509635925293\n",
            "Epoch 274/2500, Training Loss: 0.1885785609483719, Validation Loss: 0.2070898711681366\n",
            "Epoch 275/2500, Training Loss: 0.1881411224603653, Validation Loss: 0.20658446848392487\n",
            "Epoch 276/2500, Training Loss: 0.187705859541893, Validation Loss: 0.206085667014122\n",
            "Epoch 277/2500, Training Loss: 0.18727277219295502, Validation Loss: 0.20559628307819366\n",
            "Epoch 278/2500, Training Loss: 0.18684184551239014, Validation Loss: 0.20510675013065338\n",
            "Epoch 279/2500, Training Loss: 0.18641366064548492, Validation Loss: 0.20463749766349792\n",
            "Epoch 280/2500, Training Loss: 0.18598534166812897, Validation Loss: 0.20417334139347076\n",
            "Epoch 281/2500, Training Loss: 0.18556161224842072, Validation Loss: 0.2037140429019928\n",
            "Epoch 282/2500, Training Loss: 0.18514011800289154, Validation Loss: 0.20324614644050598\n",
            "Epoch 283/2500, Training Loss: 0.184721902012825, Validation Loss: 0.20278103649616241\n",
            "Epoch 284/2500, Training Loss: 0.18429990112781525, Validation Loss: 0.20231935381889343\n",
            "Epoch 285/2500, Training Loss: 0.1838817000389099, Validation Loss: 0.20186108350753784\n",
            "Epoch 286/2500, Training Loss: 0.183465838432312, Validation Loss: 0.20141026377677917\n",
            "Epoch 287/2500, Training Loss: 0.18305164575576782, Validation Loss: 0.20097126066684723\n",
            "Epoch 288/2500, Training Loss: 0.18264169991016388, Validation Loss: 0.200541689991951\n",
            "Epoch 289/2500, Training Loss: 0.18223468959331512, Validation Loss: 0.20011664927005768\n",
            "Epoch 290/2500, Training Loss: 0.18183021247386932, Validation Loss: 0.1996956467628479\n",
            "Epoch 291/2500, Training Loss: 0.18143104016780853, Validation Loss: 0.19928224384784698\n",
            "Epoch 292/2500, Training Loss: 0.18103653192520142, Validation Loss: 0.19888412952423096\n",
            "Epoch 293/2500, Training Loss: 0.18064691126346588, Validation Loss: 0.1984967738389969\n",
            "Epoch 294/2500, Training Loss: 0.1802634447813034, Validation Loss: 0.19811291992664337\n",
            "Epoch 295/2500, Training Loss: 0.17988333106040955, Validation Loss: 0.19774635136127472\n",
            "Epoch 296/2500, Training Loss: 0.17950588464736938, Validation Loss: 0.19738200306892395\n",
            "Epoch 297/2500, Training Loss: 0.17913170158863068, Validation Loss: 0.19701755046844482\n",
            "Epoch 298/2500, Training Loss: 0.1787596493959427, Validation Loss: 0.19665171205997467\n",
            "Epoch 299/2500, Training Loss: 0.17839035391807556, Validation Loss: 0.19628740847110748\n",
            "Epoch 300/2500, Training Loss: 0.17802366614341736, Validation Loss: 0.19592465460300446\n",
            "Epoch 301/2500, Training Loss: 0.17765793204307556, Validation Loss: 0.1955624520778656\n",
            "Epoch 302/2500, Training Loss: 0.177293062210083, Validation Loss: 0.1951994001865387\n",
            "Epoch 303/2500, Training Loss: 0.17692796885967255, Validation Loss: 0.19483582675457\n",
            "Epoch 304/2500, Training Loss: 0.17656423151493073, Validation Loss: 0.19447706639766693\n",
            "Epoch 305/2500, Training Loss: 0.1762017011642456, Validation Loss: 0.19411861896514893\n",
            "Epoch 306/2500, Training Loss: 0.17583860456943512, Validation Loss: 0.19375728070735931\n",
            "Epoch 307/2500, Training Loss: 0.1754767894744873, Validation Loss: 0.19339433312416077\n",
            "Epoch 308/2500, Training Loss: 0.17511588335037231, Validation Loss: 0.19303067028522491\n",
            "Epoch 309/2500, Training Loss: 0.17475587129592896, Validation Loss: 0.19266566634178162\n",
            "Epoch 310/2500, Training Loss: 0.17439348995685577, Validation Loss: 0.1923007220029831\n",
            "Epoch 311/2500, Training Loss: 0.17403197288513184, Validation Loss: 0.19193893671035767\n",
            "Epoch 312/2500, Training Loss: 0.1736721396446228, Validation Loss: 0.19158148765563965\n",
            "Epoch 313/2500, Training Loss: 0.17331518232822418, Validation Loss: 0.19122976064682007\n",
            "Epoch 314/2500, Training Loss: 0.17295970022678375, Validation Loss: 0.19088634848594666\n",
            "Epoch 315/2500, Training Loss: 0.17260821163654327, Validation Loss: 0.1905597448348999\n",
            "Epoch 316/2500, Training Loss: 0.1722579449415207, Validation Loss: 0.19023767113685608\n",
            "Epoch 317/2500, Training Loss: 0.171908900141716, Validation Loss: 0.18991798162460327\n",
            "Epoch 318/2500, Training Loss: 0.17156334221363068, Validation Loss: 0.1896091103553772\n",
            "Epoch 319/2500, Training Loss: 0.17122182250022888, Validation Loss: 0.1893114447593689\n",
            "Epoch 320/2500, Training Loss: 0.1708819717168808, Validation Loss: 0.18902480602264404\n",
            "Epoch 321/2500, Training Loss: 0.1705411672592163, Validation Loss: 0.1887436956167221\n",
            "Epoch 322/2500, Training Loss: 0.17020179331302643, Validation Loss: 0.18846800923347473\n",
            "Epoch 323/2500, Training Loss: 0.16986414790153503, Validation Loss: 0.18819671869277954\n",
            "Epoch 324/2500, Training Loss: 0.16952788829803467, Validation Loss: 0.18792787194252014\n",
            "Epoch 325/2500, Training Loss: 0.16919256746768951, Validation Loss: 0.18766021728515625\n",
            "Epoch 326/2500, Training Loss: 0.16885779798030853, Validation Loss: 0.1873890608549118\n",
            "Epoch 327/2500, Training Loss: 0.16852234303951263, Validation Loss: 0.18711324036121368\n",
            "Epoch 328/2500, Training Loss: 0.16818839311599731, Validation Loss: 0.18683244287967682\n",
            "Epoch 329/2500, Training Loss: 0.16785667836666107, Validation Loss: 0.1865464299917221\n",
            "Epoch 330/2500, Training Loss: 0.16752628982067108, Validation Loss: 0.1862545758485794\n",
            "Epoch 331/2500, Training Loss: 0.16719606518745422, Validation Loss: 0.18595711886882782\n",
            "Epoch 332/2500, Training Loss: 0.16686353087425232, Validation Loss: 0.1856536716222763\n",
            "Epoch 333/2500, Training Loss: 0.16652914881706238, Validation Loss: 0.18534469604492188\n",
            "Epoch 334/2500, Training Loss: 0.16619551181793213, Validation Loss: 0.18503762781620026\n",
            "Epoch 335/2500, Training Loss: 0.1658650040626526, Validation Loss: 0.18472988903522491\n",
            "Epoch 336/2500, Training Loss: 0.16553646326065063, Validation Loss: 0.18442358076572418\n",
            "Epoch 337/2500, Training Loss: 0.1652059406042099, Validation Loss: 0.1841183453798294\n",
            "Epoch 338/2500, Training Loss: 0.16487371921539307, Validation Loss: 0.18381863832473755\n",
            "Epoch 339/2500, Training Loss: 0.16453993320465088, Validation Loss: 0.1835225224494934\n",
            "Epoch 340/2500, Training Loss: 0.16420376300811768, Validation Loss: 0.18323101103305817\n",
            "Epoch 341/2500, Training Loss: 0.1638680100440979, Validation Loss: 0.18294507265090942\n",
            "Epoch 342/2500, Training Loss: 0.16353565454483032, Validation Loss: 0.18266530334949493\n",
            "Epoch 343/2500, Training Loss: 0.16320575773715973, Validation Loss: 0.18239398300647736\n",
            "Epoch 344/2500, Training Loss: 0.1628766506910324, Validation Loss: 0.18212753534317017\n",
            "Epoch 345/2500, Training Loss: 0.16254936158657074, Validation Loss: 0.181866854429245\n",
            "Epoch 346/2500, Training Loss: 0.16221988201141357, Validation Loss: 0.18158499896526337\n",
            "Epoch 347/2500, Training Loss: 0.16188716888427734, Validation Loss: 0.1812838613986969\n",
            "Epoch 348/2500, Training Loss: 0.16155695915222168, Validation Loss: 0.18097874522209167\n",
            "Epoch 349/2500, Training Loss: 0.16122855246067047, Validation Loss: 0.18067608773708344\n",
            "Epoch 350/2500, Training Loss: 0.16090607643127441, Validation Loss: 0.18037737905979156\n",
            "Epoch 351/2500, Training Loss: 0.16058553755283356, Validation Loss: 0.18007831275463104\n",
            "Epoch 352/2500, Training Loss: 0.1602649986743927, Validation Loss: 0.17978279292583466\n",
            "Epoch 353/2500, Training Loss: 0.15994803607463837, Validation Loss: 0.1795070469379425\n",
            "Epoch 354/2500, Training Loss: 0.15963508188724518, Validation Loss: 0.17923659086227417\n",
            "Epoch 355/2500, Training Loss: 0.1593252271413803, Validation Loss: 0.17896777391433716\n",
            "Epoch 356/2500, Training Loss: 0.1590183675289154, Validation Loss: 0.17870117723941803\n",
            "Epoch 357/2500, Training Loss: 0.1587141454219818, Validation Loss: 0.17843849956989288\n",
            "Epoch 358/2500, Training Loss: 0.15841013193130493, Validation Loss: 0.17817728221416473\n",
            "Epoch 359/2500, Training Loss: 0.1581096351146698, Validation Loss: 0.17791412770748138\n",
            "Epoch 360/2500, Training Loss: 0.1578132063150406, Validation Loss: 0.17765291035175323\n",
            "Epoch 361/2500, Training Loss: 0.1575194150209427, Validation Loss: 0.17739522457122803\n",
            "Epoch 362/2500, Training Loss: 0.15722784399986267, Validation Loss: 0.17714068293571472\n",
            "Epoch 363/2500, Training Loss: 0.15693829953670502, Validation Loss: 0.17688988149166107\n",
            "Epoch 364/2500, Training Loss: 0.1566503793001175, Validation Loss: 0.17664295434951782\n",
            "Epoch 365/2500, Training Loss: 0.156363844871521, Validation Loss: 0.17640681564807892\n",
            "Epoch 366/2500, Training Loss: 0.1560797095298767, Validation Loss: 0.17617732286453247\n",
            "Epoch 367/2500, Training Loss: 0.15579742193222046, Validation Loss: 0.17595165967941284\n",
            "Epoch 368/2500, Training Loss: 0.15551608800888062, Validation Loss: 0.17572912573814392\n",
            "Epoch 369/2500, Training Loss: 0.15523672103881836, Validation Loss: 0.17550718784332275\n",
            "Epoch 370/2500, Training Loss: 0.154959037899971, Validation Loss: 0.1752856820821762\n",
            "Epoch 371/2500, Training Loss: 0.1546824723482132, Validation Loss: 0.17506280541419983\n",
            "Epoch 372/2500, Training Loss: 0.1544072926044464, Validation Loss: 0.17483720183372498\n",
            "Epoch 373/2500, Training Loss: 0.15413345396518707, Validation Loss: 0.17461135983467102\n",
            "Epoch 374/2500, Training Loss: 0.15386049449443817, Validation Loss: 0.1743859499692917\n",
            "Epoch 375/2500, Training Loss: 0.1535891592502594, Validation Loss: 0.1741604059934616\n",
            "Epoch 376/2500, Training Loss: 0.15331892669200897, Validation Loss: 0.1739376187324524\n",
            "Epoch 377/2500, Training Loss: 0.15305010974407196, Validation Loss: 0.17371855676174164\n",
            "Epoch 378/2500, Training Loss: 0.15278223156929016, Validation Loss: 0.17350046336650848\n",
            "Epoch 379/2500, Training Loss: 0.1525154709815979, Validation Loss: 0.17328304052352905\n",
            "Epoch 380/2500, Training Loss: 0.15225018560886383, Validation Loss: 0.17306482791900635\n",
            "Epoch 381/2500, Training Loss: 0.15198558568954468, Validation Loss: 0.1728452891111374\n",
            "Epoch 382/2500, Training Loss: 0.15172016620635986, Validation Loss: 0.17262479662895203\n",
            "Epoch 383/2500, Training Loss: 0.15145598351955414, Validation Loss: 0.17240406572818756\n",
            "Epoch 384/2500, Training Loss: 0.15119224786758423, Validation Loss: 0.1721850484609604\n",
            "Epoch 385/2500, Training Loss: 0.15092970430850983, Validation Loss: 0.17196565866470337\n",
            "Epoch 386/2500, Training Loss: 0.15067072212696075, Validation Loss: 0.1717461347579956\n",
            "Epoch 387/2500, Training Loss: 0.15041369199752808, Validation Loss: 0.17152704298496246\n",
            "Epoch 388/2500, Training Loss: 0.15015776455402374, Validation Loss: 0.17131070792675018\n",
            "Epoch 389/2500, Training Loss: 0.14990290999412537, Validation Loss: 0.17109675705432892\n",
            "Epoch 390/2500, Training Loss: 0.14965228736400604, Validation Loss: 0.17088361084461212\n",
            "Epoch 391/2500, Training Loss: 0.14940287172794342, Validation Loss: 0.17067141830921173\n",
            "Epoch 392/2500, Training Loss: 0.14915436506271362, Validation Loss: 0.17046011984348297\n",
            "Epoch 393/2500, Training Loss: 0.14890603721141815, Validation Loss: 0.1702529788017273\n",
            "Epoch 394/2500, Training Loss: 0.14865770936012268, Validation Loss: 0.17005179822444916\n",
            "Epoch 395/2500, Training Loss: 0.14841045439243317, Validation Loss: 0.1698552519083023\n",
            "Epoch 396/2500, Training Loss: 0.14816385507583618, Validation Loss: 0.16966189444065094\n",
            "Epoch 397/2500, Training Loss: 0.14791831374168396, Validation Loss: 0.1694704294204712\n",
            "Epoch 398/2500, Training Loss: 0.14767411351203918, Validation Loss: 0.16927950084209442\n",
            "Epoch 399/2500, Training Loss: 0.14743155241012573, Validation Loss: 0.16908758878707886\n",
            "Epoch 400/2500, Training Loss: 0.1471901834011078, Validation Loss: 0.1688956618309021\n",
            "Epoch 401/2500, Training Loss: 0.146949902176857, Validation Loss: 0.168705552816391\n",
            "Epoch 402/2500, Training Loss: 0.14671091735363007, Validation Loss: 0.1685151755809784\n",
            "Epoch 403/2500, Training Loss: 0.14647310972213745, Validation Loss: 0.16832371056079865\n",
            "Epoch 404/2500, Training Loss: 0.1462370604276657, Validation Loss: 0.168130561709404\n",
            "Epoch 405/2500, Training Loss: 0.14600194990634918, Validation Loss: 0.16793504357337952\n",
            "Epoch 406/2500, Training Loss: 0.14576809108257294, Validation Loss: 0.16773512959480286\n",
            "Epoch 407/2500, Training Loss: 0.14553554356098175, Validation Loss: 0.16753239929676056\n",
            "Epoch 408/2500, Training Loss: 0.14530423283576965, Validation Loss: 0.16732695698738098\n",
            "Epoch 409/2500, Training Loss: 0.14507390558719635, Validation Loss: 0.16711896657943726\n",
            "Epoch 410/2500, Training Loss: 0.1448444426059723, Validation Loss: 0.16690649092197418\n",
            "Epoch 411/2500, Training Loss: 0.14461584389209747, Validation Loss: 0.16669078171253204\n",
            "Epoch 412/2500, Training Loss: 0.14438802003860474, Validation Loss: 0.1664736568927765\n",
            "Epoch 413/2500, Training Loss: 0.14416147768497467, Validation Loss: 0.16625702381134033\n",
            "Epoch 414/2500, Training Loss: 0.14393626153469086, Validation Loss: 0.16603843867778778\n",
            "Epoch 415/2500, Training Loss: 0.14371222257614136, Validation Loss: 0.16581900417804718\n",
            "Epoch 416/2500, Training Loss: 0.1434895396232605, Validation Loss: 0.16559667885303497\n",
            "Epoch 417/2500, Training Loss: 0.14326807856559753, Validation Loss: 0.16537462174892426\n",
            "Epoch 418/2500, Training Loss: 0.14304763078689575, Validation Loss: 0.16515350341796875\n",
            "Epoch 419/2500, Training Loss: 0.14282771944999695, Validation Loss: 0.16493062674999237\n",
            "Epoch 420/2500, Training Loss: 0.14260819554328918, Validation Loss: 0.1647072583436966\n",
            "Epoch 421/2500, Training Loss: 0.14238931238651276, Validation Loss: 0.16448688507080078\n",
            "Epoch 422/2500, Training Loss: 0.142171248793602, Validation Loss: 0.1642713099718094\n",
            "Epoch 423/2500, Training Loss: 0.14195480942726135, Validation Loss: 0.16406239569187164\n",
            "Epoch 424/2500, Training Loss: 0.1417391300201416, Validation Loss: 0.16386069357395172\n",
            "Epoch 425/2500, Training Loss: 0.1415237933397293, Validation Loss: 0.1636667549610138\n",
            "Epoch 426/2500, Training Loss: 0.14130957424640656, Validation Loss: 0.16347770392894745\n",
            "Epoch 427/2500, Training Loss: 0.14109604060649872, Validation Loss: 0.16329458355903625\n",
            "Epoch 428/2500, Training Loss: 0.14088329672813416, Validation Loss: 0.16311641037464142\n",
            "Epoch 429/2500, Training Loss: 0.1406712681055069, Validation Loss: 0.16294051706790924\n",
            "Epoch 430/2500, Training Loss: 0.1404605507850647, Validation Loss: 0.16276536881923676\n",
            "Epoch 431/2500, Training Loss: 0.14025059342384338, Validation Loss: 0.16258956491947174\n",
            "Epoch 432/2500, Training Loss: 0.1400413066148758, Validation Loss: 0.1624116599559784\n",
            "Epoch 433/2500, Training Loss: 0.13983267545700073, Validation Loss: 0.1622290313243866\n",
            "Epoch 434/2500, Training Loss: 0.1396244913339615, Validation Loss: 0.1620430201292038\n",
            "Epoch 435/2500, Training Loss: 0.13941766321659088, Validation Loss: 0.1618528962135315\n",
            "Epoch 436/2500, Training Loss: 0.13921211659908295, Validation Loss: 0.16165661811828613\n",
            "Epoch 437/2500, Training Loss: 0.13900697231292725, Validation Loss: 0.16145537793636322\n",
            "Epoch 438/2500, Training Loss: 0.13880163431167603, Validation Loss: 0.161251500248909\n",
            "Epoch 439/2500, Training Loss: 0.1385970264673233, Validation Loss: 0.16104759275913239\n",
            "Epoch 440/2500, Training Loss: 0.13839292526245117, Validation Loss: 0.16084393858909607\n",
            "Epoch 441/2500, Training Loss: 0.13818961381912231, Validation Loss: 0.16064591705799103\n",
            "Epoch 442/2500, Training Loss: 0.13798701763153076, Validation Loss: 0.16045303642749786\n",
            "Epoch 443/2500, Training Loss: 0.13778461515903473, Validation Loss: 0.16026464104652405\n",
            "Epoch 444/2500, Training Loss: 0.13758282363414764, Validation Loss: 0.160080686211586\n",
            "Epoch 445/2500, Training Loss: 0.1373818963766098, Validation Loss: 0.1599007546901703\n",
            "Epoch 446/2500, Training Loss: 0.13718168437480927, Validation Loss: 0.15972574055194855\n",
            "Epoch 447/2500, Training Loss: 0.1369820088148117, Validation Loss: 0.15955573320388794\n",
            "Epoch 448/2500, Training Loss: 0.1367829591035843, Validation Loss: 0.1593867987394333\n",
            "Epoch 449/2500, Training Loss: 0.13658560812473297, Validation Loss: 0.15922673046588898\n",
            "Epoch 450/2500, Training Loss: 0.13638943433761597, Validation Loss: 0.1590730994939804\n",
            "Epoch 451/2500, Training Loss: 0.13619402050971985, Validation Loss: 0.15892179310321808\n",
            "Epoch 452/2500, Training Loss: 0.13600057363510132, Validation Loss: 0.1587686836719513\n",
            "Epoch 453/2500, Training Loss: 0.1358075588941574, Validation Loss: 0.1586124151945114\n",
            "Epoch 454/2500, Training Loss: 0.13561514019966125, Validation Loss: 0.1584516167640686\n",
            "Epoch 455/2500, Training Loss: 0.13542336225509644, Validation Loss: 0.15828576683998108\n",
            "Epoch 456/2500, Training Loss: 0.1352320909500122, Validation Loss: 0.15811476111412048\n",
            "Epoch 457/2500, Training Loss: 0.13504146039485931, Validation Loss: 0.15793952345848083\n",
            "Epoch 458/2500, Training Loss: 0.13485129177570343, Validation Loss: 0.15776057541370392\n",
            "Epoch 459/2500, Training Loss: 0.13466155529022217, Validation Loss: 0.15757890045642853\n",
            "Epoch 460/2500, Training Loss: 0.13447263836860657, Validation Loss: 0.15739357471466064\n",
            "Epoch 461/2500, Training Loss: 0.13428477942943573, Validation Loss: 0.15720625221729279\n",
            "Epoch 462/2500, Training Loss: 0.13409771025180817, Validation Loss: 0.15701769292354584\n",
            "Epoch 463/2500, Training Loss: 0.13391171395778656, Validation Loss: 0.15683016180992126\n",
            "Epoch 464/2500, Training Loss: 0.13372547924518585, Validation Loss: 0.15664398670196533\n",
            "Epoch 465/2500, Training Loss: 0.1335393339395523, Validation Loss: 0.15646150708198547\n",
            "Epoch 466/2500, Training Loss: 0.13335378468036652, Validation Loss: 0.15628449618816376\n",
            "Epoch 467/2500, Training Loss: 0.13316860795021057, Validation Loss: 0.15611286461353302\n",
            "Epoch 468/2500, Training Loss: 0.1329840123653412, Validation Loss: 0.15594767034053802\n",
            "Epoch 469/2500, Training Loss: 0.1327999383211136, Validation Loss: 0.1557898074388504\n",
            "Epoch 470/2500, Training Loss: 0.13261644542217255, Validation Loss: 0.15563830733299255\n",
            "Epoch 471/2500, Training Loss: 0.13243407011032104, Validation Loss: 0.15549270808696747\n",
            "Epoch 472/2500, Training Loss: 0.13225297629833221, Validation Loss: 0.1553504467010498\n",
            "Epoch 473/2500, Training Loss: 0.13207291066646576, Validation Loss: 0.15521027147769928\n",
            "Epoch 474/2500, Training Loss: 0.13189341127872467, Validation Loss: 0.15507091581821442\n",
            "Epoch 475/2500, Training Loss: 0.1317145675420761, Validation Loss: 0.15493127703666687\n",
            "Epoch 476/2500, Training Loss: 0.13153597712516785, Validation Loss: 0.15478812158107758\n",
            "Epoch 477/2500, Training Loss: 0.13135771453380585, Validation Loss: 0.1546410322189331\n",
            "Epoch 478/2500, Training Loss: 0.13118000328540802, Validation Loss: 0.15448956191539764\n",
            "Epoch 479/2500, Training Loss: 0.13100315630435944, Validation Loss: 0.15433479845523834\n",
            "Epoch 480/2500, Training Loss: 0.13082635402679443, Validation Loss: 0.15417678654193878\n",
            "Epoch 481/2500, Training Loss: 0.13064934313297272, Validation Loss: 0.15401694178581238\n",
            "Epoch 482/2500, Training Loss: 0.13047267496585846, Validation Loss: 0.1538553088903427\n",
            "Epoch 483/2500, Training Loss: 0.13029642403125763, Validation Loss: 0.15369290113449097\n",
            "Epoch 484/2500, Training Loss: 0.13012076914310455, Validation Loss: 0.1535312384366989\n",
            "Epoch 485/2500, Training Loss: 0.12994563579559326, Validation Loss: 0.1533711850643158\n",
            "Epoch 486/2500, Training Loss: 0.12977100908756256, Validation Loss: 0.1532110571861267\n",
            "Epoch 487/2500, Training Loss: 0.12959711253643036, Validation Loss: 0.15305495262145996\n",
            "Epoch 488/2500, Training Loss: 0.12942379713058472, Validation Loss: 0.15290305018424988\n",
            "Epoch 489/2500, Training Loss: 0.12925098836421967, Validation Loss: 0.15275590121746063\n",
            "Epoch 490/2500, Training Loss: 0.12907862663269043, Validation Loss: 0.15261289477348328\n",
            "Epoch 491/2500, Training Loss: 0.128906711935997, Validation Loss: 0.1524726152420044\n",
            "Epoch 492/2500, Training Loss: 0.12873542308807373, Validation Loss: 0.1523374319076538\n",
            "Epoch 493/2500, Training Loss: 0.12856470048427582, Validation Loss: 0.15220719575881958\n",
            "Epoch 494/2500, Training Loss: 0.1283944845199585, Validation Loss: 0.15207313001155853\n",
            "Epoch 495/2500, Training Loss: 0.12822522222995758, Validation Loss: 0.1519353836774826\n",
            "Epoch 496/2500, Training Loss: 0.1280563473701477, Validation Loss: 0.15179386734962463\n",
            "Epoch 497/2500, Training Loss: 0.12788817286491394, Validation Loss: 0.1516508162021637\n",
            "Epoch 498/2500, Training Loss: 0.12772026658058167, Validation Loss: 0.15150490403175354\n",
            "Epoch 499/2500, Training Loss: 0.1275530904531479, Validation Loss: 0.15135429799556732\n",
            "Epoch 500/2500, Training Loss: 0.12738649547100067, Validation Loss: 0.15119953453540802\n",
            "Epoch 501/2500, Training Loss: 0.1272200047969818, Validation Loss: 0.15103775262832642\n",
            "Epoch 502/2500, Training Loss: 0.12705391645431519, Validation Loss: 0.1508718580007553\n",
            "Epoch 503/2500, Training Loss: 0.12688873708248138, Validation Loss: 0.15070532262325287\n",
            "Epoch 504/2500, Training Loss: 0.1267250031232834, Validation Loss: 0.15053772926330566\n",
            "Epoch 505/2500, Training Loss: 0.12656170129776, Validation Loss: 0.1503712683916092\n",
            "Epoch 506/2500, Training Loss: 0.1263984888792038, Validation Loss: 0.1502077430486679\n",
            "Epoch 507/2500, Training Loss: 0.1262357085943222, Validation Loss: 0.1500478982925415\n",
            "Epoch 508/2500, Training Loss: 0.12607334554195404, Validation Loss: 0.1499016284942627\n",
            "Epoch 509/2500, Training Loss: 0.125910222530365, Validation Loss: 0.14976684749126434\n",
            "Epoch 510/2500, Training Loss: 0.12574684619903564, Validation Loss: 0.1496412307024002\n",
            "Epoch 511/2500, Training Loss: 0.12558358907699585, Validation Loss: 0.14952602982521057\n",
            "Epoch 512/2500, Training Loss: 0.12542039155960083, Validation Loss: 0.14941471815109253\n",
            "Epoch 513/2500, Training Loss: 0.1252570003271103, Validation Loss: 0.14930115640163422\n",
            "Epoch 514/2500, Training Loss: 0.1250942200422287, Validation Loss: 0.1491791009902954\n",
            "Epoch 515/2500, Training Loss: 0.12493221461772919, Validation Loss: 0.14904747903347015\n",
            "Epoch 516/2500, Training Loss: 0.12477052956819534, Validation Loss: 0.14890563488006592\n",
            "Epoch 517/2500, Training Loss: 0.1246088445186615, Validation Loss: 0.1487533152103424\n",
            "Epoch 518/2500, Training Loss: 0.12444743514060974, Validation Loss: 0.14859174191951752\n",
            "Epoch 519/2500, Training Loss: 0.1242859736084938, Validation Loss: 0.14842307567596436\n",
            "Epoch 520/2500, Training Loss: 0.12412521988153458, Validation Loss: 0.14825056493282318\n",
            "Epoch 521/2500, Training Loss: 0.12396452575922012, Validation Loss: 0.14807629585266113\n",
            "Epoch 522/2500, Training Loss: 0.12380380183458328, Validation Loss: 0.14790192246437073\n",
            "Epoch 523/2500, Training Loss: 0.12364350259304047, Validation Loss: 0.14773359894752502\n",
            "Epoch 524/2500, Training Loss: 0.12348365038633347, Validation Loss: 0.1475791335105896\n",
            "Epoch 525/2500, Training Loss: 0.12332382053136826, Validation Loss: 0.14743879437446594\n",
            "Epoch 526/2500, Training Loss: 0.12316394597291946, Validation Loss: 0.1473088413476944\n",
            "Epoch 527/2500, Training Loss: 0.12300360202789307, Validation Loss: 0.14718686044216156\n",
            "Epoch 528/2500, Training Loss: 0.12284287810325623, Validation Loss: 0.14707012474536896\n",
            "Epoch 529/2500, Training Loss: 0.1226821169257164, Validation Loss: 0.14695806801319122\n",
            "Epoch 530/2500, Training Loss: 0.12252301722764969, Validation Loss: 0.14684408903121948\n",
            "Epoch 531/2500, Training Loss: 0.12236463278532028, Validation Loss: 0.14672593772411346\n",
            "Epoch 532/2500, Training Loss: 0.12220732122659683, Validation Loss: 0.14659613370895386\n",
            "Epoch 533/2500, Training Loss: 0.12204995006322861, Validation Loss: 0.14645256102085114\n",
            "Epoch 534/2500, Training Loss: 0.1218925341963768, Validation Loss: 0.14629602432250977\n",
            "Epoch 535/2500, Training Loss: 0.12173520028591156, Validation Loss: 0.14612998068332672\n",
            "Epoch 536/2500, Training Loss: 0.12157785892486572, Validation Loss: 0.14595797657966614\n",
            "Epoch 537/2500, Training Loss: 0.12142053246498108, Validation Loss: 0.14578264951705933\n",
            "Epoch 538/2500, Training Loss: 0.1212640181183815, Validation Loss: 0.14560610055923462\n",
            "Epoch 539/2500, Training Loss: 0.1211080476641655, Validation Loss: 0.14544115960597992\n",
            "Epoch 540/2500, Training Loss: 0.12095276266336441, Validation Loss: 0.14528709650039673\n",
            "Epoch 541/2500, Training Loss: 0.12079748511314392, Validation Loss: 0.1451464295387268\n",
            "Epoch 542/2500, Training Loss: 0.12064280360937119, Validation Loss: 0.14501702785491943\n",
            "Epoch 543/2500, Training Loss: 0.12048868089914322, Validation Loss: 0.14489705860614777\n",
            "Epoch 544/2500, Training Loss: 0.12033472210168839, Validation Loss: 0.14478594064712524\n",
            "Epoch 545/2500, Training Loss: 0.12018055468797684, Validation Loss: 0.1446824073791504\n",
            "Epoch 546/2500, Training Loss: 0.12002701312303543, Validation Loss: 0.144589364528656\n",
            "Epoch 547/2500, Training Loss: 0.11987415701150894, Validation Loss: 0.14449450373649597\n",
            "Epoch 548/2500, Training Loss: 0.11972158402204514, Validation Loss: 0.1444000005722046\n",
            "Epoch 549/2500, Training Loss: 0.11956959962844849, Validation Loss: 0.14429938793182373\n",
            "Epoch 550/2500, Training Loss: 0.1194181963801384, Validation Loss: 0.14419151842594147\n",
            "Epoch 551/2500, Training Loss: 0.11926697939634323, Validation Loss: 0.14407414197921753\n",
            "Epoch 552/2500, Training Loss: 0.11911601573228836, Validation Loss: 0.14394645392894745\n",
            "Epoch 553/2500, Training Loss: 0.11896555125713348, Validation Loss: 0.14381016790866852\n",
            "Epoch 554/2500, Training Loss: 0.11881554126739502, Validation Loss: 0.14367589354515076\n",
            "Epoch 555/2500, Training Loss: 0.11866606771945953, Validation Loss: 0.14354418218135834\n",
            "Epoch 556/2500, Training Loss: 0.11851710826158524, Validation Loss: 0.14341595768928528\n",
            "Epoch 557/2500, Training Loss: 0.11836898326873779, Validation Loss: 0.14329108595848083\n",
            "Epoch 558/2500, Training Loss: 0.1182226911187172, Validation Loss: 0.14316721260547638\n",
            "Epoch 559/2500, Training Loss: 0.1180790364742279, Validation Loss: 0.1430446207523346\n",
            "Epoch 560/2500, Training Loss: 0.11793635040521622, Validation Loss: 0.14292585849761963\n",
            "Epoch 561/2500, Training Loss: 0.11779460310935974, Validation Loss: 0.14281170070171356\n",
            "Epoch 562/2500, Training Loss: 0.11765428632497787, Validation Loss: 0.14270026981830597\n",
            "Epoch 563/2500, Training Loss: 0.11751434206962585, Validation Loss: 0.14258462190628052\n",
            "Epoch 564/2500, Training Loss: 0.11737438291311264, Validation Loss: 0.1424638032913208\n",
            "Epoch 565/2500, Training Loss: 0.11723525077104568, Validation Loss: 0.1423363834619522\n",
            "Epoch 566/2500, Training Loss: 0.11709673702716827, Validation Loss: 0.1422128677368164\n",
            "Epoch 567/2500, Training Loss: 0.11696022003889084, Validation Loss: 0.14209437370300293\n",
            "Epoch 568/2500, Training Loss: 0.11682459712028503, Validation Loss: 0.1419808268547058\n",
            "Epoch 569/2500, Training Loss: 0.11668924987316132, Validation Loss: 0.14186835289001465\n",
            "Epoch 570/2500, Training Loss: 0.11655406653881073, Validation Loss: 0.141758531332016\n",
            "Epoch 571/2500, Training Loss: 0.11641974002122879, Validation Loss: 0.14164525270462036\n",
            "Epoch 572/2500, Training Loss: 0.1162860170006752, Validation Loss: 0.14153195917606354\n",
            "Epoch 573/2500, Training Loss: 0.11615300178527832, Validation Loss: 0.1414157748222351\n",
            "Epoch 574/2500, Training Loss: 0.11601986736059189, Validation Loss: 0.14129507541656494\n",
            "Epoch 575/2500, Training Loss: 0.11588682979345322, Validation Loss: 0.1411665380001068\n",
            "Epoch 576/2500, Training Loss: 0.11575407534837723, Validation Loss: 0.14103558659553528\n",
            "Epoch 577/2500, Training Loss: 0.11562187224626541, Validation Loss: 0.14090284705162048\n",
            "Epoch 578/2500, Training Loss: 0.11549000442028046, Validation Loss: 0.14076900482177734\n",
            "Epoch 579/2500, Training Loss: 0.1153596043586731, Validation Loss: 0.14063803851604462\n",
            "Epoch 580/2500, Training Loss: 0.11522963643074036, Validation Loss: 0.14050911366939545\n",
            "Epoch 581/2500, Training Loss: 0.11509985476732254, Validation Loss: 0.14038045704364777\n",
            "Epoch 582/2500, Training Loss: 0.11497002094984055, Validation Loss: 0.1402534395456314\n",
            "Epoch 583/2500, Training Loss: 0.11484004557132721, Validation Loss: 0.14012914896011353\n",
            "Epoch 584/2500, Training Loss: 0.11471056193113327, Validation Loss: 0.1400032490491867\n",
            "Epoch 585/2500, Training Loss: 0.11458274722099304, Validation Loss: 0.13987375795841217\n",
            "Epoch 586/2500, Training Loss: 0.11445461213588715, Validation Loss: 0.1397399753332138\n",
            "Epoch 587/2500, Training Loss: 0.11432567983865738, Validation Loss: 0.13960455358028412\n",
            "Epoch 588/2500, Training Loss: 0.11419831216335297, Validation Loss: 0.13947243988513947\n",
            "Epoch 589/2500, Training Loss: 0.11407165229320526, Validation Loss: 0.13934369385242462\n",
            "Epoch 590/2500, Training Loss: 0.11394502967596054, Validation Loss: 0.13921794295310974\n",
            "Epoch 591/2500, Training Loss: 0.11381812393665314, Validation Loss: 0.139094740152359\n",
            "Epoch 592/2500, Training Loss: 0.1136915534734726, Validation Loss: 0.13897305727005005\n",
            "Epoch 593/2500, Training Loss: 0.11356530338525772, Validation Loss: 0.13885287940502167\n",
            "Epoch 594/2500, Training Loss: 0.11343929916620255, Validation Loss: 0.13873350620269775\n",
            "Epoch 595/2500, Training Loss: 0.11331453174352646, Validation Loss: 0.13860982656478882\n",
            "Epoch 596/2500, Training Loss: 0.1131897121667862, Validation Loss: 0.13848169147968292\n",
            "Epoch 597/2500, Training Loss: 0.11306440085172653, Validation Loss: 0.1383514553308487\n",
            "Epoch 598/2500, Training Loss: 0.11293952912092209, Validation Loss: 0.13821342587471008\n",
            "Epoch 599/2500, Training Loss: 0.11281467974185944, Validation Loss: 0.13806964457035065\n",
            "Epoch 600/2500, Training Loss: 0.11268962174654007, Validation Loss: 0.13791772723197937\n",
            "Epoch 601/2500, Training Loss: 0.11256493628025055, Validation Loss: 0.1377614140510559\n",
            "Epoch 602/2500, Training Loss: 0.11244088411331177, Validation Loss: 0.1376097947359085\n",
            "Epoch 603/2500, Training Loss: 0.11231695860624313, Validation Loss: 0.13746504485607147\n",
            "Epoch 604/2500, Training Loss: 0.11219307780265808, Validation Loss: 0.13732975721359253\n",
            "Epoch 605/2500, Training Loss: 0.11206979304552078, Validation Loss: 0.13720490038394928\n",
            "Epoch 606/2500, Training Loss: 0.11194655299186707, Validation Loss: 0.1370873749256134\n",
            "Epoch 607/2500, Training Loss: 0.11182348430156708, Validation Loss: 0.1369766891002655\n",
            "Epoch 608/2500, Training Loss: 0.11170077323913574, Validation Loss: 0.13687555491924286\n",
            "Epoch 609/2500, Training Loss: 0.1115780919790268, Validation Loss: 0.13678014278411865\n",
            "Epoch 610/2500, Training Loss: 0.11145585775375366, Validation Loss: 0.13669021427631378\n",
            "Epoch 611/2500, Training Loss: 0.1113339215517044, Validation Loss: 0.13659842312335968\n",
            "Epoch 612/2500, Training Loss: 0.11121194064617157, Validation Loss: 0.13650226593017578\n",
            "Epoch 613/2500, Training Loss: 0.11108920723199844, Validation Loss: 0.136400505900383\n",
            "Epoch 614/2500, Training Loss: 0.1109662801027298, Validation Loss: 0.13629256188869476\n",
            "Epoch 615/2500, Training Loss: 0.11084336042404175, Validation Loss: 0.13617877662181854\n",
            "Epoch 616/2500, Training Loss: 0.11072036623954773, Validation Loss: 0.13606390357017517\n",
            "Epoch 617/2500, Training Loss: 0.11059783399105072, Validation Loss: 0.13594883680343628\n",
            "Epoch 618/2500, Training Loss: 0.11047617346048355, Validation Loss: 0.13583479821681976\n",
            "Epoch 619/2500, Training Loss: 0.11035450547933578, Validation Loss: 0.1357213407754898\n",
            "Epoch 620/2500, Training Loss: 0.11023299396038055, Validation Loss: 0.13560844957828522\n",
            "Epoch 621/2500, Training Loss: 0.11011186987161636, Validation Loss: 0.13549759984016418\n",
            "Epoch 622/2500, Training Loss: 0.10999103635549545, Validation Loss: 0.13539040088653564\n",
            "Epoch 623/2500, Training Loss: 0.10987012833356857, Validation Loss: 0.13528618216514587\n",
            "Epoch 624/2500, Training Loss: 0.10974899679422379, Validation Loss: 0.13518394529819489\n",
            "Epoch 625/2500, Training Loss: 0.10962783545255661, Validation Loss: 0.13508205115795135\n",
            "Epoch 626/2500, Training Loss: 0.1095062717795372, Validation Loss: 0.1349792778491974\n",
            "Epoch 627/2500, Training Loss: 0.10938498377799988, Validation Loss: 0.13487102091312408\n",
            "Epoch 628/2500, Training Loss: 0.10926368832588196, Validation Loss: 0.13475780189037323\n",
            "Epoch 629/2500, Training Loss: 0.10914256423711777, Validation Loss: 0.1346399039030075\n",
            "Epoch 630/2500, Training Loss: 0.10902154445648193, Validation Loss: 0.13451829552650452\n",
            "Epoch 631/2500, Training Loss: 0.10890111327171326, Validation Loss: 0.1343940943479538\n",
            "Epoch 632/2500, Training Loss: 0.10878164321184158, Validation Loss: 0.13426582515239716\n",
            "Epoch 633/2500, Training Loss: 0.1086609959602356, Validation Loss: 0.13413973152637482\n",
            "Epoch 634/2500, Training Loss: 0.10853996127843857, Validation Loss: 0.1340169459581375\n",
            "Epoch 635/2500, Training Loss: 0.1084197461605072, Validation Loss: 0.1339009404182434\n",
            "Epoch 636/2500, Training Loss: 0.10829997062683105, Validation Loss: 0.1337873488664627\n",
            "Epoch 637/2500, Training Loss: 0.10818051546812057, Validation Loss: 0.13367652893066406\n",
            "Epoch 638/2500, Training Loss: 0.10806166380643845, Validation Loss: 0.13355086743831635\n",
            "Epoch 639/2500, Training Loss: 0.10794305801391602, Validation Loss: 0.1334197223186493\n",
            "Epoch 640/2500, Training Loss: 0.10782454162836075, Validation Loss: 0.1332888901233673\n",
            "Epoch 641/2500, Training Loss: 0.10770659148693085, Validation Loss: 0.1331581473350525\n",
            "Epoch 642/2500, Training Loss: 0.10758858174085617, Validation Loss: 0.13303802907466888\n",
            "Epoch 643/2500, Training Loss: 0.10747084766626358, Validation Loss: 0.1329270750284195\n",
            "Epoch 644/2500, Training Loss: 0.10735271871089935, Validation Loss: 0.13282333314418793\n",
            "Epoch 645/2500, Training Loss: 0.10723453760147095, Validation Loss: 0.1327144056558609\n",
            "Epoch 646/2500, Training Loss: 0.10711675137281418, Validation Loss: 0.1325998604297638\n",
            "Epoch 647/2500, Training Loss: 0.10699906200170517, Validation Loss: 0.13247942924499512\n",
            "Epoch 648/2500, Training Loss: 0.10688155889511108, Validation Loss: 0.13236376643180847\n",
            "Epoch 649/2500, Training Loss: 0.10676400363445282, Validation Loss: 0.13225197792053223\n",
            "Epoch 650/2500, Training Loss: 0.10664680600166321, Validation Loss: 0.1321425586938858\n",
            "Epoch 651/2500, Training Loss: 0.10653021931648254, Validation Loss: 0.13202372193336487\n",
            "Epoch 652/2500, Training Loss: 0.1064138412475586, Validation Loss: 0.13188956677913666\n",
            "Epoch 653/2500, Training Loss: 0.10629740357398987, Validation Loss: 0.13174915313720703\n",
            "Epoch 654/2500, Training Loss: 0.106181800365448, Validation Loss: 0.13161085546016693\n",
            "Epoch 655/2500, Training Loss: 0.10606640577316284, Validation Loss: 0.1314793974161148\n",
            "Epoch 656/2500, Training Loss: 0.10595144331455231, Validation Loss: 0.13135482370853424\n",
            "Epoch 657/2500, Training Loss: 0.10583856701850891, Validation Loss: 0.13123677670955658\n",
            "Epoch 658/2500, Training Loss: 0.10572592914104462, Validation Loss: 0.13111315667629242\n",
            "Epoch 659/2500, Training Loss: 0.10561329126358032, Validation Loss: 0.13098514080047607\n",
            "Epoch 660/2500, Training Loss: 0.10550149530172348, Validation Loss: 0.13085311651229858\n",
            "Epoch 661/2500, Training Loss: 0.10539020597934723, Validation Loss: 0.1307116150856018\n",
            "Epoch 662/2500, Training Loss: 0.10527908056974411, Validation Loss: 0.13057921826839447\n",
            "Epoch 663/2500, Training Loss: 0.1051679402589798, Validation Loss: 0.13045555353164673\n",
            "Epoch 664/2500, Training Loss: 0.1050567775964737, Validation Loss: 0.13033944368362427\n",
            "Epoch 665/2500, Training Loss: 0.10494586825370789, Validation Loss: 0.13022203743457794\n",
            "Epoch 666/2500, Training Loss: 0.10483519732952118, Validation Loss: 0.1300981491804123\n",
            "Epoch 667/2500, Training Loss: 0.10472490638494492, Validation Loss: 0.1299753189086914\n",
            "Epoch 668/2500, Training Loss: 0.10461538285017014, Validation Loss: 0.12985211610794067\n",
            "Epoch 669/2500, Training Loss: 0.10450597107410431, Validation Loss: 0.12971921265125275\n",
            "Epoch 670/2500, Training Loss: 0.10439682751893997, Validation Loss: 0.12958647310733795\n",
            "Epoch 671/2500, Training Loss: 0.10428820550441742, Validation Loss: 0.12945500016212463\n",
            "Epoch 672/2500, Training Loss: 0.1041799783706665, Validation Loss: 0.1293245255947113\n",
            "Epoch 673/2500, Training Loss: 0.10407209396362305, Validation Loss: 0.12920109927654266\n",
            "Epoch 674/2500, Training Loss: 0.1039639562368393, Validation Loss: 0.12906132638454437\n",
            "Epoch 675/2500, Training Loss: 0.10385371744632721, Validation Loss: 0.12891381978988647\n",
            "Epoch 676/2500, Training Loss: 0.10374359041452408, Validation Loss: 0.1287606954574585\n",
            "Epoch 677/2500, Training Loss: 0.10363294184207916, Validation Loss: 0.12860365211963654\n",
            "Epoch 678/2500, Training Loss: 0.10352201014757156, Validation Loss: 0.12844544649124146\n",
            "Epoch 679/2500, Training Loss: 0.10341086983680725, Validation Loss: 0.1282801777124405\n",
            "Epoch 680/2500, Training Loss: 0.1033003032207489, Validation Loss: 0.12811431288719177\n",
            "Epoch 681/2500, Training Loss: 0.10319008678197861, Validation Loss: 0.12795047461986542\n",
            "Epoch 682/2500, Training Loss: 0.10307970643043518, Validation Loss: 0.12778764963150024\n",
            "Epoch 683/2500, Training Loss: 0.10296855121850967, Validation Loss: 0.1276303082704544\n",
            "Epoch 684/2500, Training Loss: 0.10285834968090057, Validation Loss: 0.12748582661151886\n",
            "Epoch 685/2500, Training Loss: 0.1027485579252243, Validation Loss: 0.1273520439863205\n",
            "Epoch 686/2500, Training Loss: 0.10263746231794357, Validation Loss: 0.12722426652908325\n",
            "Epoch 687/2500, Training Loss: 0.10252558439970016, Validation Loss: 0.12709437310695648\n",
            "Epoch 688/2500, Training Loss: 0.10241348296403885, Validation Loss: 0.12696827948093414\n",
            "Epoch 689/2500, Training Loss: 0.102301225066185, Validation Loss: 0.1268383413553238\n",
            "Epoch 690/2500, Training Loss: 0.1021897941827774, Validation Loss: 0.1267130970954895\n",
            "Epoch 691/2500, Training Loss: 0.1020783856511116, Validation Loss: 0.12658575177192688\n",
            "Epoch 692/2500, Training Loss: 0.1019672378897667, Validation Loss: 0.12645156681537628\n",
            "Epoch 693/2500, Training Loss: 0.10185655206441879, Validation Loss: 0.12631072103977203\n",
            "Epoch 694/2500, Training Loss: 0.10174611955881119, Validation Loss: 0.12617303431034088\n",
            "Epoch 695/2500, Training Loss: 0.10163561999797821, Validation Loss: 0.12603673338890076\n",
            "Epoch 696/2500, Training Loss: 0.10152482986450195, Validation Loss: 0.12588396668434143\n",
            "Epoch 697/2500, Training Loss: 0.10141343623399734, Validation Loss: 0.12571923434734344\n",
            "Epoch 698/2500, Training Loss: 0.10130192339420319, Validation Loss: 0.12554390728473663\n",
            "Epoch 699/2500, Training Loss: 0.10119067132472992, Validation Loss: 0.1253657191991806\n",
            "Epoch 700/2500, Training Loss: 0.10107941180467606, Validation Loss: 0.1251930147409439\n",
            "Epoch 701/2500, Training Loss: 0.10096807777881622, Validation Loss: 0.12502507865428925\n",
            "Epoch 702/2500, Training Loss: 0.10085731744766235, Validation Loss: 0.12486550956964493\n",
            "Epoch 703/2500, Training Loss: 0.10074938088655472, Validation Loss: 0.12470852583646774\n",
            "Epoch 704/2500, Training Loss: 0.10064196586608887, Validation Loss: 0.12455470860004425\n",
            "Epoch 705/2500, Training Loss: 0.10053450614213943, Validation Loss: 0.12440520524978638\n",
            "Epoch 706/2500, Training Loss: 0.10042695701122284, Validation Loss: 0.12426465004682541\n",
            "Epoch 707/2500, Training Loss: 0.10031914710998535, Validation Loss: 0.12413570284843445\n",
            "Epoch 708/2500, Training Loss: 0.10021098703145981, Validation Loss: 0.12401667982339859\n",
            "Epoch 709/2500, Training Loss: 0.10010311007499695, Validation Loss: 0.12389114499092102\n",
            "Epoch 710/2500, Training Loss: 0.09999661892652512, Validation Loss: 0.12375868856906891\n",
            "Epoch 711/2500, Training Loss: 0.09989010542631149, Validation Loss: 0.12362007051706314\n",
            "Epoch 712/2500, Training Loss: 0.0997837707400322, Validation Loss: 0.12348265200853348\n",
            "Epoch 713/2500, Training Loss: 0.09967732429504395, Validation Loss: 0.1233479380607605\n",
            "Epoch 714/2500, Training Loss: 0.09957081079483032, Validation Loss: 0.12321510165929794\n",
            "Epoch 715/2500, Training Loss: 0.09946442395448685, Validation Loss: 0.12307540327310562\n",
            "Epoch 716/2500, Training Loss: 0.09935791790485382, Validation Loss: 0.12293020635843277\n",
            "Epoch 717/2500, Training Loss: 0.09925135970115662, Validation Loss: 0.12278138101100922\n",
            "Epoch 718/2500, Training Loss: 0.09914505481719971, Validation Loss: 0.12262836843729019\n",
            "Epoch 719/2500, Training Loss: 0.09903977811336517, Validation Loss: 0.1224823072552681\n",
            "Epoch 720/2500, Training Loss: 0.0989345833659172, Validation Loss: 0.12234611809253693\n",
            "Epoch 721/2500, Training Loss: 0.0988294929265976, Validation Loss: 0.12221290171146393\n",
            "Epoch 722/2500, Training Loss: 0.09872432798147202, Validation Loss: 0.12208390235900879\n",
            "Epoch 723/2500, Training Loss: 0.09861905872821808, Validation Loss: 0.12195898592472076\n",
            "Epoch 724/2500, Training Loss: 0.09851362556219101, Validation Loss: 0.12183759361505508\n",
            "Epoch 725/2500, Training Loss: 0.0984080359339714, Validation Loss: 0.12171295285224915\n",
            "Epoch 726/2500, Training Loss: 0.09830427169799805, Validation Loss: 0.1215844601392746\n",
            "Epoch 727/2500, Training Loss: 0.09820037335157394, Validation Loss: 0.12145715206861496\n",
            "Epoch 728/2500, Training Loss: 0.09809678792953491, Validation Loss: 0.12133137881755829\n",
            "Epoch 729/2500, Training Loss: 0.09799346327781677, Validation Loss: 0.12121085822582245\n",
            "Epoch 730/2500, Training Loss: 0.09789089858531952, Validation Loss: 0.12109707295894623\n",
            "Epoch 731/2500, Training Loss: 0.09778881072998047, Validation Loss: 0.12098849564790726\n",
            "Epoch 732/2500, Training Loss: 0.09768534451723099, Validation Loss: 0.12086883932352066\n",
            "Epoch 733/2500, Training Loss: 0.09758108854293823, Validation Loss: 0.12073981016874313\n",
            "Epoch 734/2500, Training Loss: 0.0974765196442604, Validation Loss: 0.12060423195362091\n",
            "Epoch 735/2500, Training Loss: 0.09737156331539154, Validation Loss: 0.12046603113412857\n",
            "Epoch 736/2500, Training Loss: 0.09726673364639282, Validation Loss: 0.12032922357320786\n",
            "Epoch 737/2500, Training Loss: 0.09716220200061798, Validation Loss: 0.12019328027963638\n",
            "Epoch 738/2500, Training Loss: 0.09705783426761627, Validation Loss: 0.12006185203790665\n",
            "Epoch 739/2500, Training Loss: 0.09695377200841904, Validation Loss: 0.11994021385908127\n",
            "Epoch 740/2500, Training Loss: 0.0968499481678009, Validation Loss: 0.11982931941747665\n",
            "Epoch 741/2500, Training Loss: 0.09674546867609024, Validation Loss: 0.11973938345909119\n",
            "Epoch 742/2500, Training Loss: 0.09663968533277512, Validation Loss: 0.11965907365083694\n",
            "Epoch 743/2500, Training Loss: 0.09653373807668686, Validation Loss: 0.11958453059196472\n",
            "Epoch 744/2500, Training Loss: 0.09642767161130905, Validation Loss: 0.1195148378610611\n",
            "Epoch 745/2500, Training Loss: 0.09632156789302826, Validation Loss: 0.11944970488548279\n",
            "Epoch 746/2500, Training Loss: 0.09621599316596985, Validation Loss: 0.1193862110376358\n",
            "Epoch 747/2500, Training Loss: 0.09611167758703232, Validation Loss: 0.11932504922151566\n",
            "Epoch 748/2500, Training Loss: 0.09600748121738434, Validation Loss: 0.11926774680614471\n",
            "Epoch 749/2500, Training Loss: 0.09590369462966919, Validation Loss: 0.11921955645084381\n",
            "Epoch 750/2500, Training Loss: 0.09579906612634659, Validation Loss: 0.11917725205421448\n",
            "Epoch 751/2500, Training Loss: 0.09569478034973145, Validation Loss: 0.11913096159696579\n",
            "Epoch 752/2500, Training Loss: 0.09559125453233719, Validation Loss: 0.11908725649118423\n",
            "Epoch 753/2500, Training Loss: 0.09548752754926682, Validation Loss: 0.11904316395521164\n",
            "Epoch 754/2500, Training Loss: 0.09538500010967255, Validation Loss: 0.11899727582931519\n",
            "Epoch 755/2500, Training Loss: 0.09528256207704544, Validation Loss: 0.11894668638706207\n",
            "Epoch 756/2500, Training Loss: 0.09518012404441833, Validation Loss: 0.11889545619487762\n",
            "Epoch 757/2500, Training Loss: 0.0950782522559166, Validation Loss: 0.11884598433971405\n",
            "Epoch 758/2500, Training Loss: 0.094977006316185, Validation Loss: 0.11879759281873703\n",
            "Epoch 759/2500, Training Loss: 0.09487612545490265, Validation Loss: 0.11874771863222122\n",
            "Epoch 760/2500, Training Loss: 0.09477567672729492, Validation Loss: 0.11869914084672928\n",
            "Epoch 761/2500, Training Loss: 0.09467531740665436, Validation Loss: 0.11864302307367325\n",
            "Epoch 762/2500, Training Loss: 0.0945759192109108, Validation Loss: 0.1185796856880188\n",
            "Epoch 763/2500, Training Loss: 0.09447705745697021, Validation Loss: 0.11851510405540466\n",
            "Epoch 764/2500, Training Loss: 0.09437932819128036, Validation Loss: 0.11844920367002487\n",
            "Epoch 765/2500, Training Loss: 0.09428142011165619, Validation Loss: 0.11838319152593613\n",
            "Epoch 766/2500, Training Loss: 0.09418372064828873, Validation Loss: 0.11831524968147278\n",
            "Epoch 767/2500, Training Loss: 0.09408696740865707, Validation Loss: 0.1182432696223259\n",
            "Epoch 768/2500, Training Loss: 0.0939902514219284, Validation Loss: 0.11816643923521042\n",
            "Epoch 769/2500, Training Loss: 0.0938933789730072, Validation Loss: 0.1180800274014473\n",
            "Epoch 770/2500, Training Loss: 0.09379696100950241, Validation Loss: 0.1179959774017334\n",
            "Epoch 771/2500, Training Loss: 0.0937008410692215, Validation Loss: 0.11791307479143143\n",
            "Epoch 772/2500, Training Loss: 0.0936049148440361, Validation Loss: 0.11782462149858475\n",
            "Epoch 773/2500, Training Loss: 0.093509741127491, Validation Loss: 0.11772851645946503\n",
            "Epoch 774/2500, Training Loss: 0.09341452270746231, Validation Loss: 0.11762423068284988\n",
            "Epoch 775/2500, Training Loss: 0.09332000464200974, Validation Loss: 0.11752001941204071\n",
            "Epoch 776/2500, Training Loss: 0.09322536736726761, Validation Loss: 0.11741490662097931\n",
            "Epoch 777/2500, Training Loss: 0.09313036501407623, Validation Loss: 0.11730843037366867\n",
            "Epoch 778/2500, Training Loss: 0.09303601831197739, Validation Loss: 0.11719293147325516\n",
            "Epoch 779/2500, Training Loss: 0.09294168651103973, Validation Loss: 0.11706992238759995\n",
            "Epoch 780/2500, Training Loss: 0.09284653514623642, Validation Loss: 0.11692733317613602\n",
            "Epoch 781/2500, Training Loss: 0.09275005757808685, Validation Loss: 0.1167711466550827\n",
            "Epoch 782/2500, Training Loss: 0.09265552461147308, Validation Loss: 0.11662277579307556\n",
            "Epoch 783/2500, Training Loss: 0.09256060421466827, Validation Loss: 0.11648428440093994\n",
            "Epoch 784/2500, Training Loss: 0.09246519207954407, Validation Loss: 0.11636074632406235\n",
            "Epoch 785/2500, Training Loss: 0.09236904978752136, Validation Loss: 0.1162458285689354\n",
            "Epoch 786/2500, Training Loss: 0.09227382391691208, Validation Loss: 0.11614083498716354\n",
            "Epoch 787/2500, Training Loss: 0.09217913448810577, Validation Loss: 0.11604582518339157\n",
            "Epoch 788/2500, Training Loss: 0.09208536148071289, Validation Loss: 0.11594927310943604\n",
            "Epoch 789/2500, Training Loss: 0.09199094772338867, Validation Loss: 0.11585215479135513\n",
            "Epoch 790/2500, Training Loss: 0.09189581125974655, Validation Loss: 0.11575423181056976\n",
            "Epoch 791/2500, Training Loss: 0.09180094301700592, Validation Loss: 0.11566425859928131\n",
            "Epoch 792/2500, Training Loss: 0.09170688688755035, Validation Loss: 0.11558086425065994\n",
            "Epoch 793/2500, Training Loss: 0.09161287546157837, Validation Loss: 0.11550113558769226\n",
            "Epoch 794/2500, Training Loss: 0.09151878952980042, Validation Loss: 0.11542260646820068\n",
            "Epoch 795/2500, Training Loss: 0.09142487496137619, Validation Loss: 0.1153428927063942\n",
            "Epoch 796/2500, Training Loss: 0.09133119136095047, Validation Loss: 0.11526032537221909\n",
            "Epoch 797/2500, Training Loss: 0.0912376418709755, Validation Loss: 0.11517398059368134\n",
            "Epoch 798/2500, Training Loss: 0.0911441519856453, Validation Loss: 0.11508378386497498\n",
            "Epoch 799/2500, Training Loss: 0.09105093777179718, Validation Loss: 0.11499191075563431\n",
            "Epoch 800/2500, Training Loss: 0.09095801413059235, Validation Loss: 0.114898182451725\n",
            "Epoch 801/2500, Training Loss: 0.09086553007364273, Validation Loss: 0.11480081081390381\n",
            "Epoch 802/2500, Training Loss: 0.09077339619398117, Validation Loss: 0.11470259726047516\n",
            "Epoch 803/2500, Training Loss: 0.0906834825873375, Validation Loss: 0.1146140843629837\n",
            "Epoch 804/2500, Training Loss: 0.09059379249811172, Validation Loss: 0.11453314125537872\n",
            "Epoch 805/2500, Training Loss: 0.0905037671327591, Validation Loss: 0.11445633322000504\n",
            "Epoch 806/2500, Training Loss: 0.09041392803192139, Validation Loss: 0.11438044160604477\n",
            "Epoch 807/2500, Training Loss: 0.09032425284385681, Validation Loss: 0.11430076509714127\n",
            "Epoch 808/2500, Training Loss: 0.09023469686508179, Validation Loss: 0.11421459913253784\n",
            "Epoch 809/2500, Training Loss: 0.09014541655778885, Validation Loss: 0.11412195861339569\n",
            "Epoch 810/2500, Training Loss: 0.09005654603242874, Validation Loss: 0.11402114480733871\n",
            "Epoch 811/2500, Training Loss: 0.08996764570474625, Validation Loss: 0.11391070485115051\n",
            "Epoch 812/2500, Training Loss: 0.08987843990325928, Validation Loss: 0.11379069089889526\n",
            "Epoch 813/2500, Training Loss: 0.08978918194770813, Validation Loss: 0.11366257071495056\n",
            "Epoch 814/2500, Training Loss: 0.08970004320144653, Validation Loss: 0.11352859437465668\n",
            "Epoch 815/2500, Training Loss: 0.0896110013127327, Validation Loss: 0.11339131742715836\n",
            "Epoch 816/2500, Training Loss: 0.08952207118272781, Validation Loss: 0.11325448751449585\n",
            "Epoch 817/2500, Training Loss: 0.08943356573581696, Validation Loss: 0.11312118172645569\n",
            "Epoch 818/2500, Training Loss: 0.089345283806324, Validation Loss: 0.11299317330121994\n",
            "Epoch 819/2500, Training Loss: 0.08925767987966537, Validation Loss: 0.11286832392215729\n",
            "Epoch 820/2500, Training Loss: 0.08917037397623062, Validation Loss: 0.11274712532758713\n",
            "Epoch 821/2500, Training Loss: 0.0890829935669899, Validation Loss: 0.11262853443622589\n",
            "Epoch 822/2500, Training Loss: 0.0889948159456253, Validation Loss: 0.11251469701528549\n",
            "Epoch 823/2500, Training Loss: 0.08890723437070847, Validation Loss: 0.1124071329832077\n",
            "Epoch 824/2500, Training Loss: 0.08882041275501251, Validation Loss: 0.1122979149222374\n",
            "Epoch 825/2500, Training Loss: 0.08873461186885834, Validation Loss: 0.11218272894620895\n",
            "Epoch 826/2500, Training Loss: 0.08864878118038177, Validation Loss: 0.11206181347370148\n",
            "Epoch 827/2500, Training Loss: 0.08856284618377686, Validation Loss: 0.11193648725748062\n",
            "Epoch 828/2500, Training Loss: 0.08847697824239731, Validation Loss: 0.1118091493844986\n",
            "Epoch 829/2500, Training Loss: 0.08839114010334015, Validation Loss: 0.11168234050273895\n",
            "Epoch 830/2500, Training Loss: 0.08830547332763672, Validation Loss: 0.11155818402767181\n",
            "Epoch 831/2500, Training Loss: 0.08822040259838104, Validation Loss: 0.11143907159566879\n",
            "Epoch 832/2500, Training Loss: 0.08813585340976715, Validation Loss: 0.11132773756980896\n",
            "Epoch 833/2500, Training Loss: 0.08805165439844131, Validation Loss: 0.11121697723865509\n",
            "Epoch 834/2500, Training Loss: 0.08796768635511398, Validation Loss: 0.11110778898000717\n",
            "Epoch 835/2500, Training Loss: 0.0878836065530777, Validation Loss: 0.11100821197032928\n",
            "Epoch 836/2500, Training Loss: 0.08779974281787872, Validation Loss: 0.1109098270535469\n",
            "Epoch 837/2500, Training Loss: 0.0877162367105484, Validation Loss: 0.1108207032084465\n",
            "Epoch 838/2500, Training Loss: 0.08763285726308823, Validation Loss: 0.11073877662420273\n",
            "Epoch 839/2500, Training Loss: 0.08754965662956238, Validation Loss: 0.11065320670604706\n",
            "Epoch 840/2500, Training Loss: 0.08746667206287384, Validation Loss: 0.11056293547153473\n",
            "Epoch 841/2500, Training Loss: 0.08738420158624649, Validation Loss: 0.11047494411468506\n",
            "Epoch 842/2500, Training Loss: 0.08730163425207138, Validation Loss: 0.11038777232170105\n",
            "Epoch 843/2500, Training Loss: 0.0872189924120903, Validation Loss: 0.11030057072639465\n",
            "Epoch 844/2500, Training Loss: 0.08713652938604355, Validation Loss: 0.1102014109492302\n",
            "Epoch 845/2500, Training Loss: 0.08705435693264008, Validation Loss: 0.11010082811117172\n",
            "Epoch 846/2500, Training Loss: 0.08697255700826645, Validation Loss: 0.11000293493270874\n",
            "Epoch 847/2500, Training Loss: 0.08689066767692566, Validation Loss: 0.10990586876869202\n",
            "Epoch 848/2500, Training Loss: 0.08680906146764755, Validation Loss: 0.10981234908103943\n",
            "Epoch 849/2500, Training Loss: 0.08672791719436646, Validation Loss: 0.1097181960940361\n",
            "Epoch 850/2500, Training Loss: 0.08664664626121521, Validation Loss: 0.10962758958339691\n",
            "Epoch 851/2500, Training Loss: 0.0865657702088356, Validation Loss: 0.10954416543245316\n",
            "Epoch 852/2500, Training Loss: 0.08648494631052017, Validation Loss: 0.1094609797000885\n",
            "Epoch 853/2500, Training Loss: 0.08640474826097488, Validation Loss: 0.10937649756669998\n",
            "Epoch 854/2500, Training Loss: 0.0863242819905281, Validation Loss: 0.10929382592439651\n",
            "Epoch 855/2500, Training Loss: 0.08624419569969177, Validation Loss: 0.10921809822320938\n",
            "Epoch 856/2500, Training Loss: 0.08616441488265991, Validation Loss: 0.10914230346679688\n",
            "Epoch 857/2500, Training Loss: 0.08608458936214447, Validation Loss: 0.10905680060386658\n",
            "Epoch 858/2500, Training Loss: 0.08600495010614395, Validation Loss: 0.10896659642457962\n",
            "Epoch 859/2500, Training Loss: 0.08592493087053299, Validation Loss: 0.10887201875448227\n",
            "Epoch 860/2500, Training Loss: 0.08584561944007874, Validation Loss: 0.10878127813339233\n",
            "Epoch 861/2500, Training Loss: 0.08576572686433792, Validation Loss: 0.10869477689266205\n",
            "Epoch 862/2500, Training Loss: 0.08568551391363144, Validation Loss: 0.10861270874738693\n",
            "Epoch 863/2500, Training Loss: 0.08560599386692047, Validation Loss: 0.10852070897817612\n",
            "Epoch 864/2500, Training Loss: 0.08552620559930801, Validation Loss: 0.10842257738113403\n",
            "Epoch 865/2500, Training Loss: 0.08544610440731049, Validation Loss: 0.10832449048757553\n",
            "Epoch 866/2500, Training Loss: 0.08536601811647415, Validation Loss: 0.10823400318622589\n",
            "Epoch 867/2500, Training Loss: 0.08528588712215424, Validation Loss: 0.1081383004784584\n",
            "Epoch 868/2500, Training Loss: 0.0852060467004776, Validation Loss: 0.10804561525583267\n",
            "Epoch 869/2500, Training Loss: 0.08512634038925171, Validation Loss: 0.10796212404966354\n",
            "Epoch 870/2500, Training Loss: 0.08504698425531387, Validation Loss: 0.10787039250135422\n",
            "Epoch 871/2500, Training Loss: 0.08496777713298798, Validation Loss: 0.10777658224105835\n",
            "Epoch 872/2500, Training Loss: 0.08488871157169342, Validation Loss: 0.10768930613994598\n",
            "Epoch 873/2500, Training Loss: 0.08481036126613617, Validation Loss: 0.10760051012039185\n",
            "Epoch 874/2500, Training Loss: 0.08473168313503265, Validation Loss: 0.10752324759960175\n",
            "Epoch 875/2500, Training Loss: 0.08465293794870377, Validation Loss: 0.10744830965995789\n",
            "Epoch 876/2500, Training Loss: 0.08457498997449875, Validation Loss: 0.10736565291881561\n",
            "Epoch 877/2500, Training Loss: 0.08449650555849075, Validation Loss: 0.10728660970926285\n",
            "Epoch 878/2500, Training Loss: 0.08441773802042007, Validation Loss: 0.10721118748188019\n",
            "Epoch 879/2500, Training Loss: 0.08433948457241058, Validation Loss: 0.10713399201631546\n",
            "Epoch 880/2500, Training Loss: 0.08426171541213989, Validation Loss: 0.1070534735918045\n",
            "Epoch 881/2500, Training Loss: 0.08418340981006622, Validation Loss: 0.10696996003389359\n",
            "Epoch 882/2500, Training Loss: 0.0841054618358612, Validation Loss: 0.10689520835876465\n",
            "Epoch 883/2500, Training Loss: 0.08402731269598007, Validation Loss: 0.10682718455791473\n",
            "Epoch 884/2500, Training Loss: 0.08394937962293625, Validation Loss: 0.10675912350416183\n",
            "Epoch 885/2500, Training Loss: 0.08387167006731033, Validation Loss: 0.10668408125638962\n",
            "Epoch 886/2500, Training Loss: 0.08379403501749039, Validation Loss: 0.10660212486982346\n",
            "Epoch 887/2500, Training Loss: 0.08371628820896149, Validation Loss: 0.10651000589132309\n",
            "Epoch 888/2500, Training Loss: 0.08363931626081467, Validation Loss: 0.1064235121011734\n",
            "Epoch 889/2500, Training Loss: 0.08356260508298874, Validation Loss: 0.10634317249059677\n",
            "Epoch 890/2500, Training Loss: 0.08348545432090759, Validation Loss: 0.10626495629549026\n",
            "Epoch 891/2500, Training Loss: 0.08340767025947571, Validation Loss: 0.10618928074836731\n",
            "Epoch 892/2500, Training Loss: 0.08333078026771545, Validation Loss: 0.10611657053232193\n",
            "Epoch 893/2500, Training Loss: 0.08325358480215073, Validation Loss: 0.10604873299598694\n",
            "Epoch 894/2500, Training Loss: 0.08317666500806808, Validation Loss: 0.10597994178533554\n",
            "Epoch 895/2500, Training Loss: 0.08309928327798843, Validation Loss: 0.10590552538633347\n",
            "Epoch 896/2500, Training Loss: 0.0830218717455864, Validation Loss: 0.10582622140645981\n",
            "Epoch 897/2500, Training Loss: 0.08294427394866943, Validation Loss: 0.1057475134730339\n",
            "Epoch 898/2500, Training Loss: 0.08286719769239426, Validation Loss: 0.10567128658294678\n",
            "Epoch 899/2500, Training Loss: 0.08279001712799072, Validation Loss: 0.10559801757335663\n",
            "Epoch 900/2500, Training Loss: 0.08271273970603943, Validation Loss: 0.10553310066461563\n",
            "Epoch 901/2500, Training Loss: 0.08263573050498962, Validation Loss: 0.10546168684959412\n",
            "Epoch 902/2500, Training Loss: 0.08255860209465027, Validation Loss: 0.10539368540048599\n",
            "Epoch 903/2500, Training Loss: 0.08248180150985718, Validation Loss: 0.10532892495393753\n",
            "Epoch 904/2500, Training Loss: 0.08240478485822678, Validation Loss: 0.10526274889707565\n",
            "Epoch 905/2500, Training Loss: 0.0823276937007904, Validation Loss: 0.10519415140151978\n",
            "Epoch 906/2500, Training Loss: 0.08225078135728836, Validation Loss: 0.10512708127498627\n",
            "Epoch 907/2500, Training Loss: 0.08217426389455795, Validation Loss: 0.10506199300289154\n",
            "Epoch 908/2500, Training Loss: 0.08209791779518127, Validation Loss: 0.10500062257051468\n",
            "Epoch 909/2500, Training Loss: 0.08202192187309265, Validation Loss: 0.10493222624063492\n",
            "Epoch 910/2500, Training Loss: 0.0819452553987503, Validation Loss: 0.10487024486064911\n",
            "Epoch 911/2500, Training Loss: 0.08186890929937363, Validation Loss: 0.1048073023557663\n",
            "Epoch 912/2500, Training Loss: 0.08179259300231934, Validation Loss: 0.10474482923746109\n",
            "Epoch 913/2500, Training Loss: 0.08171677589416504, Validation Loss: 0.10467365384101868\n",
            "Epoch 914/2500, Training Loss: 0.08164092898368835, Validation Loss: 0.1046028658747673\n",
            "Epoch 915/2500, Training Loss: 0.08156632632017136, Validation Loss: 0.10453574359416962\n",
            "Epoch 916/2500, Training Loss: 0.08149118721485138, Validation Loss: 0.1044713482260704\n",
            "Epoch 917/2500, Training Loss: 0.08141719549894333, Validation Loss: 0.10440120100975037\n",
            "Epoch 918/2500, Training Loss: 0.08134405314922333, Validation Loss: 0.1043320894241333\n",
            "Epoch 919/2500, Training Loss: 0.08127080649137497, Validation Loss: 0.10426492244005203\n",
            "Epoch 920/2500, Training Loss: 0.08119737356901169, Validation Loss: 0.10420392453670502\n",
            "Epoch 921/2500, Training Loss: 0.08112379908561707, Validation Loss: 0.1041473001241684\n",
            "Epoch 922/2500, Training Loss: 0.0810515508055687, Validation Loss: 0.1040802001953125\n",
            "Epoch 923/2500, Training Loss: 0.08097857981920242, Validation Loss: 0.10400400310754776\n",
            "Epoch 924/2500, Training Loss: 0.08090657740831375, Validation Loss: 0.10392796993255615\n",
            "Epoch 925/2500, Training Loss: 0.08083534240722656, Validation Loss: 0.1038590595126152\n",
            "Epoch 926/2500, Training Loss: 0.0807640552520752, Validation Loss: 0.10379189252853394\n",
            "Epoch 927/2500, Training Loss: 0.08069241046905518, Validation Loss: 0.10372546315193176\n",
            "Epoch 928/2500, Training Loss: 0.08062057197093964, Validation Loss: 0.10366490483283997\n",
            "Epoch 929/2500, Training Loss: 0.08054840564727783, Validation Loss: 0.10360392183065414\n",
            "Epoch 930/2500, Training Loss: 0.08047602325677872, Validation Loss: 0.10354076325893402\n",
            "Epoch 931/2500, Training Loss: 0.08040353655815125, Validation Loss: 0.10347484797239304\n",
            "Epoch 932/2500, Training Loss: 0.08033378422260284, Validation Loss: 0.10340309143066406\n",
            "Epoch 933/2500, Training Loss: 0.08026175945997238, Validation Loss: 0.10332832485437393\n",
            "Epoch 934/2500, Training Loss: 0.08018872886896133, Validation Loss: 0.10325470566749573\n",
            "Epoch 935/2500, Training Loss: 0.08011724054813385, Validation Loss: 0.10318221896886826\n",
            "Epoch 936/2500, Training Loss: 0.08004556596279144, Validation Loss: 0.10311140865087509\n",
            "Epoch 937/2500, Training Loss: 0.07997354865074158, Validation Loss: 0.10304275900125504\n",
            "Epoch 938/2500, Training Loss: 0.0799013078212738, Validation Loss: 0.10298199951648712\n",
            "Epoch 939/2500, Training Loss: 0.07982923835515976, Validation Loss: 0.10292485356330872\n",
            "Epoch 940/2500, Training Loss: 0.07975773513317108, Validation Loss: 0.10286165773868561\n",
            "Epoch 941/2500, Training Loss: 0.07968749105930328, Validation Loss: 0.10278933495283127\n",
            "Epoch 942/2500, Training Loss: 0.0796160027384758, Validation Loss: 0.10271283239126205\n",
            "Epoch 943/2500, Training Loss: 0.07954546064138412, Validation Loss: 0.10263639688491821\n",
            "Epoch 944/2500, Training Loss: 0.07947543263435364, Validation Loss: 0.10256635397672653\n",
            "Epoch 945/2500, Training Loss: 0.07940534502267838, Validation Loss: 0.10250764340162277\n",
            "Epoch 946/2500, Training Loss: 0.07933473587036133, Validation Loss: 0.1024554967880249\n",
            "Epoch 947/2500, Training Loss: 0.07926449179649353, Validation Loss: 0.10239768773317337\n",
            "Epoch 948/2500, Training Loss: 0.0791955292224884, Validation Loss: 0.10232637077569962\n",
            "Epoch 949/2500, Training Loss: 0.07912594079971313, Validation Loss: 0.10224558413028717\n",
            "Epoch 950/2500, Training Loss: 0.07905764132738113, Validation Loss: 0.10216785222291946\n",
            "Epoch 951/2500, Training Loss: 0.07898964732885361, Validation Loss: 0.10209617763757706\n",
            "Epoch 952/2500, Training Loss: 0.07892146706581116, Validation Loss: 0.10203507542610168\n",
            "Epoch 953/2500, Training Loss: 0.07885367423295975, Validation Loss: 0.1019832044839859\n",
            "Epoch 954/2500, Training Loss: 0.07878587394952774, Validation Loss: 0.10193493962287903\n",
            "Epoch 955/2500, Training Loss: 0.0787179097533226, Validation Loss: 0.10188984125852585\n",
            "Epoch 956/2500, Training Loss: 0.07865123450756073, Validation Loss: 0.1018296554684639\n",
            "Epoch 957/2500, Training Loss: 0.07858403772115707, Validation Loss: 0.10175695270299911\n",
            "Epoch 958/2500, Training Loss: 0.07851624488830566, Validation Loss: 0.10169059038162231\n",
            "Epoch 959/2500, Training Loss: 0.07844943553209305, Validation Loss: 0.10163068771362305\n",
            "Epoch 960/2500, Training Loss: 0.0783829540014267, Validation Loss: 0.10157781839370728\n",
            "Epoch 961/2500, Training Loss: 0.0783163458108902, Validation Loss: 0.10152329504489899\n",
            "Epoch 962/2500, Training Loss: 0.07824960350990295, Validation Loss: 0.10146638005971909\n",
            "Epoch 963/2500, Training Loss: 0.0781828984618187, Validation Loss: 0.10140570998191833\n",
            "Epoch 964/2500, Training Loss: 0.07811649143695831, Validation Loss: 0.10134099423885345\n",
            "Epoch 965/2500, Training Loss: 0.07804986089468002, Validation Loss: 0.10127490013837814\n",
            "Epoch 966/2500, Training Loss: 0.07798358798027039, Validation Loss: 0.10120433568954468\n",
            "Epoch 967/2500, Training Loss: 0.07791773974895477, Validation Loss: 0.10114174336194992\n",
            "Epoch 968/2500, Training Loss: 0.07785158604383469, Validation Loss: 0.10108666867017746\n",
            "Epoch 969/2500, Training Loss: 0.07778522372245789, Validation Loss: 0.10103236883878708\n",
            "Epoch 970/2500, Training Loss: 0.07771924138069153, Validation Loss: 0.10097808390855789\n",
            "Epoch 971/2500, Training Loss: 0.0776534229516983, Validation Loss: 0.10091820359230042\n",
            "Epoch 972/2500, Training Loss: 0.07758744806051254, Validation Loss: 0.10085878521203995\n",
            "Epoch 973/2500, Training Loss: 0.07752174884080887, Validation Loss: 0.10078978538513184\n",
            "Epoch 974/2500, Training Loss: 0.0774565264582634, Validation Loss: 0.10072805732488632\n",
            "Epoch 975/2500, Training Loss: 0.07739117741584778, Validation Loss: 0.10067403316497803\n",
            "Epoch 976/2500, Training Loss: 0.07732576876878738, Validation Loss: 0.10062813758850098\n",
            "Epoch 977/2500, Training Loss: 0.07726031541824341, Validation Loss: 0.10058432072401047\n",
            "Epoch 978/2500, Training Loss: 0.07719488441944122, Validation Loss: 0.10053909569978714\n",
            "Epoch 979/2500, Training Loss: 0.0771307498216629, Validation Loss: 0.10048070549964905\n",
            "Epoch 980/2500, Training Loss: 0.07706522941589355, Validation Loss: 0.10041617602109909\n",
            "Epoch 981/2500, Training Loss: 0.07699944078922272, Validation Loss: 0.10035883635282516\n",
            "Epoch 982/2500, Training Loss: 0.07693425565958023, Validation Loss: 0.10030723363161087\n",
            "Epoch 983/2500, Training Loss: 0.07686871290206909, Validation Loss: 0.1002550944685936\n",
            "Epoch 984/2500, Training Loss: 0.07680264860391617, Validation Loss: 0.10020177811384201\n",
            "Epoch 985/2500, Training Loss: 0.07673639059066772, Validation Loss: 0.10014905780553818\n",
            "Epoch 986/2500, Training Loss: 0.07667042315006256, Validation Loss: 0.10010173916816711\n",
            "Epoch 987/2500, Training Loss: 0.07660453021526337, Validation Loss: 0.10004871338605881\n",
            "Epoch 988/2500, Training Loss: 0.07653853297233582, Validation Loss: 0.09999318420886993\n",
            "Epoch 989/2500, Training Loss: 0.07647249847650528, Validation Loss: 0.09994029253721237\n",
            "Epoch 990/2500, Training Loss: 0.07640662789344788, Validation Loss: 0.09988629817962646\n",
            "Epoch 991/2500, Training Loss: 0.07634064555168152, Validation Loss: 0.09983111917972565\n",
            "Epoch 992/2500, Training Loss: 0.07627499848604202, Validation Loss: 0.09977394342422485\n",
            "Epoch 993/2500, Training Loss: 0.07620963454246521, Validation Loss: 0.09972014278173447\n",
            "Epoch 994/2500, Training Loss: 0.07614371180534363, Validation Loss: 0.09967616945505142\n",
            "Epoch 995/2500, Training Loss: 0.07607808709144592, Validation Loss: 0.0996338501572609\n",
            "Epoch 996/2500, Training Loss: 0.07601270079612732, Validation Loss: 0.0995870977640152\n",
            "Epoch 997/2500, Training Loss: 0.07594717293977737, Validation Loss: 0.09953617304563522\n",
            "Epoch 998/2500, Training Loss: 0.07588179409503937, Validation Loss: 0.09948699176311493\n",
            "Epoch 999/2500, Training Loss: 0.07581646740436554, Validation Loss: 0.09943877160549164\n",
            "Epoch 1000/2500, Training Loss: 0.07575105875730515, Validation Loss: 0.09939179569482803\n",
            "Epoch 1001/2500, Training Loss: 0.07568571716547012, Validation Loss: 0.0993410050868988\n",
            "Epoch 1002/2500, Training Loss: 0.07562055438756943, Validation Loss: 0.09928896278142929\n",
            "Epoch 1003/2500, Training Loss: 0.0755554735660553, Validation Loss: 0.09924017637968063\n",
            "Epoch 1004/2500, Training Loss: 0.07549049705266953, Validation Loss: 0.09919483214616776\n",
            "Epoch 1005/2500, Training Loss: 0.07542551308870316, Validation Loss: 0.09914961457252502\n",
            "Epoch 1006/2500, Training Loss: 0.0753607377409935, Validation Loss: 0.09910313040018082\n",
            "Epoch 1007/2500, Training Loss: 0.0752960741519928, Validation Loss: 0.09906195849180222\n",
            "Epoch 1008/2500, Training Loss: 0.07523147761821747, Validation Loss: 0.09902602434158325\n",
            "Epoch 1009/2500, Training Loss: 0.07516725361347198, Validation Loss: 0.09899356961250305\n",
            "Epoch 1010/2500, Training Loss: 0.07510288804769516, Validation Loss: 0.09895831346511841\n",
            "Epoch 1011/2500, Training Loss: 0.07503976672887802, Validation Loss: 0.09892430901527405\n",
            "Epoch 1012/2500, Training Loss: 0.07497707009315491, Validation Loss: 0.09888888895511627\n",
            "Epoch 1013/2500, Training Loss: 0.07491441071033478, Validation Loss: 0.09884122014045715\n",
            "Epoch 1014/2500, Training Loss: 0.07485201209783554, Validation Loss: 0.09879302233457565\n",
            "Epoch 1015/2500, Training Loss: 0.0747896060347557, Validation Loss: 0.09874562919139862\n",
            "Epoch 1016/2500, Training Loss: 0.07472766190767288, Validation Loss: 0.09869995713233948\n",
            "Epoch 1017/2500, Training Loss: 0.07466591894626617, Validation Loss: 0.0986604318022728\n",
            "Epoch 1018/2500, Training Loss: 0.0746041014790535, Validation Loss: 0.09861579537391663\n",
            "Epoch 1019/2500, Training Loss: 0.07454244792461395, Validation Loss: 0.09856635332107544\n",
            "Epoch 1020/2500, Training Loss: 0.07448173314332962, Validation Loss: 0.09852153807878494\n",
            "Epoch 1021/2500, Training Loss: 0.07441969960927963, Validation Loss: 0.09848161041736603\n",
            "Epoch 1022/2500, Training Loss: 0.07435943931341171, Validation Loss: 0.09843694418668747\n",
            "Epoch 1023/2500, Training Loss: 0.07429803907871246, Validation Loss: 0.09838386625051498\n",
            "Epoch 1024/2500, Training Loss: 0.07423649728298187, Validation Loss: 0.09833100438117981\n",
            "Epoch 1025/2500, Training Loss: 0.07417641580104828, Validation Loss: 0.09828431904315948\n",
            "Epoch 1026/2500, Training Loss: 0.07411447912454605, Validation Loss: 0.09824373573064804\n",
            "Epoch 1027/2500, Training Loss: 0.0740542784333229, Validation Loss: 0.0981932207942009\n",
            "Epoch 1028/2500, Training Loss: 0.073993980884552, Validation Loss: 0.09813924133777618\n",
            "Epoch 1029/2500, Training Loss: 0.0739336758852005, Validation Loss: 0.0980863943696022\n",
            "Epoch 1030/2500, Training Loss: 0.07387581467628479, Validation Loss: 0.09804432094097137\n",
            "Epoch 1031/2500, Training Loss: 0.07381638884544373, Validation Loss: 0.09800970554351807\n",
            "Epoch 1032/2500, Training Loss: 0.07375858724117279, Validation Loss: 0.09796696156263351\n",
            "Epoch 1033/2500, Training Loss: 0.07370108366012573, Validation Loss: 0.09791668504476547\n",
            "Epoch 1034/2500, Training Loss: 0.07364155352115631, Validation Loss: 0.09786161780357361\n",
            "Epoch 1035/2500, Training Loss: 0.07358456403017044, Validation Loss: 0.09781347960233688\n",
            "Epoch 1036/2500, Training Loss: 0.07352648675441742, Validation Loss: 0.0977754220366478\n",
            "Epoch 1037/2500, Training Loss: 0.07346730679273605, Validation Loss: 0.09773751348257065\n",
            "Epoch 1038/2500, Training Loss: 0.07341071963310242, Validation Loss: 0.09769732505083084\n",
            "Epoch 1039/2500, Training Loss: 0.07335338741540909, Validation Loss: 0.09765233844518661\n",
            "Epoch 1040/2500, Training Loss: 0.0732945054769516, Validation Loss: 0.09759750217199326\n",
            "Epoch 1041/2500, Training Loss: 0.07323750108480453, Validation Loss: 0.09754890948534012\n",
            "Epoch 1042/2500, Training Loss: 0.07318071275949478, Validation Loss: 0.0975121334195137\n",
            "Epoch 1043/2500, Training Loss: 0.07312247902154922, Validation Loss: 0.09746871888637543\n",
            "Epoch 1044/2500, Training Loss: 0.07306482642889023, Validation Loss: 0.09741901606321335\n",
            "Epoch 1045/2500, Training Loss: 0.07300889492034912, Validation Loss: 0.09736815094947815\n",
            "Epoch 1046/2500, Training Loss: 0.0729525163769722, Validation Loss: 0.09731575101613998\n",
            "Epoch 1047/2500, Training Loss: 0.07289540767669678, Validation Loss: 0.09726307541131973\n",
            "Epoch 1048/2500, Training Loss: 0.07283784449100494, Validation Loss: 0.09721065312623978\n",
            "Epoch 1049/2500, Training Loss: 0.07278169691562653, Validation Loss: 0.0971619263291359\n",
            "Epoch 1050/2500, Training Loss: 0.07272633165121078, Validation Loss: 0.09711254388093948\n",
            "Epoch 1051/2500, Training Loss: 0.07266931980848312, Validation Loss: 0.09705936908721924\n",
            "Epoch 1052/2500, Training Loss: 0.07261280715465546, Validation Loss: 0.09700652956962585\n",
            "Epoch 1053/2500, Training Loss: 0.0725577101111412, Validation Loss: 0.09695432335138321\n",
            "Epoch 1054/2500, Training Loss: 0.07250134646892548, Validation Loss: 0.09690718352794647\n",
            "Epoch 1055/2500, Training Loss: 0.072445347905159, Validation Loss: 0.09686516970396042\n",
            "Epoch 1056/2500, Training Loss: 0.07239022105932236, Validation Loss: 0.09682285040616989\n",
            "Epoch 1057/2500, Training Loss: 0.07233633100986481, Validation Loss: 0.09677485376596451\n",
            "Epoch 1058/2500, Training Loss: 0.07227993756532669, Validation Loss: 0.09672259539365768\n",
            "Epoch 1059/2500, Training Loss: 0.0722254067659378, Validation Loss: 0.09667206555604935\n",
            "Epoch 1060/2500, Training Loss: 0.07217062264680862, Validation Loss: 0.09662379324436188\n",
            "Epoch 1061/2500, Training Loss: 0.07211697101593018, Validation Loss: 0.09657604992389679\n",
            "Epoch 1062/2500, Training Loss: 0.07206270843744278, Validation Loss: 0.09652981907129288\n",
            "Epoch 1063/2500, Training Loss: 0.0720081776380539, Validation Loss: 0.09648343920707703\n",
            "Epoch 1064/2500, Training Loss: 0.07195338606834412, Validation Loss: 0.09643901884555817\n",
            "Epoch 1065/2500, Training Loss: 0.07189955562353134, Validation Loss: 0.09639596939086914\n",
            "Epoch 1066/2500, Training Loss: 0.0718461275100708, Validation Loss: 0.09635182470083237\n",
            "Epoch 1067/2500, Training Loss: 0.07179322838783264, Validation Loss: 0.0963025689125061\n",
            "Epoch 1068/2500, Training Loss: 0.07173926383256912, Validation Loss: 0.09625031799077988\n",
            "Epoch 1069/2500, Training Loss: 0.07168500125408173, Validation Loss: 0.09620067477226257\n",
            "Epoch 1070/2500, Training Loss: 0.07163243740797043, Validation Loss: 0.09615733474493027\n",
            "Epoch 1071/2500, Training Loss: 0.07157938927412033, Validation Loss: 0.09611933678388596\n",
            "Epoch 1072/2500, Training Loss: 0.07152541726827621, Validation Loss: 0.0960841029882431\n",
            "Epoch 1073/2500, Training Loss: 0.07147236168384552, Validation Loss: 0.09604768455028534\n",
            "Epoch 1074/2500, Training Loss: 0.07142013311386108, Validation Loss: 0.09600815176963806\n",
            "Epoch 1075/2500, Training Loss: 0.07136619836091995, Validation Loss: 0.09596655517816544\n",
            "Epoch 1076/2500, Training Loss: 0.07131306827068329, Validation Loss: 0.09592064470052719\n",
            "Epoch 1077/2500, Training Loss: 0.0712619423866272, Validation Loss: 0.09587443619966507\n",
            "Epoch 1078/2500, Training Loss: 0.071209616959095, Validation Loss: 0.09583227336406708\n",
            "Epoch 1079/2500, Training Loss: 0.07115554809570312, Validation Loss: 0.09579440951347351\n",
            "Epoch 1080/2500, Training Loss: 0.07110569626092911, Validation Loss: 0.09575711935758591\n",
            "Epoch 1081/2500, Training Loss: 0.07105331867933273, Validation Loss: 0.09571631997823715\n",
            "Epoch 1082/2500, Training Loss: 0.07099901884794235, Validation Loss: 0.09567051380872726\n",
            "Epoch 1083/2500, Training Loss: 0.07094953209161758, Validation Loss: 0.09562403708696365\n",
            "Epoch 1084/2500, Training Loss: 0.07089661061763763, Validation Loss: 0.09557855874300003\n",
            "Epoch 1085/2500, Training Loss: 0.07084275037050247, Validation Loss: 0.09552986919879913\n",
            "Epoch 1086/2500, Training Loss: 0.07079192996025085, Validation Loss: 0.09548327326774597\n",
            "Epoch 1087/2500, Training Loss: 0.07073964178562164, Validation Loss: 0.09543491899967194\n",
            "Epoch 1088/2500, Training Loss: 0.07068682461977005, Validation Loss: 0.09539499133825302\n",
            "Epoch 1089/2500, Training Loss: 0.07063516229391098, Validation Loss: 0.09535008668899536\n",
            "Epoch 1090/2500, Training Loss: 0.07058389484882355, Validation Loss: 0.0953076183795929\n",
            "Epoch 1091/2500, Training Loss: 0.0705321654677391, Validation Loss: 0.09525894373655319\n",
            "Epoch 1092/2500, Training Loss: 0.07048072665929794, Validation Loss: 0.09520633518695831\n",
            "Epoch 1093/2500, Training Loss: 0.07042958587408066, Validation Loss: 0.09515726566314697\n",
            "Epoch 1094/2500, Training Loss: 0.07037845999002457, Validation Loss: 0.0951143130660057\n",
            "Epoch 1095/2500, Training Loss: 0.0703272670507431, Validation Loss: 0.09507495164871216\n",
            "Epoch 1096/2500, Training Loss: 0.07027538865804672, Validation Loss: 0.09503934532403946\n",
            "Epoch 1097/2500, Training Loss: 0.07022476941347122, Validation Loss: 0.09499863535165787\n",
            "Epoch 1098/2500, Training Loss: 0.07017424702644348, Validation Loss: 0.09495434910058975\n",
            "Epoch 1099/2500, Training Loss: 0.07012205570936203, Validation Loss: 0.09490734338760376\n",
            "Epoch 1100/2500, Training Loss: 0.0700729489326477, Validation Loss: 0.09486517310142517\n",
            "Epoch 1101/2500, Training Loss: 0.07002219557762146, Validation Loss: 0.09482858330011368\n",
            "Epoch 1102/2500, Training Loss: 0.06997045874595642, Validation Loss: 0.09478644281625748\n",
            "Epoch 1103/2500, Training Loss: 0.06991948187351227, Validation Loss: 0.09474131464958191\n",
            "Epoch 1104/2500, Training Loss: 0.06986889988183975, Validation Loss: 0.09469476342201233\n",
            "Epoch 1105/2500, Training Loss: 0.06981772184371948, Validation Loss: 0.09465156495571136\n",
            "Epoch 1106/2500, Training Loss: 0.06976734101772308, Validation Loss: 0.09461230784654617\n",
            "Epoch 1107/2500, Training Loss: 0.06971675157546997, Validation Loss: 0.0945713147521019\n",
            "Epoch 1108/2500, Training Loss: 0.06966661661863327, Validation Loss: 0.09452575445175171\n",
            "Epoch 1109/2500, Training Loss: 0.0696161612868309, Validation Loss: 0.09447450190782547\n",
            "Epoch 1110/2500, Training Loss: 0.06956709921360016, Validation Loss: 0.0944313257932663\n",
            "Epoch 1111/2500, Training Loss: 0.0695163905620575, Validation Loss: 0.09439338743686676\n",
            "Epoch 1112/2500, Training Loss: 0.06946646422147751, Validation Loss: 0.09435442090034485\n",
            "Epoch 1113/2500, Training Loss: 0.06941652297973633, Validation Loss: 0.09431448578834534\n",
            "Epoch 1114/2500, Training Loss: 0.06936633586883545, Validation Loss: 0.09427627176046371\n",
            "Epoch 1115/2500, Training Loss: 0.06931773573160172, Validation Loss: 0.09423713386058807\n",
            "Epoch 1116/2500, Training Loss: 0.06926775723695755, Validation Loss: 0.09419206529855728\n",
            "Epoch 1117/2500, Training Loss: 0.0692182257771492, Validation Loss: 0.09415086358785629\n",
            "Epoch 1118/2500, Training Loss: 0.06916903704404831, Validation Loss: 0.09411385655403137\n",
            "Epoch 1119/2500, Training Loss: 0.06911936402320862, Validation Loss: 0.09407613426446915\n",
            "Epoch 1120/2500, Training Loss: 0.06906997412443161, Validation Loss: 0.09403868019580841\n",
            "Epoch 1121/2500, Training Loss: 0.06902105361223221, Validation Loss: 0.09400050342082977\n",
            "Epoch 1122/2500, Training Loss: 0.06897208094596863, Validation Loss: 0.09396588802337646\n",
            "Epoch 1123/2500, Training Loss: 0.06892327219247818, Validation Loss: 0.09392907470464706\n",
            "Epoch 1124/2500, Training Loss: 0.06887452304363251, Validation Loss: 0.09388821572065353\n",
            "Epoch 1125/2500, Training Loss: 0.06882511079311371, Validation Loss: 0.09384710341691971\n",
            "Epoch 1126/2500, Training Loss: 0.06877724826335907, Validation Loss: 0.09381114691495895\n",
            "Epoch 1127/2500, Training Loss: 0.06872748583555222, Validation Loss: 0.09378007054328918\n",
            "Epoch 1128/2500, Training Loss: 0.06867995113134384, Validation Loss: 0.09374471008777618\n",
            "Epoch 1129/2500, Training Loss: 0.06863267719745636, Validation Loss: 0.09371059387922287\n",
            "Epoch 1130/2500, Training Loss: 0.06858402490615845, Validation Loss: 0.09367391467094421\n",
            "Epoch 1131/2500, Training Loss: 0.06853508949279785, Validation Loss: 0.09363154321908951\n",
            "Epoch 1132/2500, Training Loss: 0.06848661601543427, Validation Loss: 0.09359542280435562\n",
            "Epoch 1133/2500, Training Loss: 0.0684366524219513, Validation Loss: 0.09356193989515305\n",
            "Epoch 1134/2500, Training Loss: 0.06838971376419067, Validation Loss: 0.09352443367242813\n",
            "Epoch 1135/2500, Training Loss: 0.06834080070257187, Validation Loss: 0.09348516911268234\n",
            "Epoch 1136/2500, Training Loss: 0.06829099357128143, Validation Loss: 0.09345222264528275\n",
            "Epoch 1137/2500, Training Loss: 0.06824194639921188, Validation Loss: 0.09342151880264282\n",
            "Epoch 1138/2500, Training Loss: 0.06819432228803635, Validation Loss: 0.09338924288749695\n",
            "Epoch 1139/2500, Training Loss: 0.06814607977867126, Validation Loss: 0.09335225075483322\n",
            "Epoch 1140/2500, Training Loss: 0.06809697300195694, Validation Loss: 0.09331519156694412\n",
            "Epoch 1141/2500, Training Loss: 0.06804827600717545, Validation Loss: 0.09328000247478485\n",
            "Epoch 1142/2500, Training Loss: 0.06799869239330292, Validation Loss: 0.09324340522289276\n",
            "Epoch 1143/2500, Training Loss: 0.06795191019773483, Validation Loss: 0.09320458769798279\n",
            "Epoch 1144/2500, Training Loss: 0.06790348887443542, Validation Loss: 0.09316688776016235\n",
            "Epoch 1145/2500, Training Loss: 0.06785470992326736, Validation Loss: 0.09313211590051651\n",
            "Epoch 1146/2500, Training Loss: 0.06780597567558289, Validation Loss: 0.09310385584831238\n",
            "Epoch 1147/2500, Training Loss: 0.06775776296854019, Validation Loss: 0.09307404607534409\n",
            "Epoch 1148/2500, Training Loss: 0.06771072745323181, Validation Loss: 0.09304200112819672\n",
            "Epoch 1149/2500, Training Loss: 0.06766287982463837, Validation Loss: 0.09300818294286728\n",
            "Epoch 1150/2500, Training Loss: 0.06761443614959717, Validation Loss: 0.09297078102827072\n",
            "Epoch 1151/2500, Training Loss: 0.06756516546010971, Validation Loss: 0.0929303988814354\n",
            "Epoch 1152/2500, Training Loss: 0.06751847267150879, Validation Loss: 0.09289814531803131\n",
            "Epoch 1153/2500, Training Loss: 0.06747027486562729, Validation Loss: 0.09287390857934952\n",
            "Epoch 1154/2500, Training Loss: 0.06742340326309204, Validation Loss: 0.09284544736146927\n",
            "Epoch 1155/2500, Training Loss: 0.06737585365772247, Validation Loss: 0.09281302243471146\n",
            "Epoch 1156/2500, Training Loss: 0.06732774525880814, Validation Loss: 0.09277569502592087\n",
            "Epoch 1157/2500, Training Loss: 0.06727997958660126, Validation Loss: 0.09273620694875717\n",
            "Epoch 1158/2500, Training Loss: 0.06723368167877197, Validation Loss: 0.09270105510950089\n",
            "Epoch 1159/2500, Training Loss: 0.06718508899211884, Validation Loss: 0.09267032146453857\n",
            "Epoch 1160/2500, Training Loss: 0.06713776290416718, Validation Loss: 0.09263861179351807\n",
            "Epoch 1161/2500, Training Loss: 0.06709179282188416, Validation Loss: 0.09260743856430054\n",
            "Epoch 1162/2500, Training Loss: 0.06704460084438324, Validation Loss: 0.0925775021314621\n",
            "Epoch 1163/2500, Training Loss: 0.06699652969837189, Validation Loss: 0.0925486832857132\n",
            "Epoch 1164/2500, Training Loss: 0.06694788485765457, Validation Loss: 0.09251619875431061\n",
            "Epoch 1165/2500, Training Loss: 0.06690044701099396, Validation Loss: 0.09248031675815582\n",
            "Epoch 1166/2500, Training Loss: 0.06685370951890945, Validation Loss: 0.09245000779628754\n",
            "Epoch 1167/2500, Training Loss: 0.06680471450090408, Validation Loss: 0.09242545813322067\n",
            "Epoch 1168/2500, Training Loss: 0.06675856560468674, Validation Loss: 0.09240139275789261\n",
            "Epoch 1169/2500, Training Loss: 0.06671274453401566, Validation Loss: 0.09237682074308395\n",
            "Epoch 1170/2500, Training Loss: 0.0666666105389595, Validation Loss: 0.09234923869371414\n",
            "Epoch 1171/2500, Training Loss: 0.06661811470985413, Validation Loss: 0.09232059866189957\n",
            "Epoch 1172/2500, Training Loss: 0.06657201051712036, Validation Loss: 0.09228693693876266\n",
            "Epoch 1173/2500, Training Loss: 0.06652509421110153, Validation Loss: 0.09225241094827652\n",
            "Epoch 1174/2500, Training Loss: 0.06647782772779465, Validation Loss: 0.09222118556499481\n",
            "Epoch 1175/2500, Training Loss: 0.06642935425043106, Validation Loss: 0.09218517690896988\n",
            "Epoch 1176/2500, Training Loss: 0.06638281792402267, Validation Loss: 0.09214900434017181\n",
            "Epoch 1177/2500, Training Loss: 0.06633637100458145, Validation Loss: 0.09211437404155731\n",
            "Epoch 1178/2500, Training Loss: 0.06628911197185516, Validation Loss: 0.09208335727453232\n",
            "Epoch 1179/2500, Training Loss: 0.0662437230348587, Validation Loss: 0.0920553058385849\n",
            "Epoch 1180/2500, Training Loss: 0.066197469830513, Validation Loss: 0.09202481806278229\n",
            "Epoch 1181/2500, Training Loss: 0.0661504790186882, Validation Loss: 0.091996930539608\n",
            "Epoch 1182/2500, Training Loss: 0.06610345095396042, Validation Loss: 0.09196788817644119\n",
            "Epoch 1183/2500, Training Loss: 0.06605948507785797, Validation Loss: 0.09194456040859222\n",
            "Epoch 1184/2500, Training Loss: 0.06601238995790482, Validation Loss: 0.09192124009132385\n",
            "Epoch 1185/2500, Training Loss: 0.06596555560827255, Validation Loss: 0.09189599752426147\n",
            "Epoch 1186/2500, Training Loss: 0.0659206435084343, Validation Loss: 0.0918683335185051\n",
            "Epoch 1187/2500, Training Loss: 0.06587515771389008, Validation Loss: 0.09183859825134277\n",
            "Epoch 1188/2500, Training Loss: 0.06582840532064438, Validation Loss: 0.09181515127420425\n",
            "Epoch 1189/2500, Training Loss: 0.06578206270933151, Validation Loss: 0.09179206192493439\n",
            "Epoch 1190/2500, Training Loss: 0.06573768705129623, Validation Loss: 0.09176656603813171\n",
            "Epoch 1191/2500, Training Loss: 0.06569221615791321, Validation Loss: 0.09173689782619476\n",
            "Epoch 1192/2500, Training Loss: 0.06564558297395706, Validation Loss: 0.09170849621295929\n",
            "Epoch 1193/2500, Training Loss: 0.06559925526380539, Validation Loss: 0.09167644381523132\n",
            "Epoch 1194/2500, Training Loss: 0.06555373221635818, Validation Loss: 0.09164226055145264\n",
            "Epoch 1195/2500, Training Loss: 0.0655084103345871, Validation Loss: 0.09161550551652908\n",
            "Epoch 1196/2500, Training Loss: 0.06546277552843094, Validation Loss: 0.09158650040626526\n",
            "Epoch 1197/2500, Training Loss: 0.06541812419891357, Validation Loss: 0.09155834466218948\n",
            "Epoch 1198/2500, Training Loss: 0.0653725266456604, Validation Loss: 0.09153091162443161\n",
            "Epoch 1199/2500, Training Loss: 0.06532614678144455, Validation Loss: 0.09150446206331253\n",
            "Epoch 1200/2500, Training Loss: 0.06528255343437195, Validation Loss: 0.09147021174430847\n",
            "Epoch 1201/2500, Training Loss: 0.06523727625608444, Validation Loss: 0.09143812209367752\n",
            "Epoch 1202/2500, Training Loss: 0.06519171595573425, Validation Loss: 0.09140817821025848\n",
            "Epoch 1203/2500, Training Loss: 0.06514710187911987, Validation Loss: 0.09138075262308121\n",
            "Epoch 1204/2500, Training Loss: 0.06510241329669952, Validation Loss: 0.0913601890206337\n",
            "Epoch 1205/2500, Training Loss: 0.0650564432144165, Validation Loss: 0.09133564680814743\n",
            "Epoch 1206/2500, Training Loss: 0.06501258909702301, Validation Loss: 0.09130697697401047\n",
            "Epoch 1207/2500, Training Loss: 0.06496752798557281, Validation Loss: 0.09127464145421982\n",
            "Epoch 1208/2500, Training Loss: 0.06492214649915695, Validation Loss: 0.091248519718647\n",
            "Epoch 1209/2500, Training Loss: 0.06487933546304703, Validation Loss: 0.09122166782617569\n",
            "Epoch 1210/2500, Training Loss: 0.0648341104388237, Validation Loss: 0.09119562804698944\n",
            "Epoch 1211/2500, Training Loss: 0.06478801369667053, Validation Loss: 0.09116857498884201\n",
            "Epoch 1212/2500, Training Loss: 0.06474536657333374, Validation Loss: 0.09114599227905273\n",
            "Epoch 1213/2500, Training Loss: 0.06469994783401489, Validation Loss: 0.09111811220645905\n",
            "Epoch 1214/2500, Training Loss: 0.06465395539999008, Validation Loss: 0.09108927100896835\n",
            "Epoch 1215/2500, Training Loss: 0.0646112784743309, Validation Loss: 0.09106149524450302\n",
            "Epoch 1216/2500, Training Loss: 0.06456620991230011, Validation Loss: 0.09104134887456894\n",
            "Epoch 1217/2500, Training Loss: 0.06452160328626633, Validation Loss: 0.09101401269435883\n",
            "Epoch 1218/2500, Training Loss: 0.06447815150022507, Validation Loss: 0.090977743268013\n",
            "Epoch 1219/2500, Training Loss: 0.06443312019109726, Validation Loss: 0.09094801545143127\n",
            "Epoch 1220/2500, Training Loss: 0.06438940018415451, Validation Loss: 0.090920589864254\n",
            "Epoch 1221/2500, Training Loss: 0.06434642523527145, Validation Loss: 0.09089533984661102\n",
            "Epoch 1222/2500, Training Loss: 0.06430190801620483, Validation Loss: 0.09087242931127548\n",
            "Epoch 1223/2500, Training Loss: 0.06425656378269196, Validation Loss: 0.09085414558649063\n",
            "Epoch 1224/2500, Training Loss: 0.064212866127491, Validation Loss: 0.09082575142383575\n",
            "Epoch 1225/2500, Training Loss: 0.0641695037484169, Validation Loss: 0.09078957885503769\n",
            "Epoch 1226/2500, Training Loss: 0.06412481516599655, Validation Loss: 0.09076346457004547\n",
            "Epoch 1227/2500, Training Loss: 0.06408144533634186, Validation Loss: 0.0907377377152443\n",
            "Epoch 1228/2500, Training Loss: 0.06403780728578568, Validation Loss: 0.09071341156959534\n",
            "Epoch 1229/2500, Training Loss: 0.06399460136890411, Validation Loss: 0.09068755805492401\n",
            "Epoch 1230/2500, Training Loss: 0.0639510527253151, Validation Loss: 0.0906553789973259\n",
            "Epoch 1231/2500, Training Loss: 0.06390795111656189, Validation Loss: 0.09063224494457245\n",
            "Epoch 1232/2500, Training Loss: 0.06386467069387436, Validation Loss: 0.09060847014188766\n",
            "Epoch 1233/2500, Training Loss: 0.0638209879398346, Validation Loss: 0.09057784825563431\n",
            "Epoch 1234/2500, Training Loss: 0.06377852708101273, Validation Loss: 0.09054498374462128\n",
            "Epoch 1235/2500, Training Loss: 0.06373563408851624, Validation Loss: 0.0905214250087738\n",
            "Epoch 1236/2500, Training Loss: 0.06369143724441528, Validation Loss: 0.09049668908119202\n",
            "Epoch 1237/2500, Training Loss: 0.06365042924880981, Validation Loss: 0.0904766172170639\n",
            "Epoch 1238/2500, Training Loss: 0.06360623240470886, Validation Loss: 0.0904567614197731\n",
            "Epoch 1239/2500, Training Loss: 0.06356287747621536, Validation Loss: 0.09043710678815842\n",
            "Epoch 1240/2500, Training Loss: 0.06352100521326065, Validation Loss: 0.09041273593902588\n",
            "Epoch 1241/2500, Training Loss: 0.06347787380218506, Validation Loss: 0.09039172530174255\n",
            "Epoch 1242/2500, Training Loss: 0.06343423575162888, Validation Loss: 0.09036613255739212\n",
            "Epoch 1243/2500, Training Loss: 0.06339215487241745, Validation Loss: 0.090336874127388\n",
            "Epoch 1244/2500, Training Loss: 0.06334906071424484, Validation Loss: 0.09031562507152557\n",
            "Epoch 1245/2500, Training Loss: 0.06330719590187073, Validation Loss: 0.09029925614595413\n",
            "Epoch 1246/2500, Training Loss: 0.0632648766040802, Validation Loss: 0.09027624130249023\n",
            "Epoch 1247/2500, Training Loss: 0.06322207301855087, Validation Loss: 0.09025082737207413\n",
            "Epoch 1248/2500, Training Loss: 0.06318021565675735, Validation Loss: 0.09022055566310883\n",
            "Epoch 1249/2500, Training Loss: 0.06313847750425339, Validation Loss: 0.09020321071147919\n",
            "Epoch 1250/2500, Training Loss: 0.06309495121240616, Validation Loss: 0.09018414467573166\n",
            "Epoch 1251/2500, Training Loss: 0.06305290758609772, Validation Loss: 0.09015986323356628\n",
            "Epoch 1252/2500, Training Loss: 0.06301065534353256, Validation Loss: 0.09013155102729797\n",
            "Epoch 1253/2500, Training Loss: 0.06296928972005844, Validation Loss: 0.09011523425579071\n",
            "Epoch 1254/2500, Training Loss: 0.0629267543554306, Validation Loss: 0.09010370075702667\n",
            "Epoch 1255/2500, Training Loss: 0.06288566440343857, Validation Loss: 0.09008437395095825\n",
            "Epoch 1256/2500, Training Loss: 0.06284452974796295, Validation Loss: 0.09006691724061966\n",
            "Epoch 1257/2500, Training Loss: 0.06280267983675003, Validation Loss: 0.09004918485879898\n",
            "Epoch 1258/2500, Training Loss: 0.06276062875986099, Validation Loss: 0.09002556651830673\n",
            "Epoch 1259/2500, Training Loss: 0.06271802634000778, Validation Loss: 0.08999833464622498\n",
            "Epoch 1260/2500, Training Loss: 0.06267517060041428, Validation Loss: 0.08996811509132385\n",
            "Epoch 1261/2500, Training Loss: 0.06263238191604614, Validation Loss: 0.08994153141975403\n",
            "Epoch 1262/2500, Training Loss: 0.06259153038263321, Validation Loss: 0.08992701768875122\n",
            "Epoch 1263/2500, Training Loss: 0.0625481978058815, Validation Loss: 0.08991657942533493\n",
            "Epoch 1264/2500, Training Loss: 0.06250721961259842, Validation Loss: 0.08990126103162766\n",
            "Epoch 1265/2500, Training Loss: 0.06246603652834892, Validation Loss: 0.08988896012306213\n",
            "Epoch 1266/2500, Training Loss: 0.06242324411869049, Validation Loss: 0.08987154811620712\n",
            "Epoch 1267/2500, Training Loss: 0.06238136067986488, Validation Loss: 0.08985109627246857\n",
            "Epoch 1268/2500, Training Loss: 0.06233781948685646, Validation Loss: 0.08982741087675095\n",
            "Epoch 1269/2500, Training Loss: 0.06229518726468086, Validation Loss: 0.08980473130941391\n",
            "Epoch 1270/2500, Training Loss: 0.06225001439452171, Validation Loss: 0.08978044986724854\n",
            "Epoch 1271/2500, Training Loss: 0.062209874391555786, Validation Loss: 0.08976567536592484\n",
            "Epoch 1272/2500, Training Loss: 0.06216760352253914, Validation Loss: 0.08976823836565018\n",
            "Epoch 1273/2500, Training Loss: 0.06212184205651283, Validation Loss: 0.08978421986103058\n",
            "Epoch 1274/2500, Training Loss: 0.06207900866866112, Validation Loss: 0.08979428559541702\n",
            "Epoch 1275/2500, Training Loss: 0.06203775852918625, Validation Loss: 0.08980268985033035\n",
            "Epoch 1276/2500, Training Loss: 0.06199394911527634, Validation Loss: 0.08980260044336319\n",
            "Epoch 1277/2500, Training Loss: 0.06195077672600746, Validation Loss: 0.08979830145835876\n",
            "Epoch 1278/2500, Training Loss: 0.06190641596913338, Validation Loss: 0.08979074656963348\n",
            "Epoch 1279/2500, Training Loss: 0.0618620365858078, Validation Loss: 0.08978303521871567\n",
            "Epoch 1280/2500, Training Loss: 0.06182141974568367, Validation Loss: 0.0897894948720932\n",
            "Epoch 1281/2500, Training Loss: 0.06177850067615509, Validation Loss: 0.08980120718479156\n",
            "Epoch 1282/2500, Training Loss: 0.06173396855592728, Validation Loss: 0.08980700373649597\n",
            "Epoch 1283/2500, Training Loss: 0.06169132888317108, Validation Loss: 0.08980771899223328\n",
            "Epoch 1284/2500, Training Loss: 0.06164873391389847, Validation Loss: 0.08980436623096466\n",
            "Epoch 1285/2500, Training Loss: 0.06160565838217735, Validation Loss: 0.08979781717061996\n",
            "Epoch 1286/2500, Training Loss: 0.06156347319483757, Validation Loss: 0.08978883922100067\n",
            "Epoch 1287/2500, Training Loss: 0.061520159244537354, Validation Loss: 0.08977720886468887\n",
            "Epoch 1288/2500, Training Loss: 0.061477091163396835, Validation Loss: 0.08977010101079941\n",
            "Epoch 1289/2500, Training Loss: 0.061435602605342865, Validation Loss: 0.08977733552455902\n",
            "Epoch 1290/2500, Training Loss: 0.06139293313026428, Validation Loss: 0.08979109674692154\n",
            "Epoch 1291/2500, Training Loss: 0.06135048344731331, Validation Loss: 0.08979824185371399\n",
            "Epoch 1292/2500, Training Loss: 0.061309464275836945, Validation Loss: 0.0898032858967781\n",
            "Epoch 1293/2500, Training Loss: 0.06126714125275612, Validation Loss: 0.0898059606552124\n",
            "Epoch 1294/2500, Training Loss: 0.06122443452477455, Validation Loss: 0.08980096131563187\n",
            "Epoch 1295/2500, Training Loss: 0.06118252873420715, Validation Loss: 0.08979348093271255\n",
            "Epoch 1296/2500, Training Loss: 0.061139918863773346, Validation Loss: 0.08977945148944855\n",
            "Epoch 1297/2500, Training Loss: 0.06109917163848877, Validation Loss: 0.0897660180926323\n",
            "Epoch 1298/2500, Training Loss: 0.06105753779411316, Validation Loss: 0.08975933492183685\n",
            "Epoch 1299/2500, Training Loss: 0.06101575121283531, Validation Loss: 0.08974754065275192\n",
            "Epoch 1300/2500, Training Loss: 0.06097409501671791, Validation Loss: 0.08972997963428497\n",
            "Epoch 1301/2500, Training Loss: 0.06093239411711693, Validation Loss: 0.08971187472343445\n",
            "Epoch 1302/2500, Training Loss: 0.060891810804605484, Validation Loss: 0.08969432860612869\n",
            "Epoch 1303/2500, Training Loss: 0.060850564390420914, Validation Loss: 0.0896747037768364\n",
            "Epoch 1304/2500, Training Loss: 0.060808081179857254, Validation Loss: 0.08965342491865158\n",
            "Epoch 1305/2500, Training Loss: 0.06076660379767418, Validation Loss: 0.08963499963283539\n",
            "Epoch 1306/2500, Training Loss: 0.060726020485162735, Validation Loss: 0.0896211788058281\n",
            "Epoch 1307/2500, Training Loss: 0.06068447604775429, Validation Loss: 0.089609295129776\n",
            "Epoch 1308/2500, Training Loss: 0.06064358353614807, Validation Loss: 0.0895937979221344\n",
            "Epoch 1309/2500, Training Loss: 0.06060207262635231, Validation Loss: 0.08957675844430923\n",
            "Epoch 1310/2500, Training Loss: 0.060559529811143875, Validation Loss: 0.08955762535333633\n",
            "Epoch 1311/2500, Training Loss: 0.06051940470933914, Validation Loss: 0.08953582495450974\n",
            "Epoch 1312/2500, Training Loss: 0.06047918274998665, Validation Loss: 0.08952689170837402\n",
            "Epoch 1313/2500, Training Loss: 0.06043528765439987, Validation Loss: 0.08952373266220093\n",
            "Epoch 1314/2500, Training Loss: 0.06039559096097946, Validation Loss: 0.08951262384653091\n",
            "Epoch 1315/2500, Training Loss: 0.060355816036462784, Validation Loss: 0.08949447423219681\n",
            "Epoch 1316/2500, Training Loss: 0.06031499058008194, Validation Loss: 0.08947569131851196\n",
            "Epoch 1317/2500, Training Loss: 0.060272976756095886, Validation Loss: 0.08945640176534653\n",
            "Epoch 1318/2500, Training Loss: 0.06023012101650238, Validation Loss: 0.0894361287355423\n",
            "Epoch 1319/2500, Training Loss: 0.06018822267651558, Validation Loss: 0.08942481875419617\n",
            "Epoch 1320/2500, Training Loss: 0.060148321092128754, Validation Loss: 0.08942081034183502\n",
            "Epoch 1321/2500, Training Loss: 0.06010742112994194, Validation Loss: 0.08941230922937393\n",
            "Epoch 1322/2500, Training Loss: 0.060065366327762604, Validation Loss: 0.08940307050943375\n",
            "Epoch 1323/2500, Training Loss: 0.060024600476026535, Validation Loss: 0.08939282596111298\n",
            "Epoch 1324/2500, Training Loss: 0.05998389795422554, Validation Loss: 0.08937527984380722\n",
            "Epoch 1325/2500, Training Loss: 0.0599425733089447, Validation Loss: 0.08935216069221497\n",
            "Epoch 1326/2500, Training Loss: 0.059901684522628784, Validation Loss: 0.08933892101049423\n",
            "Epoch 1327/2500, Training Loss: 0.05986036732792854, Validation Loss: 0.08932219445705414\n",
            "Epoch 1328/2500, Training Loss: 0.059819601476192474, Validation Loss: 0.08930323272943497\n",
            "Epoch 1329/2500, Training Loss: 0.05977863445878029, Validation Loss: 0.08928286284208298\n",
            "Epoch 1330/2500, Training Loss: 0.059738025069236755, Validation Loss: 0.08926920592784882\n",
            "Epoch 1331/2500, Training Loss: 0.05969813093543053, Validation Loss: 0.08926182240247726\n",
            "Epoch 1332/2500, Training Loss: 0.0596594475209713, Validation Loss: 0.08925580978393555\n",
            "Epoch 1333/2500, Training Loss: 0.05961984023451805, Validation Loss: 0.08924800902605057\n",
            "Epoch 1334/2500, Training Loss: 0.05958186462521553, Validation Loss: 0.08923639357089996\n",
            "Epoch 1335/2500, Training Loss: 0.05954277142882347, Validation Loss: 0.08922597020864487\n",
            "Epoch 1336/2500, Training Loss: 0.05950399860739708, Validation Loss: 0.08921130746603012\n",
            "Epoch 1337/2500, Training Loss: 0.05946611985564232, Validation Loss: 0.08919505029916763\n",
            "Epoch 1338/2500, Training Loss: 0.059427112340927124, Validation Loss: 0.0891750305891037\n",
            "Epoch 1339/2500, Training Loss: 0.05938900634646416, Validation Loss: 0.08915223926305771\n",
            "Epoch 1340/2500, Training Loss: 0.05934995412826538, Validation Loss: 0.08912099152803421\n",
            "Epoch 1341/2500, Training Loss: 0.059311557561159134, Validation Loss: 0.0890921875834465\n",
            "Epoch 1342/2500, Training Loss: 0.0592728815972805, Validation Loss: 0.08906041830778122\n",
            "Epoch 1343/2500, Training Loss: 0.05923500657081604, Validation Loss: 0.08903330564498901\n",
            "Epoch 1344/2500, Training Loss: 0.05919690802693367, Validation Loss: 0.08900851756334305\n",
            "Epoch 1345/2500, Training Loss: 0.059158749878406525, Validation Loss: 0.08898669481277466\n",
            "Epoch 1346/2500, Training Loss: 0.0591202937066555, Validation Loss: 0.08896339684724808\n",
            "Epoch 1347/2500, Training Loss: 0.059082623571157455, Validation Loss: 0.08895447850227356\n",
            "Epoch 1348/2500, Training Loss: 0.05904373154044151, Validation Loss: 0.08894248306751251\n",
            "Epoch 1349/2500, Training Loss: 0.0590057373046875, Validation Loss: 0.0889231413602829\n",
            "Epoch 1350/2500, Training Loss: 0.05896800011396408, Validation Loss: 0.08889928460121155\n",
            "Epoch 1351/2500, Training Loss: 0.05893036350607872, Validation Loss: 0.08887379616498947\n",
            "Epoch 1352/2500, Training Loss: 0.05889253318309784, Validation Loss: 0.08884675800800323\n",
            "Epoch 1353/2500, Training Loss: 0.05885438248515129, Validation Loss: 0.08881766349077225\n",
            "Epoch 1354/2500, Training Loss: 0.058816976845264435, Validation Loss: 0.08879867196083069\n",
            "Epoch 1355/2500, Training Loss: 0.05877909064292908, Validation Loss: 0.08877064287662506\n",
            "Epoch 1356/2500, Training Loss: 0.05874167010188103, Validation Loss: 0.08873922377824783\n",
            "Epoch 1357/2500, Training Loss: 0.058704618364572525, Validation Loss: 0.08871391415596008\n",
            "Epoch 1358/2500, Training Loss: 0.0586666502058506, Validation Loss: 0.08869508653879166\n",
            "Epoch 1359/2500, Training Loss: 0.05862874537706375, Validation Loss: 0.08867789804935455\n",
            "Epoch 1360/2500, Training Loss: 0.05859057605266571, Validation Loss: 0.08866101503372192\n",
            "Epoch 1361/2500, Training Loss: 0.05855275318026543, Validation Loss: 0.08863677084445953\n",
            "Epoch 1362/2500, Training Loss: 0.05851570889353752, Validation Loss: 0.08862034976482391\n",
            "Epoch 1363/2500, Training Loss: 0.05847679451107979, Validation Loss: 0.08861356973648071\n",
            "Epoch 1364/2500, Training Loss: 0.05843881145119667, Validation Loss: 0.08860234916210175\n",
            "Epoch 1365/2500, Training Loss: 0.05840149521827698, Validation Loss: 0.08859021961688995\n",
            "Epoch 1366/2500, Training Loss: 0.05836443975567818, Validation Loss: 0.08857928961515427\n",
            "Epoch 1367/2500, Training Loss: 0.05832628905773163, Validation Loss: 0.08856987208127975\n",
            "Epoch 1368/2500, Training Loss: 0.05828820541501045, Validation Loss: 0.08855829387903214\n",
            "Epoch 1369/2500, Training Loss: 0.05825022608041763, Validation Loss: 0.08854150027036667\n",
            "Epoch 1370/2500, Training Loss: 0.058212749660015106, Validation Loss: 0.08851922303438187\n",
            "Epoch 1371/2500, Training Loss: 0.058174632489681244, Validation Loss: 0.08849034458398819\n",
            "Epoch 1372/2500, Training Loss: 0.05813679099082947, Validation Loss: 0.0884612649679184\n",
            "Epoch 1373/2500, Training Loss: 0.05809846147894859, Validation Loss: 0.08842853456735611\n",
            "Epoch 1374/2500, Training Loss: 0.0580606535077095, Validation Loss: 0.08839454501867294\n",
            "Epoch 1375/2500, Training Loss: 0.0580243319272995, Validation Loss: 0.08837764710187912\n",
            "Epoch 1376/2500, Training Loss: 0.05798545479774475, Validation Loss: 0.08837521821260452\n",
            "Epoch 1377/2500, Training Loss: 0.05794764310121536, Validation Loss: 0.08836767822504044\n",
            "Epoch 1378/2500, Training Loss: 0.0579107441008091, Validation Loss: 0.08835901319980621\n",
            "Epoch 1379/2500, Training Loss: 0.057873621582984924, Validation Loss: 0.0883513018488884\n",
            "Epoch 1380/2500, Training Loss: 0.05783591791987419, Validation Loss: 0.08833752572536469\n",
            "Epoch 1381/2500, Training Loss: 0.057798486202955246, Validation Loss: 0.08832022547721863\n",
            "Epoch 1382/2500, Training Loss: 0.05776045471429825, Validation Loss: 0.08829962462186813\n",
            "Epoch 1383/2500, Training Loss: 0.05772284045815468, Validation Loss: 0.08827979117631912\n",
            "Epoch 1384/2500, Training Loss: 0.05768511816859245, Validation Loss: 0.08825773745775223\n",
            "Epoch 1385/2500, Training Loss: 0.05764823406934738, Validation Loss: 0.08823709189891815\n",
            "Epoch 1386/2500, Training Loss: 0.057611431926488876, Validation Loss: 0.08822884410619736\n",
            "Epoch 1387/2500, Training Loss: 0.0575735941529274, Validation Loss: 0.08822440356016159\n",
            "Epoch 1388/2500, Training Loss: 0.05753671005368233, Validation Loss: 0.0882183313369751\n",
            "Epoch 1389/2500, Training Loss: 0.05749969184398651, Validation Loss: 0.08820755779743195\n",
            "Epoch 1390/2500, Training Loss: 0.057462289929389954, Validation Loss: 0.08819064497947693\n",
            "Epoch 1391/2500, Training Loss: 0.05742587894201279, Validation Loss: 0.08817560225725174\n",
            "Epoch 1392/2500, Training Loss: 0.057388730347156525, Validation Loss: 0.0881662592291832\n",
            "Epoch 1393/2500, Training Loss: 0.05735123157501221, Validation Loss: 0.08815471827983856\n",
            "Epoch 1394/2500, Training Loss: 0.05731412023305893, Validation Loss: 0.08813757449388504\n",
            "Epoch 1395/2500, Training Loss: 0.057276759296655655, Validation Loss: 0.08811347186565399\n",
            "Epoch 1396/2500, Training Loss: 0.057240985333919525, Validation Loss: 0.08810564875602722\n",
            "Epoch 1397/2500, Training Loss: 0.05720308795571327, Validation Loss: 0.08810698240995407\n",
            "Epoch 1398/2500, Training Loss: 0.057166073471307755, Validation Loss: 0.08810246735811234\n",
            "Epoch 1399/2500, Training Loss: 0.05712888762354851, Validation Loss: 0.08809413015842438\n",
            "Epoch 1400/2500, Training Loss: 0.057092178612947464, Validation Loss: 0.08808562159538269\n",
            "Epoch 1401/2500, Training Loss: 0.0570557527244091, Validation Loss: 0.08807540684938431\n",
            "Epoch 1402/2500, Training Loss: 0.057018328458070755, Validation Loss: 0.08806366473436356\n",
            "Epoch 1403/2500, Training Loss: 0.05698121711611748, Validation Loss: 0.08804800361394882\n",
            "Epoch 1404/2500, Training Loss: 0.05694475769996643, Validation Loss: 0.08802890032529831\n",
            "Epoch 1405/2500, Training Loss: 0.05690731108188629, Validation Loss: 0.0880114957690239\n",
            "Epoch 1406/2500, Training Loss: 0.05687067285180092, Validation Loss: 0.08800430595874786\n",
            "Epoch 1407/2500, Training Loss: 0.056834205985069275, Validation Loss: 0.08801241219043732\n",
            "Epoch 1408/2500, Training Loss: 0.05679650977253914, Validation Loss: 0.0880161002278328\n",
            "Epoch 1409/2500, Training Loss: 0.05676000565290451, Validation Loss: 0.08801407366991043\n",
            "Epoch 1410/2500, Training Loss: 0.05672350153326988, Validation Loss: 0.08800546824932098\n",
            "Epoch 1411/2500, Training Loss: 0.05668702349066734, Validation Loss: 0.08799412846565247\n",
            "Epoch 1412/2500, Training Loss: 0.05665045604109764, Validation Loss: 0.08797606825828552\n",
            "Epoch 1413/2500, Training Loss: 0.05661378055810928, Validation Loss: 0.08795687556266785\n",
            "Epoch 1414/2500, Training Loss: 0.056577760726213455, Validation Loss: 0.08794121444225311\n",
            "Epoch 1415/2500, Training Loss: 0.056541670113801956, Validation Loss: 0.08793792128562927\n",
            "Epoch 1416/2500, Training Loss: 0.05650535225868225, Validation Loss: 0.08793815225362778\n",
            "Epoch 1417/2500, Training Loss: 0.05646859481930733, Validation Loss: 0.08793728053569794\n",
            "Epoch 1418/2500, Training Loss: 0.056433189660310745, Validation Loss: 0.08793014287948608\n",
            "Epoch 1419/2500, Training Loss: 0.0563969612121582, Validation Loss: 0.08791576325893402\n",
            "Epoch 1420/2500, Training Loss: 0.056360796093940735, Validation Loss: 0.08789625763893127\n",
            "Epoch 1421/2500, Training Loss: 0.05632470175623894, Validation Loss: 0.08787460625171661\n",
            "Epoch 1422/2500, Training Loss: 0.05628811568021774, Validation Loss: 0.08785224705934525\n",
            "Epoch 1423/2500, Training Loss: 0.056251682341098785, Validation Loss: 0.08782746642827988\n",
            "Epoch 1424/2500, Training Loss: 0.05621727183461189, Validation Loss: 0.08781524747610092\n",
            "Epoch 1425/2500, Training Loss: 0.05618055537343025, Validation Loss: 0.08781683444976807\n",
            "Epoch 1426/2500, Training Loss: 0.05614472180604935, Validation Loss: 0.08781970292329788\n",
            "Epoch 1427/2500, Training Loss: 0.05610893294215202, Validation Loss: 0.08781826496124268\n",
            "Epoch 1428/2500, Training Loss: 0.056072790175676346, Validation Loss: 0.0878164991736412\n",
            "Epoch 1429/2500, Training Loss: 0.056037355214357376, Validation Loss: 0.08780571073293686\n",
            "Epoch 1430/2500, Training Loss: 0.056001707911491394, Validation Loss: 0.08779212087392807\n",
            "Epoch 1431/2500, Training Loss: 0.05596604570746422, Validation Loss: 0.0877768024802208\n",
            "Epoch 1432/2500, Training Loss: 0.05593051388859749, Validation Loss: 0.0877634733915329\n",
            "Epoch 1433/2500, Training Loss: 0.05589490756392479, Validation Loss: 0.08774731308221817\n",
            "Epoch 1434/2500, Training Loss: 0.055860552936792374, Validation Loss: 0.08774285763502121\n",
            "Epoch 1435/2500, Training Loss: 0.055823665112257004, Validation Loss: 0.08774328976869583\n",
            "Epoch 1436/2500, Training Loss: 0.05578772351145744, Validation Loss: 0.0877387747168541\n",
            "Epoch 1437/2500, Training Loss: 0.05575180426239967, Validation Loss: 0.08773396909236908\n",
            "Epoch 1438/2500, Training Loss: 0.05571660399436951, Validation Loss: 0.08772926032543182\n",
            "Epoch 1439/2500, Training Loss: 0.055680058896541595, Validation Loss: 0.08772647380828857\n",
            "Epoch 1440/2500, Training Loss: 0.05564441159367561, Validation Loss: 0.08772128075361252\n",
            "Epoch 1441/2500, Training Loss: 0.05560776963829994, Validation Loss: 0.08770987391471863\n",
            "Epoch 1442/2500, Training Loss: 0.055571530014276505, Validation Loss: 0.08769411593675613\n",
            "Epoch 1443/2500, Training Loss: 0.055535782128572464, Validation Loss: 0.08767781406641006\n",
            "Epoch 1444/2500, Training Loss: 0.05549928545951843, Validation Loss: 0.0876602828502655\n",
            "Epoch 1445/2500, Training Loss: 0.055462971329689026, Validation Loss: 0.08763621002435684\n",
            "Epoch 1446/2500, Training Loss: 0.055427778512239456, Validation Loss: 0.08762944489717484\n",
            "Epoch 1447/2500, Training Loss: 0.05539104714989662, Validation Loss: 0.08763603866100311\n",
            "Epoch 1448/2500, Training Loss: 0.05535445362329483, Validation Loss: 0.08764234930276871\n",
            "Epoch 1449/2500, Training Loss: 0.055318184196949005, Validation Loss: 0.08764804154634476\n",
            "Epoch 1450/2500, Training Loss: 0.0552821084856987, Validation Loss: 0.08765027672052383\n",
            "Epoch 1451/2500, Training Loss: 0.05524628609418869, Validation Loss: 0.08765284717082977\n",
            "Epoch 1452/2500, Training Loss: 0.05520971864461899, Validation Loss: 0.08765126764774323\n",
            "Epoch 1453/2500, Training Loss: 0.05517309904098511, Validation Loss: 0.08763978630304337\n",
            "Epoch 1454/2500, Training Loss: 0.05513738468289375, Validation Loss: 0.08761993050575256\n",
            "Epoch 1455/2500, Training Loss: 0.055100470781326294, Validation Loss: 0.08759865164756775\n",
            "Epoch 1456/2500, Training Loss: 0.05506463348865509, Validation Loss: 0.08758193254470825\n",
            "Epoch 1457/2500, Training Loss: 0.055028390139341354, Validation Loss: 0.0875665470957756\n",
            "Epoch 1458/2500, Training Loss: 0.0549938902258873, Validation Loss: 0.08757130801677704\n",
            "Epoch 1459/2500, Training Loss: 0.054956626147031784, Validation Loss: 0.08757659047842026\n",
            "Epoch 1460/2500, Training Loss: 0.05492093414068222, Validation Loss: 0.08757632225751877\n",
            "Epoch 1461/2500, Training Loss: 0.054886121302843094, Validation Loss: 0.08757144957780838\n",
            "Epoch 1462/2500, Training Loss: 0.05484999716281891, Validation Loss: 0.08755972236394882\n",
            "Epoch 1463/2500, Training Loss: 0.054814208298921585, Validation Loss: 0.0875445231795311\n",
            "Epoch 1464/2500, Training Loss: 0.05477878078818321, Validation Loss: 0.08753277361392975\n",
            "Epoch 1465/2500, Training Loss: 0.05474308878183365, Validation Loss: 0.08752406388521194\n",
            "Epoch 1466/2500, Training Loss: 0.054707448929548264, Validation Loss: 0.08753444254398346\n",
            "Epoch 1467/2500, Training Loss: 0.05467230826616287, Validation Loss: 0.08754982799291611\n",
            "Epoch 1468/2500, Training Loss: 0.05463618412613869, Validation Loss: 0.08755699545145035\n",
            "Epoch 1469/2500, Training Loss: 0.05460070073604584, Validation Loss: 0.08755707740783691\n",
            "Epoch 1470/2500, Training Loss: 0.054565172642469406, Validation Loss: 0.08755243569612503\n",
            "Epoch 1471/2500, Training Loss: 0.05452977865934372, Validation Loss: 0.087544284760952\n",
            "Epoch 1472/2500, Training Loss: 0.054493825882673264, Validation Loss: 0.0875362977385521\n",
            "Epoch 1473/2500, Training Loss: 0.05445847660303116, Validation Loss: 0.08753269165754318\n",
            "Epoch 1474/2500, Training Loss: 0.054422639310359955, Validation Loss: 0.0875282883644104\n",
            "Epoch 1475/2500, Training Loss: 0.05438697338104248, Validation Loss: 0.08751895278692245\n",
            "Epoch 1476/2500, Training Loss: 0.054352011531591415, Validation Loss: 0.0875086560845375\n",
            "Epoch 1477/2500, Training Loss: 0.054316483438014984, Validation Loss: 0.0874999612569809\n",
            "Epoch 1478/2500, Training Loss: 0.054281510412693024, Validation Loss: 0.0874939039349556\n",
            "Epoch 1479/2500, Training Loss: 0.05424635857343674, Validation Loss: 0.08748932182788849\n",
            "Epoch 1480/2500, Training Loss: 0.054210759699344635, Validation Loss: 0.08749210089445114\n",
            "Epoch 1481/2500, Training Loss: 0.05417570099234581, Validation Loss: 0.08749577403068542\n",
            "Epoch 1482/2500, Training Loss: 0.05414051190018654, Validation Loss: 0.0874963328242302\n",
            "Epoch 1483/2500, Training Loss: 0.054105617105960846, Validation Loss: 0.08749843388795853\n",
            "Epoch 1484/2500, Training Loss: 0.05407053604722023, Validation Loss: 0.08749575167894363\n",
            "Epoch 1485/2500, Training Loss: 0.05403653904795647, Validation Loss: 0.087488554418087\n",
            "Epoch 1486/2500, Training Loss: 0.05400097370147705, Validation Loss: 0.08747752755880356\n",
            "Epoch 1487/2500, Training Loss: 0.0539667084813118, Validation Loss: 0.08746881783008575\n",
            "Epoch 1488/2500, Training Loss: 0.05393227934837341, Validation Loss: 0.08746714144945145\n",
            "Epoch 1489/2500, Training Loss: 0.05389745905995369, Validation Loss: 0.08746436983346939\n",
            "Epoch 1490/2500, Training Loss: 0.05386241525411606, Validation Loss: 0.08746930211782455\n",
            "Epoch 1491/2500, Training Loss: 0.053829360753297806, Validation Loss: 0.08746639639139175\n",
            "Epoch 1492/2500, Training Loss: 0.053794749081134796, Validation Loss: 0.08745525032281876\n",
            "Epoch 1493/2500, Training Loss: 0.05375932902097702, Validation Loss: 0.08744850009679794\n",
            "Epoch 1494/2500, Training Loss: 0.053725194185972214, Validation Loss: 0.08744671940803528\n",
            "Epoch 1495/2500, Training Loss: 0.05369069054722786, Validation Loss: 0.08744195103645325\n",
            "Epoch 1496/2500, Training Loss: 0.05365584045648575, Validation Loss: 0.08743373304605484\n",
            "Epoch 1497/2500, Training Loss: 0.05362267047166824, Validation Loss: 0.08742810785770416\n",
            "Epoch 1498/2500, Training Loss: 0.05358760058879852, Validation Loss: 0.08742963522672653\n",
            "Epoch 1499/2500, Training Loss: 0.05355383828282356, Validation Loss: 0.0874321386218071\n",
            "Epoch 1500/2500, Training Loss: 0.05352054908871651, Validation Loss: 0.08743634819984436\n",
            "Epoch 1501/2500, Training Loss: 0.053486328572034836, Validation Loss: 0.08744210749864578\n",
            "Epoch 1502/2500, Training Loss: 0.05345262587070465, Validation Loss: 0.08744016289710999\n",
            "Epoch 1503/2500, Training Loss: 0.053418613970279694, Validation Loss: 0.08743149042129517\n",
            "Epoch 1504/2500, Training Loss: 0.053384892642498016, Validation Loss: 0.08741053938865662\n",
            "Epoch 1505/2500, Training Loss: 0.05335012078285217, Validation Loss: 0.08739043027162552\n",
            "Epoch 1506/2500, Training Loss: 0.053316857665777206, Validation Loss: 0.08737673610448837\n",
            "Epoch 1507/2500, Training Loss: 0.05328337475657463, Validation Loss: 0.08737437427043915\n",
            "Epoch 1508/2500, Training Loss: 0.053249944001436234, Validation Loss: 0.08737923949956894\n",
            "Epoch 1509/2500, Training Loss: 0.05321529507637024, Validation Loss: 0.0873810350894928\n",
            "Epoch 1510/2500, Training Loss: 0.05318164825439453, Validation Loss: 0.0873839408159256\n",
            "Epoch 1511/2500, Training Loss: 0.05314844846725464, Validation Loss: 0.08738496154546738\n",
            "Epoch 1512/2500, Training Loss: 0.053114939481019974, Validation Loss: 0.08738163113594055\n",
            "Epoch 1513/2500, Training Loss: 0.0530812107026577, Validation Loss: 0.08737952262163162\n",
            "Epoch 1514/2500, Training Loss: 0.05304792523384094, Validation Loss: 0.08737185597419739\n",
            "Epoch 1515/2500, Training Loss: 0.053014565259218216, Validation Loss: 0.08736293017864227\n",
            "Epoch 1516/2500, Training Loss: 0.052981067448854446, Validation Loss: 0.08735448867082596\n",
            "Epoch 1517/2500, Training Loss: 0.05294835567474365, Validation Loss: 0.0873502641916275\n",
            "Epoch 1518/2500, Training Loss: 0.05291443690657616, Validation Loss: 0.08735334873199463\n",
            "Epoch 1519/2500, Training Loss: 0.052881643176078796, Validation Loss: 0.08735530823469162\n",
            "Epoch 1520/2500, Training Loss: 0.052848994731903076, Validation Loss: 0.08735176175832748\n",
            "Epoch 1521/2500, Training Loss: 0.05281587690114975, Validation Loss: 0.08734600245952606\n",
            "Epoch 1522/2500, Training Loss: 0.05278210714459419, Validation Loss: 0.08734007179737091\n",
            "Epoch 1523/2500, Training Loss: 0.05274800956249237, Validation Loss: 0.08733899891376495\n",
            "Epoch 1524/2500, Training Loss: 0.052715204656124115, Validation Loss: 0.08733605593442917\n",
            "Epoch 1525/2500, Training Loss: 0.05268177017569542, Validation Loss: 0.08733747154474258\n",
            "Epoch 1526/2500, Training Loss: 0.052648600190877914, Validation Loss: 0.08733221888542175\n",
            "Epoch 1527/2500, Training Loss: 0.05261547863483429, Validation Loss: 0.08731885999441147\n",
            "Epoch 1528/2500, Training Loss: 0.05258278176188469, Validation Loss: 0.08731433749198914\n",
            "Epoch 1529/2500, Training Loss: 0.05255039036273956, Validation Loss: 0.0873212069272995\n",
            "Epoch 1530/2500, Training Loss: 0.05251657962799072, Validation Loss: 0.0873287245631218\n",
            "Epoch 1531/2500, Training Loss: 0.05248367413878441, Validation Loss: 0.08732878416776657\n",
            "Epoch 1532/2500, Training Loss: 0.052450623363256454, Validation Loss: 0.08732123672962189\n",
            "Epoch 1533/2500, Training Loss: 0.05241783708333969, Validation Loss: 0.08730969578027725\n",
            "Epoch 1534/2500, Training Loss: 0.05238465592265129, Validation Loss: 0.08730465918779373\n",
            "Epoch 1535/2500, Training Loss: 0.05235230550169945, Validation Loss: 0.08730029314756393\n",
            "Epoch 1536/2500, Training Loss: 0.052318744361400604, Validation Loss: 0.08729888498783112\n",
            "Epoch 1537/2500, Training Loss: 0.052285660058259964, Validation Loss: 0.08730560541152954\n",
            "Epoch 1538/2500, Training Loss: 0.05225251242518425, Validation Loss: 0.08730856329202652\n",
            "Epoch 1539/2500, Training Loss: 0.052219994366168976, Validation Loss: 0.08731046319007874\n",
            "Epoch 1540/2500, Training Loss: 0.052186962217092514, Validation Loss: 0.08730366080999374\n",
            "Epoch 1541/2500, Training Loss: 0.052154116332530975, Validation Loss: 0.08729200065135956\n",
            "Epoch 1542/2500, Training Loss: 0.052121613174676895, Validation Loss: 0.08728842437267303\n",
            "Epoch 1543/2500, Training Loss: 0.052088748663663864, Validation Loss: 0.08729241788387299\n",
            "Epoch 1544/2500, Training Loss: 0.05205652490258217, Validation Loss: 0.08729852735996246\n",
            "Epoch 1545/2500, Training Loss: 0.05202377587556839, Validation Loss: 0.08730054646730423\n",
            "Epoch 1546/2500, Training Loss: 0.05199066549539566, Validation Loss: 0.08729231357574463\n",
            "Epoch 1547/2500, Training Loss: 0.051958661526441574, Validation Loss: 0.08728183060884476\n",
            "Epoch 1548/2500, Training Loss: 0.051926255226135254, Validation Loss: 0.08727389574050903\n",
            "Epoch 1549/2500, Training Loss: 0.051893383264541626, Validation Loss: 0.08727070689201355\n",
            "Epoch 1550/2500, Training Loss: 0.051860254257917404, Validation Loss: 0.08727791905403137\n",
            "Epoch 1551/2500, Training Loss: 0.05182747170329094, Validation Loss: 0.08728689700365067\n",
            "Epoch 1552/2500, Training Loss: 0.05179538577795029, Validation Loss: 0.08728650212287903\n",
            "Epoch 1553/2500, Training Loss: 0.05176284909248352, Validation Loss: 0.08728751540184021\n",
            "Epoch 1554/2500, Training Loss: 0.0517297238111496, Validation Loss: 0.0872860923409462\n",
            "Epoch 1555/2500, Training Loss: 0.051697149872779846, Validation Loss: 0.08728396892547607\n",
            "Epoch 1556/2500, Training Loss: 0.051664870232343674, Validation Loss: 0.08728212118148804\n",
            "Epoch 1557/2500, Training Loss: 0.05163220688700676, Validation Loss: 0.08727692067623138\n",
            "Epoch 1558/2500, Training Loss: 0.0515996590256691, Validation Loss: 0.0872730016708374\n",
            "Epoch 1559/2500, Training Loss: 0.05156758055090904, Validation Loss: 0.08727036416530609\n",
            "Epoch 1560/2500, Training Loss: 0.05153462290763855, Validation Loss: 0.08726304769515991\n",
            "Epoch 1561/2500, Training Loss: 0.05150282755494118, Validation Loss: 0.08725763857364655\n",
            "Epoch 1562/2500, Training Loss: 0.051470398902893066, Validation Loss: 0.0872592106461525\n",
            "Epoch 1563/2500, Training Loss: 0.05143800750374794, Validation Loss: 0.08726111054420471\n",
            "Epoch 1564/2500, Training Loss: 0.051406510174274445, Validation Loss: 0.08725689351558685\n",
            "Epoch 1565/2500, Training Loss: 0.05137452855706215, Validation Loss: 0.08724500238895416\n",
            "Epoch 1566/2500, Training Loss: 0.051341865211725235, Validation Loss: 0.08723338693380356\n",
            "Epoch 1567/2500, Training Loss: 0.051310501992702484, Validation Loss: 0.08722981065511703\n",
            "Epoch 1568/2500, Training Loss: 0.05127863213419914, Validation Loss: 0.08722860366106033\n",
            "Epoch 1569/2500, Training Loss: 0.0512470118701458, Validation Loss: 0.08723226189613342\n",
            "Epoch 1570/2500, Training Loss: 0.05121450498700142, Validation Loss: 0.08723573386669159\n",
            "Epoch 1571/2500, Training Loss: 0.051182374358177185, Validation Loss: 0.0872378721833229\n",
            "Epoch 1572/2500, Training Loss: 0.05115199089050293, Validation Loss: 0.08723229914903641\n",
            "Epoch 1573/2500, Training Loss: 0.0511198416352272, Validation Loss: 0.0872189849615097\n",
            "Epoch 1574/2500, Training Loss: 0.051088348031044006, Validation Loss: 0.08720681816339493\n",
            "Epoch 1575/2500, Training Loss: 0.051056575030088425, Validation Loss: 0.08720015734434128\n",
            "Epoch 1576/2500, Training Loss: 0.051025066524744034, Validation Loss: 0.08719684183597565\n",
            "Epoch 1577/2500, Training Loss: 0.05099336430430412, Validation Loss: 0.0872112512588501\n",
            "Epoch 1578/2500, Training Loss: 0.05095941200852394, Validation Loss: 0.08724161982536316\n",
            "Epoch 1579/2500, Training Loss: 0.050924915820360184, Validation Loss: 0.0872797966003418\n",
            "Epoch 1580/2500, Training Loss: 0.05088979750871658, Validation Loss: 0.08732180297374725\n",
            "Epoch 1581/2500, Training Loss: 0.05085388198494911, Validation Loss: 0.08736402541399002\n",
            "Epoch 1582/2500, Training Loss: 0.050818271934986115, Validation Loss: 0.08740938454866409\n",
            "Epoch 1583/2500, Training Loss: 0.05078274384140968, Validation Loss: 0.08744485676288605\n",
            "Epoch 1584/2500, Training Loss: 0.05074634402990341, Validation Loss: 0.0874757394194603\n",
            "Epoch 1585/2500, Training Loss: 0.0507119782269001, Validation Loss: 0.08750922232866287\n",
            "Epoch 1586/2500, Training Loss: 0.05067705735564232, Validation Loss: 0.08754196763038635\n",
            "Epoch 1587/2500, Training Loss: 0.050643086433410645, Validation Loss: 0.08757463097572327\n",
            "Epoch 1588/2500, Training Loss: 0.05060955137014389, Validation Loss: 0.08761053532361984\n",
            "Epoch 1589/2500, Training Loss: 0.05057550594210625, Validation Loss: 0.08764296025037766\n",
            "Epoch 1590/2500, Training Loss: 0.05054198205471039, Validation Loss: 0.08767015486955643\n",
            "Epoch 1591/2500, Training Loss: 0.05050834268331528, Validation Loss: 0.08769532293081284\n",
            "Epoch 1592/2500, Training Loss: 0.050475023686885834, Validation Loss: 0.0877169594168663\n",
            "Epoch 1593/2500, Training Loss: 0.05044204369187355, Validation Loss: 0.08772175014019012\n",
            "Epoch 1594/2500, Training Loss: 0.050408124923706055, Validation Loss: 0.08771465718746185\n",
            "Epoch 1595/2500, Training Loss: 0.05037464573979378, Validation Loss: 0.08770760148763657\n",
            "Epoch 1596/2500, Training Loss: 0.05034131184220314, Validation Loss: 0.08769521862268448\n",
            "Epoch 1597/2500, Training Loss: 0.05030770227313042, Validation Loss: 0.08767693489789963\n",
            "Epoch 1598/2500, Training Loss: 0.050274770706892014, Validation Loss: 0.08765924721956253\n",
            "Epoch 1599/2500, Training Loss: 0.05024058744311333, Validation Loss: 0.08764360100030899\n",
            "Epoch 1600/2500, Training Loss: 0.05020592734217644, Validation Loss: 0.08762741088867188\n",
            "Epoch 1601/2500, Training Loss: 0.050173331052064896, Validation Loss: 0.0875987634062767\n",
            "Epoch 1602/2500, Training Loss: 0.05013924092054367, Validation Loss: 0.08756368607282639\n",
            "Epoch 1603/2500, Training Loss: 0.050105225294828415, Validation Loss: 0.08753612637519836\n",
            "Epoch 1604/2500, Training Loss: 0.05007217824459076, Validation Loss: 0.08751718699932098\n",
            "Epoch 1605/2500, Training Loss: 0.0500386506319046, Validation Loss: 0.08750507235527039\n",
            "Epoch 1606/2500, Training Loss: 0.05000527575612068, Validation Loss: 0.08749525249004364\n",
            "Epoch 1607/2500, Training Loss: 0.049971915781497955, Validation Loss: 0.08748306334018707\n",
            "Epoch 1608/2500, Training Loss: 0.04993892088532448, Validation Loss: 0.08746494352817535\n",
            "Epoch 1609/2500, Training Loss: 0.04990624636411667, Validation Loss: 0.08744633942842484\n",
            "Epoch 1610/2500, Training Loss: 0.049873095005750656, Validation Loss: 0.08743464946746826\n",
            "Epoch 1611/2500, Training Loss: 0.04984036087989807, Validation Loss: 0.08742814511060715\n",
            "Epoch 1612/2500, Training Loss: 0.04980773851275444, Validation Loss: 0.08742107450962067\n",
            "Epoch 1613/2500, Training Loss: 0.049774814397096634, Validation Loss: 0.08741965889930725\n",
            "Epoch 1614/2500, Training Loss: 0.04974175617098808, Validation Loss: 0.08741995692253113\n",
            "Epoch 1615/2500, Training Loss: 0.04970874637365341, Validation Loss: 0.08742573857307434\n",
            "Epoch 1616/2500, Training Loss: 0.049676086753606796, Validation Loss: 0.08743695914745331\n",
            "Epoch 1617/2500, Training Loss: 0.049643322825431824, Validation Loss: 0.08744529634714127\n",
            "Epoch 1618/2500, Training Loss: 0.04961048439145088, Validation Loss: 0.08745560050010681\n",
            "Epoch 1619/2500, Training Loss: 0.04957832768559456, Validation Loss: 0.08745985478162766\n",
            "Epoch 1620/2500, Training Loss: 0.04954530671238899, Validation Loss: 0.08746527880430222\n",
            "Epoch 1621/2500, Training Loss: 0.04951276630163193, Validation Loss: 0.08746795356273651\n",
            "Epoch 1622/2500, Training Loss: 0.04948119819164276, Validation Loss: 0.08747249841690063\n",
            "Epoch 1623/2500, Training Loss: 0.0494484081864357, Validation Loss: 0.08747437596321106\n",
            "Epoch 1624/2500, Training Loss: 0.04941652715206146, Validation Loss: 0.08747030049562454\n",
            "Epoch 1625/2500, Training Loss: 0.049385394901037216, Validation Loss: 0.08747056871652603\n",
            "Epoch 1626/2500, Training Loss: 0.04935329407453537, Validation Loss: 0.08747176080942154\n",
            "Epoch 1627/2500, Training Loss: 0.04932071641087532, Validation Loss: 0.08747362345457077\n",
            "Epoch 1628/2500, Training Loss: 0.04928968846797943, Validation Loss: 0.08747243881225586\n",
            "Epoch 1629/2500, Training Loss: 0.04925787076354027, Validation Loss: 0.08747438341379166\n",
            "Epoch 1630/2500, Training Loss: 0.04922504350543022, Validation Loss: 0.08746901899576187\n",
            "Epoch 1631/2500, Training Loss: 0.04919351264834404, Validation Loss: 0.08745836466550827\n",
            "Epoch 1632/2500, Training Loss: 0.049161966890096664, Validation Loss: 0.08744853734970093\n",
            "Epoch 1633/2500, Training Loss: 0.0491303876042366, Validation Loss: 0.08743675798177719\n",
            "Epoch 1634/2500, Training Loss: 0.04909881576895714, Validation Loss: 0.08742978423833847\n",
            "Epoch 1635/2500, Training Loss: 0.049067214131355286, Validation Loss: 0.08742440491914749\n",
            "Epoch 1636/2500, Training Loss: 0.049035388976335526, Validation Loss: 0.08741962164640427\n",
            "Epoch 1637/2500, Training Loss: 0.049004338681697845, Validation Loss: 0.08740955591201782\n",
            "Epoch 1638/2500, Training Loss: 0.04897289723157883, Validation Loss: 0.08739049732685089\n",
            "Epoch 1639/2500, Training Loss: 0.048941511660814285, Validation Loss: 0.08736935257911682\n",
            "Epoch 1640/2500, Training Loss: 0.04891054704785347, Validation Loss: 0.08735688030719757\n",
            "Epoch 1641/2500, Training Loss: 0.04887932538986206, Validation Loss: 0.08735030889511108\n",
            "Epoch 1642/2500, Training Loss: 0.048847656697034836, Validation Loss: 0.08734682202339172\n",
            "Epoch 1643/2500, Training Loss: 0.04881709814071655, Validation Loss: 0.08733589947223663\n",
            "Epoch 1644/2500, Training Loss: 0.04878579080104828, Validation Loss: 0.08731742948293686\n",
            "Epoch 1645/2500, Training Loss: 0.04875434562563896, Validation Loss: 0.08730445057153702\n",
            "Epoch 1646/2500, Training Loss: 0.0487239845097065, Validation Loss: 0.08729541301727295\n",
            "Epoch 1647/2500, Training Loss: 0.04869307577610016, Validation Loss: 0.08729228377342224\n",
            "Epoch 1648/2500, Training Loss: 0.048661645501852036, Validation Loss: 0.087289959192276\n",
            "Epoch 1649/2500, Training Loss: 0.04863037168979645, Validation Loss: 0.08727942407131195\n",
            "Epoch 1650/2500, Training Loss: 0.04859970510005951, Validation Loss: 0.08726328611373901\n",
            "Epoch 1651/2500, Training Loss: 0.0485687293112278, Validation Loss: 0.08725001662969589\n",
            "Epoch 1652/2500, Training Loss: 0.04853792116045952, Validation Loss: 0.08723966032266617\n",
            "Epoch 1653/2500, Training Loss: 0.04850711300969124, Validation Loss: 0.08723033964633942\n",
            "Epoch 1654/2500, Training Loss: 0.048476435244083405, Validation Loss: 0.08722655475139618\n",
            "Epoch 1655/2500, Training Loss: 0.04844572767615318, Validation Loss: 0.0872252881526947\n",
            "Epoch 1656/2500, Training Loss: 0.04841532185673714, Validation Loss: 0.08722613751888275\n",
            "Epoch 1657/2500, Training Loss: 0.048384349793195724, Validation Loss: 0.08722627907991409\n",
            "Epoch 1658/2500, Training Loss: 0.0483543798327446, Validation Loss: 0.08722059428691864\n",
            "Epoch 1659/2500, Training Loss: 0.04832315817475319, Validation Loss: 0.08720451593399048\n",
            "Epoch 1660/2500, Training Loss: 0.04829297587275505, Validation Loss: 0.08719433099031448\n",
            "Epoch 1661/2500, Training Loss: 0.04826229810714722, Validation Loss: 0.08718948811292648\n",
            "Epoch 1662/2500, Training Loss: 0.048231642693281174, Validation Loss: 0.08718803524971008\n",
            "Epoch 1663/2500, Training Loss: 0.04820072278380394, Validation Loss: 0.08718652278184891\n",
            "Epoch 1664/2500, Training Loss: 0.048171184957027435, Validation Loss: 0.08717754483222961\n",
            "Epoch 1665/2500, Training Loss: 0.0481400266289711, Validation Loss: 0.0871618390083313\n",
            "Epoch 1666/2500, Training Loss: 0.048110004514455795, Validation Loss: 0.08715212345123291\n",
            "Epoch 1667/2500, Training Loss: 0.04807980731129646, Validation Loss: 0.0871439203619957\n",
            "Epoch 1668/2500, Training Loss: 0.048049721866846085, Validation Loss: 0.08714155852794647\n",
            "Epoch 1669/2500, Training Loss: 0.04801864176988602, Validation Loss: 0.08714006841182709\n",
            "Epoch 1670/2500, Training Loss: 0.0479881577193737, Validation Loss: 0.08713921904563904\n",
            "Epoch 1671/2500, Training Loss: 0.047957416623830795, Validation Loss: 0.08713338524103165\n",
            "Epoch 1672/2500, Training Loss: 0.047927677631378174, Validation Loss: 0.08712462335824966\n",
            "Epoch 1673/2500, Training Loss: 0.04789688438177109, Validation Loss: 0.08711092174053192\n",
            "Epoch 1674/2500, Training Loss: 0.047866564244031906, Validation Loss: 0.08709785342216492\n",
            "Epoch 1675/2500, Training Loss: 0.04783610254526138, Validation Loss: 0.08708349615335464\n",
            "Epoch 1676/2500, Training Loss: 0.04780646786093712, Validation Loss: 0.0870765745639801\n",
            "Epoch 1677/2500, Training Loss: 0.04777603968977928, Validation Loss: 0.08707533031702042\n",
            "Epoch 1678/2500, Training Loss: 0.0477454774081707, Validation Loss: 0.08707334846258163\n",
            "Epoch 1679/2500, Training Loss: 0.047715216875076294, Validation Loss: 0.08707240223884583\n",
            "Epoch 1680/2500, Training Loss: 0.0476858951151371, Validation Loss: 0.08706703782081604\n",
            "Epoch 1681/2500, Training Loss: 0.047655604779720306, Validation Loss: 0.08705796301364899\n",
            "Epoch 1682/2500, Training Loss: 0.04762562736868858, Validation Loss: 0.08704637736082077\n",
            "Epoch 1683/2500, Training Loss: 0.04759574681520462, Validation Loss: 0.08703698217868805\n",
            "Epoch 1684/2500, Training Loss: 0.04756647348403931, Validation Loss: 0.08702991902828217\n",
            "Epoch 1685/2500, Training Loss: 0.04753744229674339, Validation Loss: 0.08702495694160461\n",
            "Epoch 1686/2500, Training Loss: 0.047507308423519135, Validation Loss: 0.08702242374420166\n",
            "Epoch 1687/2500, Training Loss: 0.047477398067712784, Validation Loss: 0.08702119439840317\n",
            "Epoch 1688/2500, Training Loss: 0.04744771122932434, Validation Loss: 0.0870203822851181\n",
            "Epoch 1689/2500, Training Loss: 0.04741957411170006, Validation Loss: 0.08701099455356598\n",
            "Epoch 1690/2500, Training Loss: 0.04739002510905266, Validation Loss: 0.08699306100606918\n",
            "Epoch 1691/2500, Training Loss: 0.04735933616757393, Validation Loss: 0.08697696775197983\n",
            "Epoch 1692/2500, Training Loss: 0.04732989892363548, Validation Loss: 0.08696358650922775\n",
            "Epoch 1693/2500, Training Loss: 0.0473020039498806, Validation Loss: 0.08695283532142639\n",
            "Epoch 1694/2500, Training Loss: 0.047272466123104095, Validation Loss: 0.08694739639759064\n",
            "Epoch 1695/2500, Training Loss: 0.04724287986755371, Validation Loss: 0.08694774657487869\n",
            "Epoch 1696/2500, Training Loss: 0.0472135916352272, Validation Loss: 0.08695091307163239\n",
            "Epoch 1697/2500, Training Loss: 0.0471835620701313, Validation Loss: 0.08695478737354279\n",
            "Epoch 1698/2500, Training Loss: 0.04715491458773613, Validation Loss: 0.08694726973772049\n",
            "Epoch 1699/2500, Training Loss: 0.047126419842243195, Validation Loss: 0.08693741261959076\n",
            "Epoch 1700/2500, Training Loss: 0.04709627479314804, Validation Loss: 0.08693090081214905\n",
            "Epoch 1701/2500, Training Loss: 0.047066979110240936, Validation Loss: 0.0869230106472969\n",
            "Epoch 1702/2500, Training Loss: 0.04703881964087486, Validation Loss: 0.08691176772117615\n",
            "Epoch 1703/2500, Training Loss: 0.047009844332933426, Validation Loss: 0.08690306544303894\n",
            "Epoch 1704/2500, Training Loss: 0.04698055982589722, Validation Loss: 0.08689861744642258\n",
            "Epoch 1705/2500, Training Loss: 0.04695199057459831, Validation Loss: 0.08689528703689575\n",
            "Epoch 1706/2500, Training Loss: 0.0469234324991703, Validation Loss: 0.08689305931329727\n",
            "Epoch 1707/2500, Training Loss: 0.046894047409296036, Validation Loss: 0.08688835799694061\n",
            "Epoch 1708/2500, Training Loss: 0.04686528071761131, Validation Loss: 0.086878702044487\n",
            "Epoch 1709/2500, Training Loss: 0.046836867928504944, Validation Loss: 0.08686619997024536\n",
            "Epoch 1710/2500, Training Loss: 0.04680832475423813, Validation Loss: 0.08685898035764694\n",
            "Epoch 1711/2500, Training Loss: 0.046779680997133255, Validation Loss: 0.08685476332902908\n",
            "Epoch 1712/2500, Training Loss: 0.04675133526325226, Validation Loss: 0.08685538172721863\n",
            "Epoch 1713/2500, Training Loss: 0.046723052859306335, Validation Loss: 0.08685244619846344\n",
            "Epoch 1714/2500, Training Loss: 0.04669473320245743, Validation Loss: 0.08684615045785904\n",
            "Epoch 1715/2500, Training Loss: 0.046665750443935394, Validation Loss: 0.08683617413043976\n",
            "Epoch 1716/2500, Training Loss: 0.04663752764463425, Validation Loss: 0.08682262152433395\n",
            "Epoch 1717/2500, Training Loss: 0.04660964757204056, Validation Loss: 0.08681361377239227\n",
            "Epoch 1718/2500, Training Loss: 0.046581510454416275, Validation Loss: 0.08680827915668488\n",
            "Epoch 1719/2500, Training Loss: 0.04655328392982483, Validation Loss: 0.08680589497089386\n",
            "Epoch 1720/2500, Training Loss: 0.046524956822395325, Validation Loss: 0.08680637925863266\n",
            "Epoch 1721/2500, Training Loss: 0.04649651050567627, Validation Loss: 0.08680649101734161\n",
            "Epoch 1722/2500, Training Loss: 0.046468861401081085, Validation Loss: 0.0867931917309761\n",
            "Epoch 1723/2500, Training Loss: 0.04644005373120308, Validation Loss: 0.08677905052900314\n",
            "Epoch 1724/2500, Training Loss: 0.04641249403357506, Validation Loss: 0.0867670550942421\n",
            "Epoch 1725/2500, Training Loss: 0.04638409987092018, Validation Loss: 0.0867612361907959\n",
            "Epoch 1726/2500, Training Loss: 0.04635579138994217, Validation Loss: 0.08675288408994675\n",
            "Epoch 1727/2500, Training Loss: 0.046327799558639526, Validation Loss: 0.08674410730600357\n",
            "Epoch 1728/2500, Training Loss: 0.04629979655146599, Validation Loss: 0.08674110472202301\n",
            "Epoch 1729/2500, Training Loss: 0.04627186432480812, Validation Loss: 0.08673495799303055\n",
            "Epoch 1730/2500, Training Loss: 0.046243395656347275, Validation Loss: 0.08673056960105896\n",
            "Epoch 1731/2500, Training Loss: 0.04621497541666031, Validation Loss: 0.08672192692756653\n",
            "Epoch 1732/2500, Training Loss: 0.046187300235033035, Validation Loss: 0.08671736717224121\n",
            "Epoch 1733/2500, Training Loss: 0.046159740537405014, Validation Loss: 0.08671753853559494\n",
            "Epoch 1734/2500, Training Loss: 0.04613085091114044, Validation Loss: 0.08672352880239487\n",
            "Epoch 1735/2500, Training Loss: 0.046103786677122116, Validation Loss: 0.08671432733535767\n",
            "Epoch 1736/2500, Training Loss: 0.0460754856467247, Validation Loss: 0.08669137954711914\n",
            "Epoch 1737/2500, Training Loss: 0.046047698706388474, Validation Loss: 0.08667957782745361\n",
            "Epoch 1738/2500, Training Loss: 0.04601985588669777, Validation Loss: 0.08667632192373276\n",
            "Epoch 1739/2500, Training Loss: 0.04599180817604065, Validation Loss: 0.08668043464422226\n",
            "Epoch 1740/2500, Training Loss: 0.04596395045518875, Validation Loss: 0.0866827443242073\n",
            "Epoch 1741/2500, Training Loss: 0.045935966074466705, Validation Loss: 0.0866817757487297\n",
            "Epoch 1742/2500, Training Loss: 0.045908913016319275, Validation Loss: 0.08667297661304474\n",
            "Epoch 1743/2500, Training Loss: 0.045881304889917374, Validation Loss: 0.08665492385625839\n",
            "Epoch 1744/2500, Training Loss: 0.04585281386971474, Validation Loss: 0.08663211762905121\n",
            "Epoch 1745/2500, Training Loss: 0.04582531750202179, Validation Loss: 0.08661980926990509\n",
            "Epoch 1746/2500, Training Loss: 0.04579843953251839, Validation Loss: 0.08661411702632904\n",
            "Epoch 1747/2500, Training Loss: 0.04577112942934036, Validation Loss: 0.08661449700593948\n",
            "Epoch 1748/2500, Training Loss: 0.04574331268668175, Validation Loss: 0.0866193026304245\n",
            "Epoch 1749/2500, Training Loss: 0.04571564868092537, Validation Loss: 0.08662271499633789\n",
            "Epoch 1750/2500, Training Loss: 0.045687977224588394, Validation Loss: 0.08662586659193039\n",
            "Epoch 1751/2500, Training Loss: 0.045661501586437225, Validation Loss: 0.08661458641290665\n",
            "Epoch 1752/2500, Training Loss: 0.04563363641500473, Validation Loss: 0.08659348636865616\n",
            "Epoch 1753/2500, Training Loss: 0.04560638219118118, Validation Loss: 0.08657218515872955\n",
            "Epoch 1754/2500, Training Loss: 0.04557964950799942, Validation Loss: 0.086554154753685\n",
            "Epoch 1755/2500, Training Loss: 0.045553017407655716, Validation Loss: 0.08655230700969696\n",
            "Epoch 1756/2500, Training Loss: 0.045525699853897095, Validation Loss: 0.08655723929405212\n",
            "Epoch 1757/2500, Training Loss: 0.04549817368388176, Validation Loss: 0.08656622469425201\n",
            "Epoch 1758/2500, Training Loss: 0.04547061398625374, Validation Loss: 0.08656957000494003\n",
            "Epoch 1759/2500, Training Loss: 0.04544311389327049, Validation Loss: 0.08656716346740723\n",
            "Epoch 1760/2500, Training Loss: 0.045415911823511124, Validation Loss: 0.08656024187803268\n",
            "Epoch 1761/2500, Training Loss: 0.045389097183942795, Validation Loss: 0.08654820173978806\n",
            "Epoch 1762/2500, Training Loss: 0.0453614667057991, Validation Loss: 0.08653528988361359\n",
            "Epoch 1763/2500, Training Loss: 0.04533414915204048, Validation Loss: 0.08652406185865402\n",
            "Epoch 1764/2500, Training Loss: 0.045307133346796036, Validation Loss: 0.08651738613843918\n",
            "Epoch 1765/2500, Training Loss: 0.04528003931045532, Validation Loss: 0.0865163579583168\n",
            "Epoch 1766/2500, Training Loss: 0.04525264352560043, Validation Loss: 0.08652150630950928\n",
            "Epoch 1767/2500, Training Loss: 0.04522550478577614, Validation Loss: 0.08652202039957047\n",
            "Epoch 1768/2500, Training Loss: 0.04519805312156677, Validation Loss: 0.08651774376630783\n",
            "Epoch 1769/2500, Training Loss: 0.04517121613025665, Validation Loss: 0.08651319891214371\n",
            "Epoch 1770/2500, Training Loss: 0.04514363035559654, Validation Loss: 0.08650632202625275\n",
            "Epoch 1771/2500, Training Loss: 0.04511656612157822, Validation Loss: 0.08649247139692307\n",
            "Epoch 1772/2500, Training Loss: 0.045089513063430786, Validation Loss: 0.08647941797971725\n",
            "Epoch 1773/2500, Training Loss: 0.04506276175379753, Validation Loss: 0.08647129684686661\n",
            "Epoch 1774/2500, Training Loss: 0.04503514990210533, Validation Loss: 0.08646208047866821\n",
            "Epoch 1775/2500, Training Loss: 0.04500808194279671, Validation Loss: 0.0864558294415474\n",
            "Epoch 1776/2500, Training Loss: 0.04498112574219704, Validation Loss: 0.08645979315042496\n",
            "Epoch 1777/2500, Training Loss: 0.044954221695661545, Validation Loss: 0.08645690977573395\n",
            "Epoch 1778/2500, Training Loss: 0.04492691159248352, Validation Loss: 0.08644945174455643\n",
            "Epoch 1779/2500, Training Loss: 0.044900644570589066, Validation Loss: 0.08644533157348633\n",
            "Epoch 1780/2500, Training Loss: 0.0448734387755394, Validation Loss: 0.08644227683544159\n",
            "Epoch 1781/2500, Training Loss: 0.0448463149368763, Validation Loss: 0.08643965423107147\n",
            "Epoch 1782/2500, Training Loss: 0.0448194295167923, Validation Loss: 0.0864296555519104\n",
            "Epoch 1783/2500, Training Loss: 0.044792789965867996, Validation Loss: 0.0864163413643837\n",
            "Epoch 1784/2500, Training Loss: 0.04476546868681908, Validation Loss: 0.08640415221452713\n",
            "Epoch 1785/2500, Training Loss: 0.044738516211509705, Validation Loss: 0.08640044927597046\n",
            "Epoch 1786/2500, Training Loss: 0.044711899012327194, Validation Loss: 0.0864032506942749\n",
            "Epoch 1787/2500, Training Loss: 0.04468455910682678, Validation Loss: 0.08641081303358078\n",
            "Epoch 1788/2500, Training Loss: 0.04465857893228531, Validation Loss: 0.08642226457595825\n",
            "Epoch 1789/2500, Training Loss: 0.04463154450058937, Validation Loss: 0.08642452955245972\n",
            "Epoch 1790/2500, Training Loss: 0.04460495710372925, Validation Loss: 0.08641733229160309\n",
            "Epoch 1791/2500, Training Loss: 0.04457802698016167, Validation Loss: 0.08640330284833908\n",
            "Epoch 1792/2500, Training Loss: 0.04455149918794632, Validation Loss: 0.08639035373926163\n",
            "Epoch 1793/2500, Training Loss: 0.04452482983469963, Validation Loss: 0.08638522773981094\n",
            "Epoch 1794/2500, Training Loss: 0.044497717171907425, Validation Loss: 0.08639121800661087\n",
            "Epoch 1795/2500, Training Loss: 0.04447096586227417, Validation Loss: 0.08640002459287643\n",
            "Epoch 1796/2500, Training Loss: 0.04444420337677002, Validation Loss: 0.08640255033969879\n",
            "Epoch 1797/2500, Training Loss: 0.04441727325320244, Validation Loss: 0.08639676123857498\n",
            "Epoch 1798/2500, Training Loss: 0.044390544295310974, Validation Loss: 0.08639176934957504\n",
            "Epoch 1799/2500, Training Loss: 0.04436330497264862, Validation Loss: 0.0863865464925766\n",
            "Epoch 1800/2500, Training Loss: 0.04433684051036835, Validation Loss: 0.08638276159763336\n",
            "Epoch 1801/2500, Training Loss: 0.0443103164434433, Validation Loss: 0.08638203144073486\n",
            "Epoch 1802/2500, Training Loss: 0.04428359866142273, Validation Loss: 0.08637887984514236\n",
            "Epoch 1803/2500, Training Loss: 0.0442563034594059, Validation Loss: 0.08637659251689911\n",
            "Epoch 1804/2500, Training Loss: 0.044229328632354736, Validation Loss: 0.0863783061504364\n",
            "Epoch 1805/2500, Training Loss: 0.04420309141278267, Validation Loss: 0.08637510985136032\n",
            "Epoch 1806/2500, Training Loss: 0.044175922870635986, Validation Loss: 0.08637300133705139\n",
            "Epoch 1807/2500, Training Loss: 0.044149000197649, Validation Loss: 0.08637434989213943\n",
            "Epoch 1808/2500, Training Loss: 0.04412318393588066, Validation Loss: 0.08638057112693787\n",
            "Epoch 1809/2500, Training Loss: 0.044096216559410095, Validation Loss: 0.08638390153646469\n",
            "Epoch 1810/2500, Training Loss: 0.04406988248229027, Validation Loss: 0.08638229221105576\n",
            "Epoch 1811/2500, Training Loss: 0.0440433993935585, Validation Loss: 0.08638141304254532\n",
            "Epoch 1812/2500, Training Loss: 0.044016528874635696, Validation Loss: 0.08637821674346924\n",
            "Epoch 1813/2500, Training Loss: 0.04399058595299721, Validation Loss: 0.08637256175279617\n",
            "Epoch 1814/2500, Training Loss: 0.043964844197034836, Validation Loss: 0.08636844158172607\n",
            "Epoch 1815/2500, Training Loss: 0.04393880069255829, Validation Loss: 0.08635930716991425\n",
            "Epoch 1816/2500, Training Loss: 0.043912146240472794, Validation Loss: 0.08635471016168594\n",
            "Epoch 1817/2500, Training Loss: 0.04388625919818878, Validation Loss: 0.08636011183261871\n",
            "Epoch 1818/2500, Training Loss: 0.043860722333192825, Validation Loss: 0.08636627346277237\n",
            "Epoch 1819/2500, Training Loss: 0.04383524879813194, Validation Loss: 0.0863676443696022\n",
            "Epoch 1820/2500, Training Loss: 0.043808892369270325, Validation Loss: 0.08636128157377243\n",
            "Epoch 1821/2500, Training Loss: 0.043782297521829605, Validation Loss: 0.0863535925745964\n",
            "Epoch 1822/2500, Training Loss: 0.04375631362199783, Validation Loss: 0.08635036647319794\n",
            "Epoch 1823/2500, Training Loss: 0.043730251491069794, Validation Loss: 0.08635260164737701\n",
            "Epoch 1824/2500, Training Loss: 0.04370430111885071, Validation Loss: 0.08635730296373367\n",
            "Epoch 1825/2500, Training Loss: 0.04367821291089058, Validation Loss: 0.08635913580656052\n",
            "Epoch 1826/2500, Training Loss: 0.0436527356505394, Validation Loss: 0.08635488897562027\n",
            "Epoch 1827/2500, Training Loss: 0.04362686350941658, Validation Loss: 0.08634687215089798\n",
            "Epoch 1828/2500, Training Loss: 0.04360109940171242, Validation Loss: 0.08634352684020996\n",
            "Epoch 1829/2500, Training Loss: 0.0435752235352993, Validation Loss: 0.0863480195403099\n",
            "Epoch 1830/2500, Training Loss: 0.043549008667469025, Validation Loss: 0.08635838329792023\n",
            "Epoch 1831/2500, Training Loss: 0.043523289263248444, Validation Loss: 0.08636239171028137\n",
            "Epoch 1832/2500, Training Loss: 0.04349737986922264, Validation Loss: 0.08635536581277847\n",
            "Epoch 1833/2500, Training Loss: 0.04347271844744682, Validation Loss: 0.0863519161939621\n",
            "Epoch 1834/2500, Training Loss: 0.04344669356942177, Validation Loss: 0.08635144680738449\n",
            "Epoch 1835/2500, Training Loss: 0.043420907109975815, Validation Loss: 0.08635370433330536\n",
            "Epoch 1836/2500, Training Loss: 0.04339629411697388, Validation Loss: 0.08635638654232025\n",
            "Epoch 1837/2500, Training Loss: 0.043370988219976425, Validation Loss: 0.08635321259498596\n",
            "Epoch 1838/2500, Training Loss: 0.043344896286726, Validation Loss: 0.08634732663631439\n",
            "Epoch 1839/2500, Training Loss: 0.04331972822546959, Validation Loss: 0.08634564280509949\n",
            "Epoch 1840/2500, Training Loss: 0.04329453036189079, Validation Loss: 0.08634421229362488\n",
            "Epoch 1841/2500, Training Loss: 0.043268896639347076, Validation Loss: 0.08634623140096664\n",
            "Epoch 1842/2500, Training Loss: 0.043243326246738434, Validation Loss: 0.08634871989488602\n",
            "Epoch 1843/2500, Training Loss: 0.043217916041612625, Validation Loss: 0.08635365217924118\n",
            "Epoch 1844/2500, Training Loss: 0.04319201037287712, Validation Loss: 0.08635759353637695\n",
            "Epoch 1845/2500, Training Loss: 0.04316655918955803, Validation Loss: 0.08635932207107544\n",
            "Epoch 1846/2500, Training Loss: 0.04314079508185387, Validation Loss: 0.08635564893484116\n",
            "Epoch 1847/2500, Training Loss: 0.04311520978808403, Validation Loss: 0.0863458588719368\n",
            "Epoch 1848/2500, Training Loss: 0.043089475482702255, Validation Loss: 0.08633366972208023\n",
            "Epoch 1849/2500, Training Loss: 0.04306456446647644, Validation Loss: 0.08632613718509674\n",
            "Epoch 1850/2500, Training Loss: 0.04303908348083496, Validation Loss: 0.08632424473762512\n",
            "Epoch 1851/2500, Training Loss: 0.04301473870873451, Validation Loss: 0.08632025867700577\n",
            "Epoch 1852/2500, Training Loss: 0.04298927262425423, Validation Loss: 0.08630944043397903\n",
            "Epoch 1853/2500, Training Loss: 0.04296344146132469, Validation Loss: 0.08630945533514023\n",
            "Epoch 1854/2500, Training Loss: 0.04293860122561455, Validation Loss: 0.0863199308514595\n",
            "Epoch 1855/2500, Training Loss: 0.042913179844617844, Validation Loss: 0.086330845952034\n",
            "Epoch 1856/2500, Training Loss: 0.042887307703495026, Validation Loss: 0.08633316308259964\n",
            "Epoch 1857/2500, Training Loss: 0.04286225512623787, Validation Loss: 0.08632541447877884\n",
            "Epoch 1858/2500, Training Loss: 0.04283700883388519, Validation Loss: 0.08632059395313263\n",
            "Epoch 1859/2500, Training Loss: 0.04281186684966087, Validation Loss: 0.08632341027259827\n",
            "Epoch 1860/2500, Training Loss: 0.04278707131743431, Validation Loss: 0.08633176982402802\n",
            "Epoch 1861/2500, Training Loss: 0.042761895805597305, Validation Loss: 0.08633143454790115\n",
            "Epoch 1862/2500, Training Loss: 0.0427364706993103, Validation Loss: 0.08632475882768631\n",
            "Epoch 1863/2500, Training Loss: 0.042710740119218826, Validation Loss: 0.08631552755832672\n",
            "Epoch 1864/2500, Training Loss: 0.04268587753176689, Validation Loss: 0.08631021529436111\n",
            "Epoch 1865/2500, Training Loss: 0.04266123101115227, Validation Loss: 0.08630382269620895\n",
            "Epoch 1866/2500, Training Loss: 0.04263598099350929, Validation Loss: 0.0862937718629837\n",
            "Epoch 1867/2500, Training Loss: 0.04261046648025513, Validation Loss: 0.08628249168395996\n",
            "Epoch 1868/2500, Training Loss: 0.042584434151649475, Validation Loss: 0.08626453578472137\n",
            "Epoch 1869/2500, Training Loss: 0.042559873312711716, Validation Loss: 0.08624601364135742\n",
            "Epoch 1870/2500, Training Loss: 0.04253428429365158, Validation Loss: 0.08622129261493683\n",
            "Epoch 1871/2500, Training Loss: 0.042507704347372055, Validation Loss: 0.08620661497116089\n",
            "Epoch 1872/2500, Training Loss: 0.042482033371925354, Validation Loss: 0.08619313687086105\n",
            "Epoch 1873/2500, Training Loss: 0.0424567386507988, Validation Loss: 0.08616697788238525\n",
            "Epoch 1874/2500, Training Loss: 0.04243068769574165, Validation Loss: 0.08612998574972153\n",
            "Epoch 1875/2500, Training Loss: 0.04240451380610466, Validation Loss: 0.08610010147094727\n",
            "Epoch 1876/2500, Training Loss: 0.042378559708595276, Validation Loss: 0.08608201146125793\n",
            "Epoch 1877/2500, Training Loss: 0.042353928089141846, Validation Loss: 0.08607837557792664\n",
            "Epoch 1878/2500, Training Loss: 0.042328931391239166, Validation Loss: 0.08608415722846985\n",
            "Epoch 1879/2500, Training Loss: 0.042303234338760376, Validation Loss: 0.08609027415513992\n",
            "Epoch 1880/2500, Training Loss: 0.04227805882692337, Validation Loss: 0.08609907329082489\n",
            "Epoch 1881/2500, Training Loss: 0.042253028601408005, Validation Loss: 0.08610188215970993\n",
            "Epoch 1882/2500, Training Loss: 0.042227305471897125, Validation Loss: 0.0860941931605339\n",
            "Epoch 1883/2500, Training Loss: 0.042201634496450424, Validation Loss: 0.08608832210302353\n",
            "Epoch 1884/2500, Training Loss: 0.04217630252242088, Validation Loss: 0.08609259128570557\n",
            "Epoch 1885/2500, Training Loss: 0.04215081408619881, Validation Loss: 0.0861138179898262\n",
            "Epoch 1886/2500, Training Loss: 0.04212556779384613, Validation Loss: 0.0861351415514946\n",
            "Epoch 1887/2500, Training Loss: 0.04209999740123749, Validation Loss: 0.08615535497665405\n",
            "Epoch 1888/2500, Training Loss: 0.04207407683134079, Validation Loss: 0.08616576343774796\n",
            "Epoch 1889/2500, Training Loss: 0.04204919934272766, Validation Loss: 0.08616932481527328\n",
            "Epoch 1890/2500, Training Loss: 0.042024608701467514, Validation Loss: 0.08617430925369263\n",
            "Epoch 1891/2500, Training Loss: 0.04199906811118126, Validation Loss: 0.08619079738855362\n",
            "Epoch 1892/2500, Training Loss: 0.04197252541780472, Validation Loss: 0.08621077984571457\n",
            "Epoch 1893/2500, Training Loss: 0.04194782301783562, Validation Loss: 0.08622384071350098\n",
            "Epoch 1894/2500, Training Loss: 0.041923023760318756, Validation Loss: 0.0862293541431427\n",
            "Epoch 1895/2500, Training Loss: 0.0418974868953228, Validation Loss: 0.08622650802135468\n",
            "Epoch 1896/2500, Training Loss: 0.04187152534723282, Validation Loss: 0.08621494472026825\n",
            "Epoch 1897/2500, Training Loss: 0.04184620454907417, Validation Loss: 0.08620690554380417\n",
            "Epoch 1898/2500, Training Loss: 0.041821032762527466, Validation Loss: 0.08620648831129074\n",
            "Epoch 1899/2500, Training Loss: 0.04179561883211136, Validation Loss: 0.0862097293138504\n",
            "Epoch 1900/2500, Training Loss: 0.041770320385694504, Validation Loss: 0.0862012505531311\n",
            "Epoch 1901/2500, Training Loss: 0.04174508899450302, Validation Loss: 0.08618764579296112\n",
            "Epoch 1902/2500, Training Loss: 0.04171986132860184, Validation Loss: 0.08617407828569412\n",
            "Epoch 1903/2500, Training Loss: 0.04169441759586334, Validation Loss: 0.0861668512225151\n",
            "Epoch 1904/2500, Training Loss: 0.0416693314909935, Validation Loss: 0.08616843819618225\n",
            "Epoch 1905/2500, Training Loss: 0.04164421185851097, Validation Loss: 0.08616438508033752\n",
            "Epoch 1906/2500, Training Loss: 0.04161987081170082, Validation Loss: 0.08615180850028992\n",
            "Epoch 1907/2500, Training Loss: 0.04159492999315262, Validation Loss: 0.0861409604549408\n",
            "Epoch 1908/2500, Training Loss: 0.041570380330085754, Validation Loss: 0.08613791316747665\n",
            "Epoch 1909/2500, Training Loss: 0.04154534637928009, Validation Loss: 0.08613970875740051\n",
            "Epoch 1910/2500, Training Loss: 0.041520558297634125, Validation Loss: 0.08613181114196777\n",
            "Epoch 1911/2500, Training Loss: 0.04149698466062546, Validation Loss: 0.08611338585615158\n",
            "Epoch 1912/2500, Training Loss: 0.041471924632787704, Validation Loss: 0.08608914166688919\n",
            "Epoch 1913/2500, Training Loss: 0.04144670069217682, Validation Loss: 0.086075060069561\n",
            "Epoch 1914/2500, Training Loss: 0.04142250865697861, Validation Loss: 0.08607500046491623\n",
            "Epoch 1915/2500, Training Loss: 0.04139741510152817, Validation Loss: 0.08608163893222809\n",
            "Epoch 1916/2500, Training Loss: 0.04137369990348816, Validation Loss: 0.08608416467905045\n",
            "Epoch 1917/2500, Training Loss: 0.041349343955516815, Validation Loss: 0.08607646822929382\n",
            "Epoch 1918/2500, Training Loss: 0.041324298828840256, Validation Loss: 0.08606605976819992\n",
            "Epoch 1919/2500, Training Loss: 0.041298843920230865, Validation Loss: 0.08606082946062088\n",
            "Epoch 1920/2500, Training Loss: 0.04127514734864235, Validation Loss: 0.08606274425983429\n",
            "Epoch 1921/2500, Training Loss: 0.041250817477703094, Validation Loss: 0.08606179058551788\n",
            "Epoch 1922/2500, Training Loss: 0.041225530207157135, Validation Loss: 0.08604773879051208\n",
            "Epoch 1923/2500, Training Loss: 0.041200924664735794, Validation Loss: 0.086029551923275\n",
            "Epoch 1924/2500, Training Loss: 0.0411769263446331, Validation Loss: 0.08601944893598557\n",
            "Epoch 1925/2500, Training Loss: 0.04115281626582146, Validation Loss: 0.08602256327867508\n",
            "Epoch 1926/2500, Training Loss: 0.041127316653728485, Validation Loss: 0.0860363245010376\n",
            "Epoch 1927/2500, Training Loss: 0.041103050112724304, Validation Loss: 0.08604862540960312\n",
            "Epoch 1928/2500, Training Loss: 0.04107879474759102, Validation Loss: 0.08604942262172699\n",
            "Epoch 1929/2500, Training Loss: 0.04105408862233162, Validation Loss: 0.08603369444608688\n",
            "Epoch 1930/2500, Training Loss: 0.04102929309010506, Validation Loss: 0.0860178992152214\n",
            "Epoch 1931/2500, Training Loss: 0.04100571945309639, Validation Loss: 0.08601795136928558\n",
            "Epoch 1932/2500, Training Loss: 0.04098205268383026, Validation Loss: 0.08602536469697952\n",
            "Epoch 1933/2500, Training Loss: 0.04095825180411339, Validation Loss: 0.08602456748485565\n",
            "Epoch 1934/2500, Training Loss: 0.04093456640839577, Validation Loss: 0.08601394295692444\n",
            "Epoch 1935/2500, Training Loss: 0.040910907089710236, Validation Loss: 0.08601147681474686\n",
            "Epoch 1936/2500, Training Loss: 0.04088748246431351, Validation Loss: 0.0860101655125618\n",
            "Epoch 1937/2500, Training Loss: 0.04086346924304962, Validation Loss: 0.08600888401269913\n",
            "Epoch 1938/2500, Training Loss: 0.04083983227610588, Validation Loss: 0.08600182831287384\n",
            "Epoch 1939/2500, Training Loss: 0.04081645980477333, Validation Loss: 0.0860014334321022\n",
            "Epoch 1940/2500, Training Loss: 0.04079212248325348, Validation Loss: 0.08600611239671707\n",
            "Epoch 1941/2500, Training Loss: 0.040769971907138824, Validation Loss: 0.08599728345870972\n",
            "Epoch 1942/2500, Training Loss: 0.04074597358703613, Validation Loss: 0.08598322421312332\n",
            "Epoch 1943/2500, Training Loss: 0.04072190821170807, Validation Loss: 0.08597300201654434\n",
            "Epoch 1944/2500, Training Loss: 0.04069922864437103, Validation Loss: 0.08597447723150253\n",
            "Epoch 1945/2500, Training Loss: 0.04067591577768326, Validation Loss: 0.08597321063280106\n",
            "Epoch 1946/2500, Training Loss: 0.04065275564789772, Validation Loss: 0.0859617069363594\n",
            "Epoch 1947/2500, Training Loss: 0.040628641843795776, Validation Loss: 0.0859508290886879\n",
            "Epoch 1948/2500, Training Loss: 0.040604956448078156, Validation Loss: 0.08595407754182816\n",
            "Epoch 1949/2500, Training Loss: 0.040581174194812775, Validation Loss: 0.08595810830593109\n",
            "Epoch 1950/2500, Training Loss: 0.040558140724897385, Validation Loss: 0.08595528453588486\n",
            "Epoch 1951/2500, Training Loss: 0.04053442180156708, Validation Loss: 0.08595580607652664\n",
            "Epoch 1952/2500, Training Loss: 0.04051126539707184, Validation Loss: 0.08595023304224014\n",
            "Epoch 1953/2500, Training Loss: 0.04048800468444824, Validation Loss: 0.08594224601984024\n",
            "Epoch 1954/2500, Training Loss: 0.040464576333761215, Validation Loss: 0.08593650162220001\n",
            "Epoch 1955/2500, Training Loss: 0.040441639721393585, Validation Loss: 0.0859309658408165\n",
            "Epoch 1956/2500, Training Loss: 0.040418580174446106, Validation Loss: 0.08593142032623291\n",
            "Epoch 1957/2500, Training Loss: 0.04039515554904938, Validation Loss: 0.08592864871025085\n",
            "Epoch 1958/2500, Training Loss: 0.04037175700068474, Validation Loss: 0.08592326194047928\n",
            "Epoch 1959/2500, Training Loss: 0.04034867882728577, Validation Loss: 0.0859234482049942\n",
            "Epoch 1960/2500, Training Loss: 0.040325675159692764, Validation Loss: 0.08592288941144943\n",
            "Epoch 1961/2500, Training Loss: 0.04030303284525871, Validation Loss: 0.08591470122337341\n",
            "Epoch 1962/2500, Training Loss: 0.04028007760643959, Validation Loss: 0.08590436726808548\n",
            "Epoch 1963/2500, Training Loss: 0.040256768465042114, Validation Loss: 0.0858897864818573\n",
            "Epoch 1964/2500, Training Loss: 0.040234316140413284, Validation Loss: 0.08587778359651566\n",
            "Epoch 1965/2500, Training Loss: 0.04021201282739639, Validation Loss: 0.08586586266756058\n",
            "Epoch 1966/2500, Training Loss: 0.040189143270254135, Validation Loss: 0.08585952967405319\n",
            "Epoch 1967/2500, Training Loss: 0.040166061371564865, Validation Loss: 0.08585979044437408\n",
            "Epoch 1968/2500, Training Loss: 0.04014243930578232, Validation Loss: 0.08585859835147858\n",
            "Epoch 1969/2500, Training Loss: 0.04011920839548111, Validation Loss: 0.08585216104984283\n",
            "Epoch 1970/2500, Training Loss: 0.040097612887620926, Validation Loss: 0.08583368360996246\n",
            "Epoch 1971/2500, Training Loss: 0.04007367417216301, Validation Loss: 0.08580788969993591\n",
            "Epoch 1972/2500, Training Loss: 0.04005047306418419, Validation Loss: 0.08579377830028534\n",
            "Epoch 1973/2500, Training Loss: 0.040028516203165054, Validation Loss: 0.08578288555145264\n",
            "Epoch 1974/2500, Training Loss: 0.040006525814533234, Validation Loss: 0.0857747346162796\n",
            "Epoch 1975/2500, Training Loss: 0.0399833619594574, Validation Loss: 0.0857706293463707\n",
            "Epoch 1976/2500, Training Loss: 0.03996003791689873, Validation Loss: 0.08577024191617966\n",
            "Epoch 1977/2500, Training Loss: 0.039936356246471405, Validation Loss: 0.08577268570661545\n",
            "Epoch 1978/2500, Training Loss: 0.03991439566016197, Validation Loss: 0.08577200770378113\n",
            "Epoch 1979/2500, Training Loss: 0.0398918054997921, Validation Loss: 0.08576084673404694\n",
            "Epoch 1980/2500, Training Loss: 0.03986842930316925, Validation Loss: 0.08575398474931717\n",
            "Epoch 1981/2500, Training Loss: 0.03984513506293297, Validation Loss: 0.08575990796089172\n",
            "Epoch 1982/2500, Training Loss: 0.039822883903980255, Validation Loss: 0.08576253056526184\n",
            "Epoch 1983/2500, Training Loss: 0.03979996219277382, Validation Loss: 0.08575867861509323\n",
            "Epoch 1984/2500, Training Loss: 0.03977717459201813, Validation Loss: 0.08575090020895004\n",
            "Epoch 1985/2500, Training Loss: 0.03975478932261467, Validation Loss: 0.08574175089597702\n",
            "Epoch 1986/2500, Training Loss: 0.039733126759529114, Validation Loss: 0.08573698997497559\n",
            "Epoch 1987/2500, Training Loss: 0.03971065953373909, Validation Loss: 0.08574063330888748\n",
            "Epoch 1988/2500, Training Loss: 0.03968770429491997, Validation Loss: 0.08573991805315018\n",
            "Epoch 1989/2500, Training Loss: 0.039666078984737396, Validation Loss: 0.08573334664106369\n",
            "Epoch 1990/2500, Training Loss: 0.03964436426758766, Validation Loss: 0.08572724461555481\n",
            "Epoch 1991/2500, Training Loss: 0.039621882140636444, Validation Loss: 0.0857255682349205\n",
            "Epoch 1992/2500, Training Loss: 0.039599865674972534, Validation Loss: 0.08572068065404892\n",
            "Epoch 1993/2500, Training Loss: 0.0395781472325325, Validation Loss: 0.0857091099023819\n",
            "Epoch 1994/2500, Training Loss: 0.039555493742227554, Validation Loss: 0.08571824431419373\n",
            "Epoch 1995/2500, Training Loss: 0.03953324258327484, Validation Loss: 0.08574069291353226\n",
            "Epoch 1996/2500, Training Loss: 0.039512429386377335, Validation Loss: 0.08574628829956055\n",
            "Epoch 1997/2500, Training Loss: 0.03949031978845596, Validation Loss: 0.08573953807353973\n",
            "Epoch 1998/2500, Training Loss: 0.03946753218770027, Validation Loss: 0.08572255820035934\n",
            "Epoch 1999/2500, Training Loss: 0.039445798844099045, Validation Loss: 0.08570467680692673\n",
            "Epoch 2000/2500, Training Loss: 0.03942451253533363, Validation Loss: 0.08569742739200592\n",
            "Epoch 2001/2500, Training Loss: 0.03940308094024658, Validation Loss: 0.08568917214870453\n",
            "Epoch 2002/2500, Training Loss: 0.039380814880132675, Validation Loss: 0.08568335324525833\n",
            "Epoch 2003/2500, Training Loss: 0.03935910761356354, Validation Loss: 0.08568012714385986\n",
            "Epoch 2004/2500, Training Loss: 0.03933761641383171, Validation Loss: 0.08567966520786285\n",
            "Epoch 2005/2500, Training Loss: 0.03931562975049019, Validation Loss: 0.08568646013736725\n",
            "Epoch 2006/2500, Training Loss: 0.03929342329502106, Validation Loss: 0.08570146560668945\n",
            "Epoch 2007/2500, Training Loss: 0.039272744208574295, Validation Loss: 0.0856984332203865\n",
            "Epoch 2008/2500, Training Loss: 0.03925073519349098, Validation Loss: 0.08568128943443298\n",
            "Epoch 2009/2500, Training Loss: 0.03922870382666588, Validation Loss: 0.08566396683454514\n",
            "Epoch 2010/2500, Training Loss: 0.03920760378241539, Validation Loss: 0.08564917743206024\n",
            "Epoch 2011/2500, Training Loss: 0.03918604925274849, Validation Loss: 0.08564119040966034\n",
            "Epoch 2012/2500, Training Loss: 0.039164092391729355, Validation Loss: 0.08565142005681992\n",
            "Epoch 2013/2500, Training Loss: 0.03914257884025574, Validation Loss: 0.0856502503156662\n",
            "Epoch 2014/2500, Training Loss: 0.039121728390455246, Validation Loss: 0.08564566820859909\n",
            "Epoch 2015/2500, Training Loss: 0.03910037502646446, Validation Loss: 0.08564097434282303\n",
            "Epoch 2016/2500, Training Loss: 0.039078596979379654, Validation Loss: 0.08563954383134842\n",
            "Epoch 2017/2500, Training Loss: 0.039057474583387375, Validation Loss: 0.08563783764839172\n",
            "Epoch 2018/2500, Training Loss: 0.03903665766119957, Validation Loss: 0.08563841879367828\n",
            "Epoch 2019/2500, Training Loss: 0.039015304297208786, Validation Loss: 0.08564182370901108\n",
            "Epoch 2020/2500, Training Loss: 0.03899382799863815, Validation Loss: 0.0856437012553215\n",
            "Epoch 2021/2500, Training Loss: 0.038972869515419006, Validation Loss: 0.08563967049121857\n",
            "Epoch 2022/2500, Training Loss: 0.03895170986652374, Validation Loss: 0.08562499284744263\n",
            "Epoch 2023/2500, Training Loss: 0.038930200040340424, Validation Loss: 0.08560849726200104\n",
            "Epoch 2024/2500, Training Loss: 0.038909267634153366, Validation Loss: 0.08559995144605637\n",
            "Epoch 2025/2500, Training Loss: 0.03888806328177452, Validation Loss: 0.08559977263212204\n",
            "Epoch 2026/2500, Training Loss: 0.03886758163571358, Validation Loss: 0.08559024333953857\n",
            "Epoch 2027/2500, Training Loss: 0.038847170770168304, Validation Loss: 0.08558158576488495\n",
            "Epoch 2028/2500, Training Loss: 0.0388263575732708, Validation Loss: 0.08557392656803131\n",
            "Epoch 2029/2500, Training Loss: 0.03880494832992554, Validation Loss: 0.08557406812906265\n",
            "Epoch 2030/2500, Training Loss: 0.03878406062722206, Validation Loss: 0.08557359874248505\n",
            "Epoch 2031/2500, Training Loss: 0.03876383975148201, Validation Loss: 0.08556798100471497\n",
            "Epoch 2032/2500, Training Loss: 0.0387427993118763, Validation Loss: 0.08556239306926727\n",
            "Epoch 2033/2500, Training Loss: 0.038721729069948196, Validation Loss: 0.0855645015835762\n",
            "Epoch 2034/2500, Training Loss: 0.038701072335243225, Validation Loss: 0.08556973934173584\n",
            "Epoch 2035/2500, Training Loss: 0.03868181258440018, Validation Loss: 0.08556322008371353\n",
            "Epoch 2036/2500, Training Loss: 0.03866176679730415, Validation Loss: 0.08555667847394943\n",
            "Epoch 2037/2500, Training Loss: 0.03863997757434845, Validation Loss: 0.08555768430233002\n",
            "Epoch 2038/2500, Training Loss: 0.038619693368673325, Validation Loss: 0.08556146174669266\n",
            "Epoch 2039/2500, Training Loss: 0.038599494844675064, Validation Loss: 0.08556292951107025\n",
            "Epoch 2040/2500, Training Loss: 0.03857822343707085, Validation Loss: 0.08554505556821823\n",
            "Epoch 2041/2500, Training Loss: 0.038557570427656174, Validation Loss: 0.0855264812707901\n",
            "Epoch 2042/2500, Training Loss: 0.03853672742843628, Validation Loss: 0.08552665263414383\n",
            "Epoch 2043/2500, Training Loss: 0.03851675987243652, Validation Loss: 0.08552942425012589\n",
            "Epoch 2044/2500, Training Loss: 0.03849572315812111, Validation Loss: 0.08552829921245575\n",
            "Epoch 2045/2500, Training Loss: 0.03847527131438255, Validation Loss: 0.08552311360836029\n",
            "Epoch 2046/2500, Training Loss: 0.03845524787902832, Validation Loss: 0.08552415668964386\n",
            "Epoch 2047/2500, Training Loss: 0.03843424841761589, Validation Loss: 0.08552977442741394\n",
            "Epoch 2048/2500, Training Loss: 0.03841404244303703, Validation Loss: 0.08553136140108109\n",
            "Epoch 2049/2500, Training Loss: 0.03839379549026489, Validation Loss: 0.08551925420761108\n",
            "Epoch 2050/2500, Training Loss: 0.03837371990084648, Validation Loss: 0.08551095426082611\n",
            "Epoch 2051/2500, Training Loss: 0.03835310786962509, Validation Loss: 0.08550403267145157\n",
            "Epoch 2052/2500, Training Loss: 0.03833290562033653, Validation Loss: 0.08549801260232925\n",
            "Epoch 2053/2500, Training Loss: 0.038312967866659164, Validation Loss: 0.08549963682889938\n",
            "Epoch 2054/2500, Training Loss: 0.038291893899440765, Validation Loss: 0.08549906313419342\n",
            "Epoch 2055/2500, Training Loss: 0.03827255591750145, Validation Loss: 0.0854906290769577\n",
            "Epoch 2056/2500, Training Loss: 0.03825168311595917, Validation Loss: 0.08548478782176971\n",
            "Epoch 2057/2500, Training Loss: 0.0382322333753109, Validation Loss: 0.08548948168754578\n",
            "Epoch 2058/2500, Training Loss: 0.03821226954460144, Validation Loss: 0.08550090342760086\n",
            "Epoch 2059/2500, Training Loss: 0.03819163143634796, Validation Loss: 0.08551328629255295\n",
            "Epoch 2060/2500, Training Loss: 0.03817187249660492, Validation Loss: 0.08550508320331573\n",
            "Epoch 2061/2500, Training Loss: 0.038151372224092484, Validation Loss: 0.085484579205513\n",
            "Epoch 2062/2500, Training Loss: 0.03813096880912781, Validation Loss: 0.08547195792198181\n",
            "Epoch 2063/2500, Training Loss: 0.038111280649900436, Validation Loss: 0.08547522127628326\n",
            "Epoch 2064/2500, Training Loss: 0.03809119388461113, Validation Loss: 0.08549409359693527\n",
            "Epoch 2065/2500, Training Loss: 0.03807034343481064, Validation Loss: 0.08550213277339935\n",
            "Epoch 2066/2500, Training Loss: 0.03805002570152283, Validation Loss: 0.0855022668838501\n",
            "Epoch 2067/2500, Training Loss: 0.038030099123716354, Validation Loss: 0.08548609912395477\n",
            "Epoch 2068/2500, Training Loss: 0.03801003843545914, Validation Loss: 0.08548031747341156\n",
            "Epoch 2069/2500, Training Loss: 0.03798987343907356, Validation Loss: 0.08549082279205322\n",
            "Epoch 2070/2500, Training Loss: 0.03796888515353203, Validation Loss: 0.08549710363149643\n",
            "Epoch 2071/2500, Training Loss: 0.03794897720217705, Validation Loss: 0.08548711240291595\n",
            "Epoch 2072/2500, Training Loss: 0.03792959079146385, Validation Loss: 0.08547375351190567\n",
            "Epoch 2073/2500, Training Loss: 0.03790978342294693, Validation Loss: 0.08547142148017883\n",
            "Epoch 2074/2500, Training Loss: 0.037888865917921066, Validation Loss: 0.08547524362802505\n",
            "Epoch 2075/2500, Training Loss: 0.037868231534957886, Validation Loss: 0.08547383546829224\n",
            "Epoch 2076/2500, Training Loss: 0.037849895656108856, Validation Loss: 0.08546571433544159\n",
            "Epoch 2077/2500, Training Loss: 0.03782901167869568, Validation Loss: 0.08546106517314911\n",
            "Epoch 2078/2500, Training Loss: 0.03780827298760414, Validation Loss: 0.08546791970729828\n",
            "Epoch 2079/2500, Training Loss: 0.037789102643728256, Validation Loss: 0.08546913415193558\n",
            "Epoch 2080/2500, Training Loss: 0.03776904195547104, Validation Loss: 0.0854758694767952\n",
            "Epoch 2081/2500, Training Loss: 0.03774891421198845, Validation Loss: 0.08547523617744446\n",
            "Epoch 2082/2500, Training Loss: 0.0377291701734066, Validation Loss: 0.08546970784664154\n",
            "Epoch 2083/2500, Training Loss: 0.037709157913923264, Validation Loss: 0.0854618176817894\n",
            "Epoch 2084/2500, Training Loss: 0.03768999129533768, Validation Loss: 0.08545887470245361\n",
            "Epoch 2085/2500, Training Loss: 0.03766981139779091, Validation Loss: 0.08546262979507446\n",
            "Epoch 2086/2500, Training Loss: 0.03764963150024414, Validation Loss: 0.08546213805675507\n",
            "Epoch 2087/2500, Training Loss: 0.03763046860694885, Validation Loss: 0.08545097708702087\n",
            "Epoch 2088/2500, Training Loss: 0.037611253559589386, Validation Loss: 0.0854346826672554\n",
            "Epoch 2089/2500, Training Loss: 0.037591397762298584, Validation Loss: 0.0854269489645958\n",
            "Epoch 2090/2500, Training Loss: 0.03757203370332718, Validation Loss: 0.08543360233306885\n",
            "Epoch 2091/2500, Training Loss: 0.03755228966474533, Validation Loss: 0.08544144034385681\n",
            "Epoch 2092/2500, Training Loss: 0.0375327505171299, Validation Loss: 0.08543466776609421\n",
            "Epoch 2093/2500, Training Loss: 0.037512343376874924, Validation Loss: 0.08542309701442719\n",
            "Epoch 2094/2500, Training Loss: 0.03749293088912964, Validation Loss: 0.08541374653577805\n",
            "Epoch 2095/2500, Training Loss: 0.03747283294796944, Validation Loss: 0.08541012555360794\n",
            "Epoch 2096/2500, Training Loss: 0.03745387867093086, Validation Loss: 0.08542612940073013\n",
            "Epoch 2097/2500, Training Loss: 0.037434086203575134, Validation Loss: 0.08544153720140457\n",
            "Epoch 2098/2500, Training Loss: 0.037414029240608215, Validation Loss: 0.08544371277093887\n",
            "Epoch 2099/2500, Training Loss: 0.037394214421510696, Validation Loss: 0.08543587476015091\n",
            "Epoch 2100/2500, Training Loss: 0.03737529367208481, Validation Loss: 0.08542289584875107\n",
            "Epoch 2101/2500, Training Loss: 0.03735430911183357, Validation Loss: 0.08541882783174515\n",
            "Epoch 2102/2500, Training Loss: 0.037335220724344254, Validation Loss: 0.0854201465845108\n",
            "Epoch 2103/2500, Training Loss: 0.037316299974918365, Validation Loss: 0.08541207760572433\n",
            "Epoch 2104/2500, Training Loss: 0.03729628771543503, Validation Loss: 0.08539815992116928\n",
            "Epoch 2105/2500, Training Loss: 0.03727661445736885, Validation Loss: 0.08538146317005157\n",
            "Epoch 2106/2500, Training Loss: 0.037257470190525055, Validation Loss: 0.0853806808590889\n",
            "Epoch 2107/2500, Training Loss: 0.03723764792084694, Validation Loss: 0.08539237827062607\n",
            "Epoch 2108/2500, Training Loss: 0.03721759840846062, Validation Loss: 0.08540993183851242\n",
            "Epoch 2109/2500, Training Loss: 0.03719820827245712, Validation Loss: 0.08542700856924057\n",
            "Epoch 2110/2500, Training Loss: 0.037178486585617065, Validation Loss: 0.0854327604174614\n",
            "Epoch 2111/2500, Training Loss: 0.03715921938419342, Validation Loss: 0.08541934937238693\n",
            "Epoch 2112/2500, Training Loss: 0.0371396578848362, Validation Loss: 0.08541075885295868\n",
            "Epoch 2113/2500, Training Loss: 0.037120621651411057, Validation Loss: 0.08541289716959\n",
            "Epoch 2114/2500, Training Loss: 0.03710126876831055, Validation Loss: 0.08541493862867355\n",
            "Epoch 2115/2500, Training Loss: 0.03708133101463318, Validation Loss: 0.08541389554738998\n",
            "Epoch 2116/2500, Training Loss: 0.03706163167953491, Validation Loss: 0.085407555103302\n",
            "Epoch 2117/2500, Training Loss: 0.03704230114817619, Validation Loss: 0.08538351953029633\n",
            "Epoch 2118/2500, Training Loss: 0.037022341042757034, Validation Loss: 0.08536813408136368\n",
            "Epoch 2119/2500, Training Loss: 0.03700326383113861, Validation Loss: 0.08537348359823227\n",
            "Epoch 2120/2500, Training Loss: 0.036983609199523926, Validation Loss: 0.08538626879453659\n",
            "Epoch 2121/2500, Training Loss: 0.036964334547519684, Validation Loss: 0.08539055287837982\n",
            "Epoch 2122/2500, Training Loss: 0.03694505617022514, Validation Loss: 0.08538442105054855\n",
            "Epoch 2123/2500, Training Loss: 0.0369255430996418, Validation Loss: 0.08536109328269958\n",
            "Epoch 2124/2500, Training Loss: 0.03690584748983383, Validation Loss: 0.08535690605640411\n",
            "Epoch 2125/2500, Training Loss: 0.03688608109951019, Validation Loss: 0.08536755293607712\n",
            "Epoch 2126/2500, Training Loss: 0.036867767572402954, Validation Loss: 0.08537985384464264\n",
            "Epoch 2127/2500, Training Loss: 0.03684788569808006, Validation Loss: 0.08538023382425308\n",
            "Epoch 2128/2500, Training Loss: 0.036828573793172836, Validation Loss: 0.08537688851356506\n",
            "Epoch 2129/2500, Training Loss: 0.036809757351875305, Validation Loss: 0.08537304401397705\n",
            "Epoch 2130/2500, Training Loss: 0.036789946258068085, Validation Loss: 0.0853755846619606\n",
            "Epoch 2131/2500, Training Loss: 0.03677019476890564, Validation Loss: 0.0853768065571785\n",
            "Epoch 2132/2500, Training Loss: 0.03675207123160362, Validation Loss: 0.08537093549966812\n",
            "Epoch 2133/2500, Training Loss: 0.03673219308257103, Validation Loss: 0.08537011593580246\n",
            "Epoch 2134/2500, Training Loss: 0.03671244531869888, Validation Loss: 0.08537258207798004\n",
            "Epoch 2135/2500, Training Loss: 0.03669413551688194, Validation Loss: 0.08537188917398453\n",
            "Epoch 2136/2500, Training Loss: 0.03667481243610382, Validation Loss: 0.08536846190690994\n",
            "Epoch 2137/2500, Training Loss: 0.0366547666490078, Validation Loss: 0.08537550270557404\n",
            "Epoch 2138/2500, Training Loss: 0.03663615137338638, Validation Loss: 0.08538208156824112\n",
            "Epoch 2139/2500, Training Loss: 0.03661728277802467, Validation Loss: 0.08538202941417694\n",
            "Epoch 2140/2500, Training Loss: 0.03659838065505028, Validation Loss: 0.08537427335977554\n",
            "Epoch 2141/2500, Training Loss: 0.036578498780727386, Validation Loss: 0.08536182343959808\n",
            "Epoch 2142/2500, Training Loss: 0.03655916824936867, Validation Loss: 0.08535730838775635\n",
            "Epoch 2143/2500, Training Loss: 0.03654034808278084, Validation Loss: 0.08536423742771149\n",
            "Epoch 2144/2500, Training Loss: 0.03652091324329376, Validation Loss: 0.08537605404853821\n",
            "Epoch 2145/2500, Training Loss: 0.03650130704045296, Validation Loss: 0.08539312332868576\n",
            "Epoch 2146/2500, Training Loss: 0.03648317605257034, Validation Loss: 0.08539353311061859\n",
            "Epoch 2147/2500, Training Loss: 0.036463625729084015, Validation Loss: 0.08537997305393219\n",
            "Epoch 2148/2500, Training Loss: 0.036443911492824554, Validation Loss: 0.08537086099386215\n",
            "Epoch 2149/2500, Training Loss: 0.03642480820417404, Validation Loss: 0.08536317199468613\n",
            "Epoch 2150/2500, Training Loss: 0.0364062674343586, Validation Loss: 0.08537052571773529\n",
            "Epoch 2151/2500, Training Loss: 0.03638779744505882, Validation Loss: 0.0853809341788292\n",
            "Epoch 2152/2500, Training Loss: 0.03636809438467026, Validation Loss: 0.08538559824228287\n",
            "Epoch 2153/2500, Training Loss: 0.036348607391119, Validation Loss: 0.08536979556083679\n",
            "Epoch 2154/2500, Training Loss: 0.036330144852399826, Validation Loss: 0.08536684513092041\n",
            "Epoch 2155/2500, Training Loss: 0.03631142899394035, Validation Loss: 0.08538060635328293\n",
            "Epoch 2156/2500, Training Loss: 0.03629199415445328, Validation Loss: 0.08539555966854095\n",
            "Epoch 2157/2500, Training Loss: 0.03627442941069603, Validation Loss: 0.08538977056741714\n",
            "Epoch 2158/2500, Training Loss: 0.03625519946217537, Validation Loss: 0.0853748545050621\n",
            "Epoch 2159/2500, Training Loss: 0.03623553365468979, Validation Loss: 0.08536074310541153\n",
            "Epoch 2160/2500, Training Loss: 0.036217205226421356, Validation Loss: 0.08535320311784744\n",
            "Epoch 2161/2500, Training Loss: 0.036199044436216354, Validation Loss: 0.08535390347242355\n",
            "Epoch 2162/2500, Training Loss: 0.036179400980472565, Validation Loss: 0.0853617712855339\n",
            "Epoch 2163/2500, Training Loss: 0.03616025298833847, Validation Loss: 0.08536816388368607\n",
            "Epoch 2164/2500, Training Loss: 0.036141976714134216, Validation Loss: 0.08536869287490845\n",
            "Epoch 2165/2500, Training Loss: 0.03612234443426132, Validation Loss: 0.08537685871124268\n",
            "Epoch 2166/2500, Training Loss: 0.03610341623425484, Validation Loss: 0.08538869023323059\n",
            "Epoch 2167/2500, Training Loss: 0.03608467057347298, Validation Loss: 0.08539745211601257\n",
            "Epoch 2168/2500, Training Loss: 0.03606561943888664, Validation Loss: 0.08539395779371262\n",
            "Epoch 2169/2500, Training Loss: 0.03604666143655777, Validation Loss: 0.08538497984409332\n",
            "Epoch 2170/2500, Training Loss: 0.03602875396609306, Validation Loss: 0.08538225293159485\n",
            "Epoch 2171/2500, Training Loss: 0.03600958362221718, Validation Loss: 0.08537724614143372\n",
            "Epoch 2172/2500, Training Loss: 0.03599053993821144, Validation Loss: 0.08536919206380844\n",
            "Epoch 2173/2500, Training Loss: 0.03597211837768555, Validation Loss: 0.08536884933710098\n",
            "Epoch 2174/2500, Training Loss: 0.03595321252942085, Validation Loss: 0.0853637307882309\n",
            "Epoch 2175/2500, Training Loss: 0.03593401610851288, Validation Loss: 0.08537252247333527\n",
            "Epoch 2176/2500, Training Loss: 0.03591624274849892, Validation Loss: 0.08537782728672028\n",
            "Epoch 2177/2500, Training Loss: 0.03589735925197601, Validation Loss: 0.08538182079792023\n",
            "Epoch 2178/2500, Training Loss: 0.035878244787454605, Validation Loss: 0.08537320792675018\n",
            "Epoch 2179/2500, Training Loss: 0.035860251635313034, Validation Loss: 0.08535914123058319\n",
            "Epoch 2180/2500, Training Loss: 0.03584156930446625, Validation Loss: 0.08535020798444748\n",
            "Epoch 2181/2500, Training Loss: 0.03582264110445976, Validation Loss: 0.08535003662109375\n",
            "Epoch 2182/2500, Training Loss: 0.0358038991689682, Validation Loss: 0.08534596860408783\n",
            "Epoch 2183/2500, Training Loss: 0.035785872489213943, Validation Loss: 0.0853504166007042\n",
            "Epoch 2184/2500, Training Loss: 0.03576723486185074, Validation Loss: 0.08536174148321152\n",
            "Epoch 2185/2500, Training Loss: 0.03574864938855171, Validation Loss: 0.08536306768655777\n",
            "Epoch 2186/2500, Training Loss: 0.03572998195886612, Validation Loss: 0.08536718040704727\n",
            "Epoch 2187/2500, Training Loss: 0.03571171313524246, Validation Loss: 0.08536200225353241\n",
            "Epoch 2188/2500, Training Loss: 0.03569328412413597, Validation Loss: 0.08536247164011002\n",
            "Epoch 2189/2500, Training Loss: 0.035675350576639175, Validation Loss: 0.08536527305841446\n",
            "Epoch 2190/2500, Training Loss: 0.03565682843327522, Validation Loss: 0.08537363260984421\n",
            "Epoch 2191/2500, Training Loss: 0.03563922271132469, Validation Loss: 0.0853799358010292\n",
            "Epoch 2192/2500, Training Loss: 0.03562101349234581, Validation Loss: 0.08537480980157852\n",
            "Epoch 2193/2500, Training Loss: 0.03560258075594902, Validation Loss: 0.08535588532686234\n",
            "Epoch 2194/2500, Training Loss: 0.035583775490522385, Validation Loss: 0.08534247428178787\n",
            "Epoch 2195/2500, Training Loss: 0.035565294325351715, Validation Loss: 0.08534875512123108\n",
            "Epoch 2196/2500, Training Loss: 0.035547662526369095, Validation Loss: 0.08535543084144592\n",
            "Epoch 2197/2500, Training Loss: 0.035529330372810364, Validation Loss: 0.0853496640920639\n",
            "Epoch 2198/2500, Training Loss: 0.035511136054992676, Validation Loss: 0.08535367250442505\n",
            "Epoch 2199/2500, Training Loss: 0.035492997616529465, Validation Loss: 0.08536349982023239\n",
            "Epoch 2200/2500, Training Loss: 0.035474199801683426, Validation Loss: 0.0853671208024025\n",
            "Epoch 2201/2500, Training Loss: 0.03545626997947693, Validation Loss: 0.08536748588085175\n",
            "Epoch 2202/2500, Training Loss: 0.03543846681714058, Validation Loss: 0.08536955714225769\n",
            "Epoch 2203/2500, Training Loss: 0.03541979938745499, Validation Loss: 0.0853685513138771\n",
            "Epoch 2204/2500, Training Loss: 0.03540157899260521, Validation Loss: 0.08536320179700851\n",
            "Epoch 2205/2500, Training Loss: 0.035383690148591995, Validation Loss: 0.08536399900913239\n",
            "Epoch 2206/2500, Training Loss: 0.03536611050367355, Validation Loss: 0.0853702500462532\n",
            "Epoch 2207/2500, Training Loss: 0.035348229110240936, Validation Loss: 0.08537228405475616\n",
            "Epoch 2208/2500, Training Loss: 0.03532992675900459, Validation Loss: 0.08536432683467865\n",
            "Epoch 2209/2500, Training Loss: 0.035312023013830185, Validation Loss: 0.08535679429769516\n",
            "Epoch 2210/2500, Training Loss: 0.03529355674982071, Validation Loss: 0.08535824716091156\n",
            "Epoch 2211/2500, Training Loss: 0.035275522619485855, Validation Loss: 0.08536282926797867\n",
            "Epoch 2212/2500, Training Loss: 0.03525836393237114, Validation Loss: 0.08535444736480713\n",
            "Epoch 2213/2500, Training Loss: 0.03523975610733032, Validation Loss: 0.08534438163042068\n",
            "Epoch 2214/2500, Training Loss: 0.03522108122706413, Validation Loss: 0.0853441134095192\n",
            "Epoch 2215/2500, Training Loss: 0.03520362451672554, Validation Loss: 0.08534643054008484\n",
            "Epoch 2216/2500, Training Loss: 0.03518559783697128, Validation Loss: 0.08535660058259964\n",
            "Epoch 2217/2500, Training Loss: 0.035167209804058075, Validation Loss: 0.08536507934331894\n",
            "Epoch 2218/2500, Training Loss: 0.035149358212947845, Validation Loss: 0.08536482602357864\n",
            "Epoch 2219/2500, Training Loss: 0.0351315438747406, Validation Loss: 0.08535853773355484\n",
            "Epoch 2220/2500, Training Loss: 0.03511299565434456, Validation Loss: 0.08535617589950562\n",
            "Epoch 2221/2500, Training Loss: 0.03509562090039253, Validation Loss: 0.0853559672832489\n",
            "Epoch 2222/2500, Training Loss: 0.03507775068283081, Validation Loss: 0.08535171300172806\n",
            "Epoch 2223/2500, Training Loss: 0.03505910560488701, Validation Loss: 0.08535655587911606\n",
            "Epoch 2224/2500, Training Loss: 0.03504203259944916, Validation Loss: 0.08535391092300415\n",
            "Epoch 2225/2500, Training Loss: 0.035023972392082214, Validation Loss: 0.0853508785367012\n",
            "Epoch 2226/2500, Training Loss: 0.035005565732717514, Validation Loss: 0.08534862846136093\n",
            "Epoch 2227/2500, Training Loss: 0.03498787805438042, Validation Loss: 0.08534883707761765\n",
            "Epoch 2228/2500, Training Loss: 0.03497076407074928, Validation Loss: 0.08535733819007874\n",
            "Epoch 2229/2500, Training Loss: 0.03495160862803459, Validation Loss: 0.08536019921302795\n",
            "Epoch 2230/2500, Training Loss: 0.034933898597955704, Validation Loss: 0.08535358309745789\n",
            "Epoch 2231/2500, Training Loss: 0.03491605445742607, Validation Loss: 0.08534760028123856\n",
            "Epoch 2232/2500, Training Loss: 0.034898873418569565, Validation Loss: 0.08534565567970276\n",
            "Epoch 2233/2500, Training Loss: 0.03488105162978172, Validation Loss: 0.08534825593233109\n",
            "Epoch 2234/2500, Training Loss: 0.034863248467445374, Validation Loss: 0.08535252511501312\n",
            "Epoch 2235/2500, Training Loss: 0.03484535962343216, Validation Loss: 0.08533892035484314\n",
            "Epoch 2236/2500, Training Loss: 0.03482691943645477, Validation Loss: 0.08532492071390152\n",
            "Epoch 2237/2500, Training Loss: 0.03480953723192215, Validation Loss: 0.08533145487308502\n",
            "Epoch 2238/2500, Training Loss: 0.03479180857539177, Validation Loss: 0.08533037453889847\n",
            "Epoch 2239/2500, Training Loss: 0.03477412834763527, Validation Loss: 0.08532200753688812\n",
            "Epoch 2240/2500, Training Loss: 0.034756358712911606, Validation Loss: 0.0853193923830986\n",
            "Epoch 2241/2500, Training Loss: 0.034738555550575256, Validation Loss: 0.08532260358333588\n",
            "Epoch 2242/2500, Training Loss: 0.034720949828624725, Validation Loss: 0.08533396571874619\n",
            "Epoch 2243/2500, Training Loss: 0.034703079611063004, Validation Loss: 0.08534272760152817\n",
            "Epoch 2244/2500, Training Loss: 0.03468608111143112, Validation Loss: 0.0853392481803894\n",
            "Epoch 2245/2500, Training Loss: 0.03466808423399925, Validation Loss: 0.08531970530748367\n",
            "Epoch 2246/2500, Training Loss: 0.03465055674314499, Validation Loss: 0.08531353622674942\n",
            "Epoch 2247/2500, Training Loss: 0.03463342785835266, Validation Loss: 0.08531109243631363\n",
            "Epoch 2248/2500, Training Loss: 0.03461555764079094, Validation Loss: 0.08531396836042404\n",
            "Epoch 2249/2500, Training Loss: 0.03459658846259117, Validation Loss: 0.08531494438648224\n",
            "Epoch 2250/2500, Training Loss: 0.03457965701818466, Validation Loss: 0.08531613647937775\n",
            "Epoch 2251/2500, Training Loss: 0.034561220556497574, Validation Loss: 0.08531925827264786\n",
            "Epoch 2252/2500, Training Loss: 0.03454328328371048, Validation Loss: 0.08531657606363297\n",
            "Epoch 2253/2500, Training Loss: 0.034525807946920395, Validation Loss: 0.0853169709444046\n",
            "Epoch 2254/2500, Training Loss: 0.03450890630483627, Validation Loss: 0.08531054109334946\n",
            "Epoch 2255/2500, Training Loss: 0.03449004516005516, Validation Loss: 0.0853053405880928\n",
            "Epoch 2256/2500, Training Loss: 0.03447208181023598, Validation Loss: 0.08531515300273895\n",
            "Epoch 2257/2500, Training Loss: 0.034455157816410065, Validation Loss: 0.08532210439443588\n",
            "Epoch 2258/2500, Training Loss: 0.034436751157045364, Validation Loss: 0.08530385047197342\n",
            "Epoch 2259/2500, Training Loss: 0.034419890493154526, Validation Loss: 0.08528941124677658\n",
            "Epoch 2260/2500, Training Loss: 0.034402769058942795, Validation Loss: 0.08528832346200943\n",
            "Epoch 2261/2500, Training Loss: 0.0343845970928669, Validation Loss: 0.08529427647590637\n",
            "Epoch 2262/2500, Training Loss: 0.034366101026535034, Validation Loss: 0.08530329912900925\n",
            "Epoch 2263/2500, Training Loss: 0.034349508583545685, Validation Loss: 0.08529727905988693\n",
            "Epoch 2264/2500, Training Loss: 0.0343315489590168, Validation Loss: 0.08528836071491241\n",
            "Epoch 2265/2500, Training Loss: 0.03431326523423195, Validation Loss: 0.0852809026837349\n",
            "Epoch 2266/2500, Training Loss: 0.03429671376943588, Validation Loss: 0.08527041971683502\n",
            "Epoch 2267/2500, Training Loss: 0.03427944704890251, Validation Loss: 0.08526089042425156\n",
            "Epoch 2268/2500, Training Loss: 0.034261543303728104, Validation Loss: 0.08526041358709335\n",
            "Epoch 2269/2500, Training Loss: 0.034243129193782806, Validation Loss: 0.08526736497879028\n",
            "Epoch 2270/2500, Training Loss: 0.034225840121507645, Validation Loss: 0.08526670187711716\n",
            "Epoch 2271/2500, Training Loss: 0.03420863673090935, Validation Loss: 0.0852636769413948\n",
            "Epoch 2272/2500, Training Loss: 0.03419044613838196, Validation Loss: 0.08526290953159332\n",
            "Epoch 2273/2500, Training Loss: 0.034172460436820984, Validation Loss: 0.08526655286550522\n",
            "Epoch 2274/2500, Training Loss: 0.03415592759847641, Validation Loss: 0.08527541160583496\n",
            "Epoch 2275/2500, Training Loss: 0.03413809835910797, Validation Loss: 0.08528321981430054\n",
            "Epoch 2276/2500, Training Loss: 0.034119993448257446, Validation Loss: 0.08528436720371246\n",
            "Epoch 2277/2500, Training Loss: 0.03410360962152481, Validation Loss: 0.08528703451156616\n",
            "Epoch 2278/2500, Training Loss: 0.03408639878034592, Validation Loss: 0.08528943359851837\n",
            "Epoch 2279/2500, Training Loss: 0.03406805917620659, Validation Loss: 0.08527661859989166\n",
            "Epoch 2280/2500, Training Loss: 0.034050848335027695, Validation Loss: 0.0852600708603859\n",
            "Epoch 2281/2500, Training Loss: 0.03403434529900551, Validation Loss: 0.08524271845817566\n",
            "Epoch 2282/2500, Training Loss: 0.034016937017440796, Validation Loss: 0.08524410426616669\n",
            "Epoch 2283/2500, Training Loss: 0.033998578786849976, Validation Loss: 0.08526787161827087\n",
            "Epoch 2284/2500, Training Loss: 0.033980488777160645, Validation Loss: 0.08528172969818115\n",
            "Epoch 2285/2500, Training Loss: 0.033963892608881, Validation Loss: 0.08527448028326035\n",
            "Epoch 2286/2500, Training Loss: 0.033945757895708084, Validation Loss: 0.08525227010250092\n",
            "Epoch 2287/2500, Training Loss: 0.033928483724594116, Validation Loss: 0.0852370634675026\n",
            "Epoch 2288/2500, Training Loss: 0.03391137346625328, Validation Loss: 0.0852343961596489\n",
            "Epoch 2289/2500, Training Loss: 0.03389403596520424, Validation Loss: 0.08525008708238602\n",
            "Epoch 2290/2500, Training Loss: 0.03387659043073654, Validation Loss: 0.08527320623397827\n",
            "Epoch 2291/2500, Training Loss: 0.033859968185424805, Validation Loss: 0.08528213202953339\n",
            "Epoch 2292/2500, Training Loss: 0.033841922879219055, Validation Loss: 0.08528051525354385\n",
            "Epoch 2293/2500, Training Loss: 0.03382503241300583, Validation Loss: 0.08527132868766785\n",
            "Epoch 2294/2500, Training Loss: 0.03380846977233887, Validation Loss: 0.08525959402322769\n",
            "Epoch 2295/2500, Training Loss: 0.0337909534573555, Validation Loss: 0.08526552468538284\n",
            "Epoch 2296/2500, Training Loss: 0.03377414867281914, Validation Loss: 0.0852741226553917\n",
            "Epoch 2297/2500, Training Loss: 0.03375696763396263, Validation Loss: 0.08527664095163345\n",
            "Epoch 2298/2500, Training Loss: 0.033739492297172546, Validation Loss: 0.08527175337076187\n",
            "Epoch 2299/2500, Training Loss: 0.03372246399521828, Validation Loss: 0.0852721780538559\n",
            "Epoch 2300/2500, Training Loss: 0.03370528295636177, Validation Loss: 0.08528116345405579\n",
            "Epoch 2301/2500, Training Loss: 0.03368845954537392, Validation Loss: 0.08528152108192444\n",
            "Epoch 2302/2500, Training Loss: 0.03367103636264801, Validation Loss: 0.08527970314025879\n",
            "Epoch 2303/2500, Training Loss: 0.0336541086435318, Validation Loss: 0.08527988195419312\n",
            "Epoch 2304/2500, Training Loss: 0.033636968582868576, Validation Loss: 0.08527520298957825\n",
            "Epoch 2305/2500, Training Loss: 0.03361929580569267, Validation Loss: 0.08527969568967819\n",
            "Epoch 2306/2500, Training Loss: 0.03360166773200035, Validation Loss: 0.08526855707168579\n",
            "Epoch 2307/2500, Training Loss: 0.03358558937907219, Validation Loss: 0.08524859696626663\n",
            "Epoch 2308/2500, Training Loss: 0.033567897975444794, Validation Loss: 0.0852322205901146\n",
            "Epoch 2309/2500, Training Loss: 0.03355085849761963, Validation Loss: 0.08522529155015945\n",
            "Epoch 2310/2500, Training Loss: 0.03353404998779297, Validation Loss: 0.0852256715297699\n",
            "Epoch 2311/2500, Training Loss: 0.03351651504635811, Validation Loss: 0.08523394167423248\n",
            "Epoch 2312/2500, Training Loss: 0.03349807858467102, Validation Loss: 0.0852489247918129\n",
            "Epoch 2313/2500, Training Loss: 0.03348070755600929, Validation Loss: 0.08525737375020981\n",
            "Epoch 2314/2500, Training Loss: 0.03346440941095352, Validation Loss: 0.0852508619427681\n",
            "Epoch 2315/2500, Training Loss: 0.033446770161390305, Validation Loss: 0.08524591475725174\n",
            "Epoch 2316/2500, Training Loss: 0.03342922776937485, Validation Loss: 0.08524689823389053\n",
            "Epoch 2317/2500, Training Loss: 0.03341217339038849, Validation Loss: 0.08524906635284424\n",
            "Epoch 2318/2500, Training Loss: 0.03339637815952301, Validation Loss: 0.08524391055107117\n",
            "Epoch 2319/2500, Training Loss: 0.03337797522544861, Validation Loss: 0.08524459600448608\n",
            "Epoch 2320/2500, Training Loss: 0.03336009010672569, Validation Loss: 0.08525260537862778\n",
            "Epoch 2321/2500, Training Loss: 0.03334289416670799, Validation Loss: 0.08526905626058578\n",
            "Epoch 2322/2500, Training Loss: 0.03332706540822983, Validation Loss: 0.08527811616659164\n",
            "Epoch 2323/2500, Training Loss: 0.033309679478406906, Validation Loss: 0.08527643978595734\n",
            "Epoch 2324/2500, Training Loss: 0.03329283744096756, Validation Loss: 0.08526455610990524\n",
            "Epoch 2325/2500, Training Loss: 0.03327634185552597, Validation Loss: 0.08525140583515167\n",
            "Epoch 2326/2500, Training Loss: 0.03325900807976723, Validation Loss: 0.08525216579437256\n",
            "Epoch 2327/2500, Training Loss: 0.03324208781123161, Validation Loss: 0.0852564200758934\n",
            "Epoch 2328/2500, Training Loss: 0.03322583809494972, Validation Loss: 0.08525733649730682\n",
            "Epoch 2329/2500, Training Loss: 0.03320943936705589, Validation Loss: 0.08524660021066666\n",
            "Epoch 2330/2500, Training Loss: 0.03319191560149193, Validation Loss: 0.08522956073284149\n",
            "Epoch 2331/2500, Training Loss: 0.03317534551024437, Validation Loss: 0.08521796762943268\n",
            "Epoch 2332/2500, Training Loss: 0.03315908461809158, Validation Loss: 0.08522903174161911\n",
            "Epoch 2333/2500, Training Loss: 0.033142175525426865, Validation Loss: 0.08524709194898605\n",
            "Epoch 2334/2500, Training Loss: 0.033124733716249466, Validation Loss: 0.08527036756277084\n",
            "Epoch 2335/2500, Training Loss: 0.033108677715063095, Validation Loss: 0.08527674525976181\n",
            "Epoch 2336/2500, Training Loss: 0.033091895282268524, Validation Loss: 0.08525824546813965\n",
            "Epoch 2337/2500, Training Loss: 0.033074501901865005, Validation Loss: 0.08523384481668472\n",
            "Epoch 2338/2500, Training Loss: 0.033057838678359985, Validation Loss: 0.08520979434251785\n",
            "Epoch 2339/2500, Training Loss: 0.03304165229201317, Validation Loss: 0.08519048988819122\n",
            "Epoch 2340/2500, Training Loss: 0.03302473574876785, Validation Loss: 0.08518248051404953\n",
            "Epoch 2341/2500, Training Loss: 0.03300706669688225, Validation Loss: 0.0851934552192688\n",
            "Epoch 2342/2500, Training Loss: 0.03299091011285782, Validation Loss: 0.0852239653468132\n",
            "Epoch 2343/2500, Training Loss: 0.0329737514257431, Validation Loss: 0.0852532833814621\n",
            "Epoch 2344/2500, Training Loss: 0.03295688331127167, Validation Loss: 0.08526290953159332\n",
            "Epoch 2345/2500, Training Loss: 0.03293990716338158, Validation Loss: 0.08524425327777863\n",
            "Epoch 2346/2500, Training Loss: 0.03292279317975044, Validation Loss: 0.08521406352519989\n",
            "Epoch 2347/2500, Training Loss: 0.03290621191263199, Validation Loss: 0.08519169688224792\n",
            "Epoch 2348/2500, Training Loss: 0.03288908302783966, Validation Loss: 0.08519483357667923\n",
            "Epoch 2349/2500, Training Loss: 0.03287278115749359, Validation Loss: 0.08520261198282242\n",
            "Epoch 2350/2500, Training Loss: 0.03285561874508858, Validation Loss: 0.08520323038101196\n",
            "Epoch 2351/2500, Training Loss: 0.0328388586640358, Validation Loss: 0.08521319925785065\n",
            "Epoch 2352/2500, Training Loss: 0.0328226163983345, Validation Loss: 0.08522488176822662\n",
            "Epoch 2353/2500, Training Loss: 0.03280507028102875, Validation Loss: 0.0852300226688385\n",
            "Epoch 2354/2500, Training Loss: 0.032789118587970734, Validation Loss: 0.08520851284265518\n",
            "Epoch 2355/2500, Training Loss: 0.03277186304330826, Validation Loss: 0.08517515659332275\n",
            "Epoch 2356/2500, Training Loss: 0.0327552892267704, Validation Loss: 0.085163414478302\n",
            "Epoch 2357/2500, Training Loss: 0.03273933380842209, Validation Loss: 0.08517690002918243\n",
            "Epoch 2358/2500, Training Loss: 0.032722413539886475, Validation Loss: 0.08519910275936127\n",
            "Epoch 2359/2500, Training Loss: 0.03270454332232475, Validation Loss: 0.08522192388772964\n",
            "Epoch 2360/2500, Training Loss: 0.03268837928771973, Validation Loss: 0.08522169291973114\n",
            "Epoch 2361/2500, Training Loss: 0.03267204016447067, Validation Loss: 0.0852072685956955\n",
            "Epoch 2362/2500, Training Loss: 0.03265457972884178, Validation Loss: 0.08518306165933609\n",
            "Epoch 2363/2500, Training Loss: 0.03263980522751808, Validation Loss: 0.08517532050609589\n",
            "Epoch 2364/2500, Training Loss: 0.03262292221188545, Validation Loss: 0.08516957610845566\n",
            "Epoch 2365/2500, Training Loss: 0.032606691122055054, Validation Loss: 0.08518456667661667\n",
            "Epoch 2366/2500, Training Loss: 0.03258971869945526, Validation Loss: 0.08519786596298218\n",
            "Epoch 2367/2500, Training Loss: 0.032574668526649475, Validation Loss: 0.08521150052547455\n",
            "Epoch 2368/2500, Training Loss: 0.032557982951402664, Validation Loss: 0.0852021723985672\n",
            "Epoch 2369/2500, Training Loss: 0.03253980353474617, Validation Loss: 0.08519694954156876\n",
            "Epoch 2370/2500, Training Loss: 0.032523784786462784, Validation Loss: 0.08520307391881943\n",
            "Epoch 2371/2500, Training Loss: 0.03250706568360329, Validation Loss: 0.08522233366966248\n",
            "Epoch 2372/2500, Training Loss: 0.03249213099479675, Validation Loss: 0.08522432297468185\n",
            "Epoch 2373/2500, Training Loss: 0.03247600421309471, Validation Loss: 0.08520671725273132\n",
            "Epoch 2374/2500, Training Loss: 0.032458409667015076, Validation Loss: 0.08518151193857193\n",
            "Epoch 2375/2500, Training Loss: 0.032441701740026474, Validation Loss: 0.08516260981559753\n",
            "Epoch 2376/2500, Training Loss: 0.03242569416761398, Validation Loss: 0.08515845984220505\n",
            "Epoch 2377/2500, Training Loss: 0.03240931034088135, Validation Loss: 0.08517370373010635\n",
            "Epoch 2378/2500, Training Loss: 0.03239304944872856, Validation Loss: 0.08519357442855835\n",
            "Epoch 2379/2500, Training Loss: 0.032376717776060104, Validation Loss: 0.08521270751953125\n",
            "Epoch 2380/2500, Training Loss: 0.03236059844493866, Validation Loss: 0.08521739393472672\n",
            "Epoch 2381/2500, Training Loss: 0.03234391659498215, Validation Loss: 0.08521789312362671\n",
            "Epoch 2382/2500, Training Loss: 0.03232791647315025, Validation Loss: 0.08521687984466553\n",
            "Epoch 2383/2500, Training Loss: 0.03231203928589821, Validation Loss: 0.08522835373878479\n",
            "Epoch 2384/2500, Training Loss: 0.03229505196213722, Validation Loss: 0.0852479413151741\n",
            "Epoch 2385/2500, Training Loss: 0.032279565930366516, Validation Loss: 0.08525202423334122\n",
            "Epoch 2386/2500, Training Loss: 0.032264020293951035, Validation Loss: 0.085237056016922\n",
            "Epoch 2387/2500, Training Loss: 0.032246824353933334, Validation Loss: 0.08521188050508499\n",
            "Epoch 2388/2500, Training Loss: 0.03223058953881264, Validation Loss: 0.08520607650279999\n",
            "Epoch 2389/2500, Training Loss: 0.03221430629491806, Validation Loss: 0.085218146443367\n",
            "Epoch 2390/2500, Training Loss: 0.03219834715127945, Validation Loss: 0.08521869778633118\n",
            "Epoch 2391/2500, Training Loss: 0.03218187019228935, Validation Loss: 0.08520649373531342\n",
            "Epoch 2392/2500, Training Loss: 0.03216618672013283, Validation Loss: 0.08521226048469543\n",
            "Epoch 2393/2500, Training Loss: 0.032149817794561386, Validation Loss: 0.08522620797157288\n",
            "Epoch 2394/2500, Training Loss: 0.03213400021195412, Validation Loss: 0.08524172008037567\n",
            "Epoch 2395/2500, Training Loss: 0.032118506729602814, Validation Loss: 0.0852445513010025\n",
            "Epoch 2396/2500, Training Loss: 0.03210199251770973, Validation Loss: 0.08521861582994461\n",
            "Epoch 2397/2500, Training Loss: 0.032086338847875595, Validation Loss: 0.08519697934389114\n",
            "Epoch 2398/2500, Training Loss: 0.03207084536552429, Validation Loss: 0.08519801497459412\n",
            "Epoch 2399/2500, Training Loss: 0.03205486387014389, Validation Loss: 0.08521510660648346\n",
            "Epoch 2400/2500, Training Loss: 0.03203902021050453, Validation Loss: 0.08523151278495789\n",
            "Epoch 2401/2500, Training Loss: 0.03202259540557861, Validation Loss: 0.08524223417043686\n",
            "Epoch 2402/2500, Training Loss: 0.03200655058026314, Validation Loss: 0.08524654805660248\n",
            "Epoch 2403/2500, Training Loss: 0.03199199587106705, Validation Loss: 0.08523959666490555\n",
            "Epoch 2404/2500, Training Loss: 0.0319754034280777, Validation Loss: 0.0852237194776535\n",
            "Epoch 2405/2500, Training Loss: 0.03195947781205177, Validation Loss: 0.08521335572004318\n",
            "Epoch 2406/2500, Training Loss: 0.031943824142217636, Validation Loss: 0.08521352708339691\n",
            "Epoch 2407/2500, Training Loss: 0.03192766755819321, Validation Loss: 0.08523084223270416\n",
            "Epoch 2408/2500, Training Loss: 0.031911641359329224, Validation Loss: 0.08524506539106369\n",
            "Epoch 2409/2500, Training Loss: 0.03189609944820404, Validation Loss: 0.08525138348340988\n",
            "Epoch 2410/2500, Training Loss: 0.031880397349596024, Validation Loss: 0.08524663746356964\n",
            "Epoch 2411/2500, Training Loss: 0.031864382326602936, Validation Loss: 0.08522794395685196\n",
            "Epoch 2412/2500, Training Loss: 0.03184876963496208, Validation Loss: 0.08521731197834015\n",
            "Epoch 2413/2500, Training Loss: 0.031832702457904816, Validation Loss: 0.08521497249603271\n",
            "Epoch 2414/2500, Training Loss: 0.03181742876768112, Validation Loss: 0.08521613478660583\n",
            "Epoch 2415/2500, Training Loss: 0.03180219978094101, Validation Loss: 0.08522869646549225\n",
            "Epoch 2416/2500, Training Loss: 0.03178545460104942, Validation Loss: 0.0852338895201683\n",
            "Epoch 2417/2500, Training Loss: 0.03176938742399216, Validation Loss: 0.08522308617830276\n",
            "Epoch 2418/2500, Training Loss: 0.03175520524382591, Validation Loss: 0.08522775024175644\n",
            "Epoch 2419/2500, Training Loss: 0.03173958510160446, Validation Loss: 0.08523234724998474\n",
            "Epoch 2420/2500, Training Loss: 0.03172241896390915, Validation Loss: 0.08522874861955643\n",
            "Epoch 2421/2500, Training Loss: 0.03170781582593918, Validation Loss: 0.08521529287099838\n",
            "Epoch 2422/2500, Training Loss: 0.03169265761971474, Validation Loss: 0.08519692718982697\n",
            "Epoch 2423/2500, Training Loss: 0.03167635574936867, Validation Loss: 0.08519888669252396\n",
            "Epoch 2424/2500, Training Loss: 0.03166060894727707, Validation Loss: 0.08521441370248795\n",
            "Epoch 2425/2500, Training Loss: 0.03164520487189293, Validation Loss: 0.08522430807352066\n",
            "Epoch 2426/2500, Training Loss: 0.031630199402570724, Validation Loss: 0.08522104471921921\n",
            "Epoch 2427/2500, Training Loss: 0.03161386027932167, Validation Loss: 0.08521617203950882\n",
            "Epoch 2428/2500, Training Loss: 0.031598325818777084, Validation Loss: 0.08522456139326096\n",
            "Epoch 2429/2500, Training Loss: 0.03158343583345413, Validation Loss: 0.08523488789796829\n",
            "Epoch 2430/2500, Training Loss: 0.03156813234090805, Validation Loss: 0.08522617071866989\n",
            "Epoch 2431/2500, Training Loss: 0.031551793217659, Validation Loss: 0.08521196991205215\n",
            "Epoch 2432/2500, Training Loss: 0.0315360352396965, Validation Loss: 0.08521289378404617\n",
            "Epoch 2433/2500, Training Loss: 0.03152046725153923, Validation Loss: 0.0852195993065834\n",
            "Epoch 2434/2500, Training Loss: 0.031505126506090164, Validation Loss: 0.0852225050330162\n",
            "Epoch 2435/2500, Training Loss: 0.03148936107754707, Validation Loss: 0.08522245287895203\n",
            "Epoch 2436/2500, Training Loss: 0.03147421032190323, Validation Loss: 0.08521077036857605\n",
            "Epoch 2437/2500, Training Loss: 0.031458817422389984, Validation Loss: 0.0851951614022255\n",
            "Epoch 2438/2500, Training Loss: 0.031443461775779724, Validation Loss: 0.08519735932350159\n",
            "Epoch 2439/2500, Training Loss: 0.03142809495329857, Validation Loss: 0.08521202206611633\n",
            "Epoch 2440/2500, Training Loss: 0.031412821263074875, Validation Loss: 0.0852259173989296\n",
            "Epoch 2441/2500, Training Loss: 0.03139739856123924, Validation Loss: 0.08522948622703552\n",
            "Epoch 2442/2500, Training Loss: 0.03138202056288719, Validation Loss: 0.08521625399589539\n",
            "Epoch 2443/2500, Training Loss: 0.031366538256406784, Validation Loss: 0.08520237356424332\n",
            "Epoch 2444/2500, Training Loss: 0.03135136887431145, Validation Loss: 0.08520109206438065\n",
            "Epoch 2445/2500, Training Loss: 0.03133630007505417, Validation Loss: 0.085201695561409\n",
            "Epoch 2446/2500, Training Loss: 0.031320761889219284, Validation Loss: 0.0852043554186821\n",
            "Epoch 2447/2500, Training Loss: 0.031305551528930664, Validation Loss: 0.08520971983671188\n",
            "Epoch 2448/2500, Training Loss: 0.0312906950712204, Validation Loss: 0.08521604537963867\n",
            "Epoch 2449/2500, Training Loss: 0.031275395303964615, Validation Loss: 0.0852251723408699\n",
            "Epoch 2450/2500, Training Loss: 0.031260378658771515, Validation Loss: 0.0852317288517952\n",
            "Epoch 2451/2500, Training Loss: 0.031245756894350052, Validation Loss: 0.0852329283952713\n",
            "Epoch 2452/2500, Training Loss: 0.031230291351675987, Validation Loss: 0.08522062003612518\n",
            "Epoch 2453/2500, Training Loss: 0.03121492825448513, Validation Loss: 0.08520545065402985\n",
            "Epoch 2454/2500, Training Loss: 0.03119984082877636, Validation Loss: 0.08520204573869705\n",
            "Epoch 2455/2500, Training Loss: 0.031186053529381752, Validation Loss: 0.0852092057466507\n",
            "Epoch 2456/2500, Training Loss: 0.03117111138999462, Validation Loss: 0.08522490411996841\n",
            "Epoch 2457/2500, Training Loss: 0.03115495853126049, Validation Loss: 0.0852276161313057\n",
            "Epoch 2458/2500, Training Loss: 0.031139640137553215, Validation Loss: 0.08522086590528488\n",
            "Epoch 2459/2500, Training Loss: 0.031124994158744812, Validation Loss: 0.08521359413862228\n",
            "Epoch 2460/2500, Training Loss: 0.031109992414712906, Validation Loss: 0.08520728349685669\n",
            "Epoch 2461/2500, Training Loss: 0.031094105914235115, Validation Loss: 0.08521208167076111\n",
            "Epoch 2462/2500, Training Loss: 0.031079962849617004, Validation Loss: 0.08521701395511627\n",
            "Epoch 2463/2500, Training Loss: 0.03106560930609703, Validation Loss: 0.0852227732539177\n",
            "Epoch 2464/2500, Training Loss: 0.031049685552716255, Validation Loss: 0.08523351699113846\n",
            "Epoch 2465/2500, Training Loss: 0.031033914536237717, Validation Loss: 0.0852329283952713\n",
            "Epoch 2466/2500, Training Loss: 0.031019549816846848, Validation Loss: 0.08520596474409103\n",
            "Epoch 2467/2500, Training Loss: 0.031004002317786217, Validation Loss: 0.0851878821849823\n",
            "Epoch 2468/2500, Training Loss: 0.030988652259111404, Validation Loss: 0.0851876437664032\n",
            "Epoch 2469/2500, Training Loss: 0.03097265213727951, Validation Loss: 0.08520250022411346\n",
            "Epoch 2470/2500, Training Loss: 0.030957693234086037, Validation Loss: 0.08520226925611496\n",
            "Epoch 2471/2500, Training Loss: 0.030942779034376144, Validation Loss: 0.0851949006319046\n",
            "Epoch 2472/2500, Training Loss: 0.030926262959837914, Validation Loss: 0.08517216145992279\n",
            "Epoch 2473/2500, Training Loss: 0.03091157227754593, Validation Loss: 0.08515709638595581\n",
            "Epoch 2474/2500, Training Loss: 0.030896436423063278, Validation Loss: 0.08515249192714691\n",
            "Epoch 2475/2500, Training Loss: 0.030880598351359367, Validation Loss: 0.08514416217803955\n",
            "Epoch 2476/2500, Training Loss: 0.030864553526043892, Validation Loss: 0.08513090014457703\n",
            "Epoch 2477/2500, Training Loss: 0.030848367139697075, Validation Loss: 0.08512180298566818\n",
            "Epoch 2478/2500, Training Loss: 0.03083305060863495, Validation Loss: 0.08513398468494415\n",
            "Epoch 2479/2500, Training Loss: 0.030816199257969856, Validation Loss: 0.08516731858253479\n",
            "Epoch 2480/2500, Training Loss: 0.03080066107213497, Validation Loss: 0.08519000560045242\n",
            "Epoch 2481/2500, Training Loss: 0.03078538179397583, Validation Loss: 0.08517824858427048\n",
            "Epoch 2482/2500, Training Loss: 0.030768530443310738, Validation Loss: 0.08514142781496048\n",
            "Epoch 2483/2500, Training Loss: 0.03075226955115795, Validation Loss: 0.08511745929718018\n",
            "Epoch 2484/2500, Training Loss: 0.0307367742061615, Validation Loss: 0.08510539680719376\n",
            "Epoch 2485/2500, Training Loss: 0.030721183866262436, Validation Loss: 0.08510762453079224\n",
            "Epoch 2486/2500, Training Loss: 0.03070562146604061, Validation Loss: 0.08511172980070114\n",
            "Epoch 2487/2500, Training Loss: 0.030688565224409103, Validation Loss: 0.085114985704422\n",
            "Epoch 2488/2500, Training Loss: 0.030673077329993248, Validation Loss: 0.08512216061353683\n",
            "Epoch 2489/2500, Training Loss: 0.03065803460776806, Validation Loss: 0.08511701971292496\n",
            "Epoch 2490/2500, Training Loss: 0.03064163774251938, Validation Loss: 0.08511704206466675\n",
            "Epoch 2491/2500, Training Loss: 0.030624769628047943, Validation Loss: 0.0851074829697609\n",
            "Epoch 2492/2500, Training Loss: 0.030609777197241783, Validation Loss: 0.08509407192468643\n",
            "Epoch 2493/2500, Training Loss: 0.03059467114508152, Validation Loss: 0.0850641131401062\n",
            "Epoch 2494/2500, Training Loss: 0.03057878650724888, Validation Loss: 0.08503872156143188\n",
            "Epoch 2495/2500, Training Loss: 0.030563103035092354, Validation Loss: 0.08504251390695572\n",
            "Epoch 2496/2500, Training Loss: 0.030546508729457855, Validation Loss: 0.08505456894636154\n",
            "Epoch 2497/2500, Training Loss: 0.03053021803498268, Validation Loss: 0.08506795018911362\n",
            "Epoch 2498/2500, Training Loss: 0.03051476553082466, Validation Loss: 0.08506948500871658\n",
            "Epoch 2499/2500, Training Loss: 0.030499912798404694, Validation Loss: 0.08505722135305405\n",
            "Epoch 2500/2500, Training Loss: 0.030483176931738853, Validation Loss: 0.08505556732416153\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 96.8586387434555%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.load('/content/features.npz')\n",
        "X = data['features']\n",
        "y = data['labels']\n",
        "\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "    raise ValueError(\"The features or labels are empty. Please check the data preprocessing step.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "class WakeWordModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WakeWordModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = WakeWordModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "num_epochs = 2500\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_test)\n",
        "        val_loss = criterion(val_outputs, y_test)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    print(f'Accuracy: {accuracy * 100}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test.cpu(), predicted.cpu())\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
